{"cells":[{"cell_type":"markdown","id":"a786c77c","metadata":{"id":"a786c77c"},"source":["# LangChain: Memory"]},{"cell_type":"markdown","source":["* The ability to store information about past interactions is called `memory`.\n","* LangChain provides a lot of utilities for adding memory to a system.\n","  These utilities can be used by themselves or incorporated seamlessly into a chain.\n","* A chain's memory system serves two primary functions: reading and writing. In each run, a chain interacts with its memory system twice:\n","    1. Before executing its core logic, the chain reads from memory to augment user inputs.\n","    \n","    2. After executing its core logic, but before providing an answer, the chain writes the current run's inputs and outputs to memory for future reference."],"metadata":{"id":"sm-8jPSellxG"},"id":"sm-8jPSellxG"},{"cell_type":"code","source":["!pip install langchain_openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvQNmjX4vObQ","executionInfo":{"status":"ok","timestamp":1758031498239,"user_tz":-330,"elapsed":11874,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"87c239d6-41f9-4797-e78f-22f839488d5a","collapsed":true},"id":"DvQNmjX4vObQ","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_openai\n","  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n","Collecting langchain-core<1.0.0,>=0.3.76 (from langchain_openai)\n","  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.106.1)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.24)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (6.0.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (4.15.0)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (25.0)\n","Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.11.7)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.67.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.4)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain_openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.24.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n","Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: langchain-core, langchain_openai\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.75\n","    Uninstalling langchain-core-0.3.75:\n","      Successfully uninstalled langchain-core-0.3.75\n","Successfully installed langchain-core-0.3.76 langchain_openai-0.3.33\n"]}]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","if not os.environ.get(\"OPENAI_API_KEY\"):\n","  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q43yiIF0vKWB","executionInfo":{"status":"ok","timestamp":1758031509054,"user_tz":-330,"elapsed":2804,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"9de78fbf-c495-4b9a-84d3-626faa0e9039"},"id":"q43yiIF0vKWB","execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["OPENAI API Key:\n","··········\n"]}]},{"cell_type":"markdown","id":"1297dcd5","metadata":{"id":"1297dcd5"},"source":["### ConversationBufferMemory\n","\n","\n","ConversationBufferMemory is an extremely simple form of memory that just keeps a list of chat messages in a buffer and passes those into the prompt template."]},{"cell_type":"code","execution_count":3,"id":"20ad6fe2","metadata":{"id":"20ad6fe2","executionInfo":{"status":"ok","timestamp":1758031557351,"user_tz":-330,"elapsed":9170,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory"]},{"cell_type":"code","execution_count":4,"id":"88bdf13d","metadata":{"id":"88bdf13d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758031569402,"user_tz":-330,"elapsed":851,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"ce827f87-cef5-43a7-ebb7-9111c2a928f2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3581861627.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferMemory()\n","/tmp/ipython-input-3581861627.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n","  conversation = ConversationChain(\n"]}],"source":["llm = ChatOpenAI(temperature=0.0)\n","memory = ConversationBufferMemory()\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"]},{"cell_type":"code","source":["conversation.predict(input=\"Hi, my name is Baburao Ganpatrao Apte. I am from Pune, Maharastra\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"id":"s19LRPdICJNm","executionInfo":{"status":"ok","timestamp":1758031596616,"user_tz":-330,"elapsed":1472,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"4e2ebb96-06ee-4ac5-f046-a75c5e969672"},"id":"s19LRPdICJNm","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hi, my name is Baburao Ganpatrao Apte. I am from Pune, Maharastra\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Hello Baburao Ganpatrao Apte from Pune, Maharastra! It's nice to meet you. How can I assist you today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["conversation.predict(input=\"Who is owner of Star Fisheries?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"QH4atAr4C_VG","executionInfo":{"status":"ok","timestamp":1758031627993,"user_tz":-330,"elapsed":1212,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"706f3000-2087-436e-b0dc-7591265e52bb"},"id":"QH4atAr4C_VG","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Baburao Ganpatrao Apte. I am from Pune, Maharastra\n","AI: Hello Baburao Ganpatrao Apte from Pune, Maharastra! It's nice to meet you. How can I assist you today?\n","Human: Who is owner of Star Fisheries?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"I'm sorry, but I do not have that specific information. Would you like me to look it up for you?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["conversation.predict(input=\"What is double of 25 lakhs ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":323},"id":"AF7C0FNADGS6","executionInfo":{"status":"ok","timestamp":1758031663402,"user_tz":-330,"elapsed":1112,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"69fd9728-91a2-496e-9d25-f7fc8cdf9de6"},"id":"AF7C0FNADGS6","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Baburao Ganpatrao Apte. I am from Pune, Maharastra\n","AI: Hello Baburao Ganpatrao Apte from Pune, Maharastra! It's nice to meet you. How can I assist you today?\n","Human: Who is owner of Star Fisheries?\n","AI: I'm sorry, but I do not have that specific information. Would you like me to look it up for you?\n","Human: What is double of 25 lakhs ?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'The double of 25 lakhs is 50 lakhs.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["conversation.predict(input=\"What is my name?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"yLrIGhLdDRYx","executionInfo":{"status":"ok","timestamp":1758031674079,"user_tz":-330,"elapsed":1206,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"8990e8e9-d815-4435-a8f1-b7883ac34835"},"id":"yLrIGhLdDRYx","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Baburao Ganpatrao Apte. I am from Pune, Maharastra\n","AI: Hello Baburao Ganpatrao Apte from Pune, Maharastra! It's nice to meet you. How can I assist you today?\n","Human: Who is owner of Star Fisheries?\n","AI: I'm sorry, but I do not have that specific information. Would you like me to look it up for you?\n","Human: What is double of 25 lakhs ?\n","AI: The double of 25 lakhs is 50 lakhs.\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Your name is Baburao Ganpatrao Apte.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"id":"2529400d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2529400d","outputId":"12ace565-13c9-4e29-d009-1ddebb12a7f5","executionInfo":{"status":"ok","timestamp":1750852042956,"user_tz":-330,"elapsed":21,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi, my name is Baburao Ganpatrao Apte. I am from Pune, Maharastra\n","AI: Hello Baburao Ganpatrao Apte from Pune, Maharashtra! It's nice to meet you. How can I assist you today?\n","Human: Who is owner of Star Fisheries?\n","AI: I'm sorry, but I do not have that specific information.\n","Human: What is double of 25 lakhs ?\n","AI: The double of 25 lakhs is 50 lakhs.\n","Human: What is my name?\n","AI: Your name is Baburao Ganpatrao Apte.\n"]}],"source":["print(memory.buffer)"]},{"cell_type":"code","execution_count":null,"id":"5018cb0a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5018cb0a","outputId":"19690bed-7812-444c-be14-f29805be0a5b","executionInfo":{"status":"ok","timestamp":1746956868780,"user_tz":-330,"elapsed":12,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'history': \"Human: Hi, my name is Baburao Ganpatrao Apte. I am from Pune, Maharastra\\nAI: Hello Baburao Ganpatrao Apte from Pune, Maharastra! It's nice to meet you. How can I assist you today?\\nHuman: Who is owner of Star Fisheries?\\nAI: I'm sorry, but I do not have that specific information. Would you like me to look it up for you?\\nHuman: What is double of 25 lakhs ?\\nAI: The double of 25 lakhs is 50 lakhs.\\nHuman: What is my name?\\nAI: Your name is Baburao Ganpatrao Apte.\"}\n"]}],"source":["print(memory.load_memory_variables({}))"]},{"cell_type":"markdown","id":"cf98e9ff","metadata":{"id":"cf98e9ff"},"source":["### ConversationBufferWindowMemory\n","\n","ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."]},{"cell_type":"code","execution_count":9,"id":"66eeccc3","metadata":{"id":"66eeccc3","executionInfo":{"status":"ok","timestamp":1758031862238,"user_tz":-330,"elapsed":5,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain.memory import ConversationBufferWindowMemory"]},{"cell_type":"code","execution_count":10,"id":"3ea6233e","metadata":{"id":"3ea6233e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758031866271,"user_tz":-330,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"fd1b4123-29cc-4958-ab59-750de150a56d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3135384584.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferWindowMemory(k=1)\n"]}],"source":["memory = ConversationBufferWindowMemory(k=1)"]},{"cell_type":"code","execution_count":null,"id":"dc4553fb","metadata":{"id":"dc4553fb"},"outputs":[],"source":["memory.save_context({\"input\": \"Hii\"},\n","                    {\"output\": \"What's going on\"})\n","memory.save_context({\"input\": \"All good, How about you ?\"},\n","                    {\"output\": \"Same here.\"})\n"]},{"cell_type":"code","execution_count":null,"id":"6a788403","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6a788403","outputId":"73a537ae-7ff3-45f9-fd7e-03fd40ba46f7","executionInfo":{"status":"ok","timestamp":1750852077669,"user_tz":-330,"elapsed":7,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'Human: All good, How about you ?\\nAI: Same here.'}"]},"metadata":{},"execution_count":14}],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":11,"id":"4087bc87","metadata":{"id":"4087bc87","executionInfo":{"status":"ok","timestamp":1758031879520,"user_tz":-330,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["llm = ChatOpenAI(temperature=0.0)\n","memory = ConversationBufferWindowMemory(k=1)\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":12,"id":"4faaa952","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"id":"4faaa952","outputId":"e587d268-08b2-41d2-9c8d-092eb2086abe","executionInfo":{"status":"ok","timestamp":1758031888048,"user_tz":-330,"elapsed":823,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hi, my name is Raju ?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Hello Raju! It's nice to meet you. How can I assist you today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["conversation.predict(input=\"Hi, my name is Raju ?\")"]},{"cell_type":"code","execution_count":13,"id":"bb20ddaa","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"bb20ddaa","outputId":"833bd3f9-76fd-445c-f480-42d8d491bb03","executionInfo":{"status":"ok","timestamp":1758031897282,"user_tz":-330,"elapsed":1390,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Raju ?\n","AI: Hello Raju! It's nice to meet you. How can I assist you today?\n","Human: What is 4 times returns of 25 lacs as input ?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'The result of 4 times the input of 25 lacs would be 1 crore.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["conversation.predict(input=\"What is 4 times returns of 25 lacs as input ?\")"]},{"cell_type":"code","execution_count":14,"id":"489b2194","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"489b2194","outputId":"254e56cb-d202-4056-a7d7-93f3ded60a4e","executionInfo":{"status":"ok","timestamp":1758031940616,"user_tz":-330,"elapsed":1023,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: What is 4 times returns of 25 lacs as input ?\n","AI: The result of 4 times the input of 25 lacs would be 1 crore.\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"I'm sorry, I do not have access to personal information such as your name.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["conversation.predict(input=\"What is my name?\")"]},{"cell_type":"markdown","source":["We set the sliding window to 1. So, it remembers only the\n","last conversation."],"metadata":{"id":"yS9jX8Moq5Xt"},"id":"yS9jX8Moq5Xt"},{"cell_type":"markdown","id":"d2931b92","metadata":{"id":"d2931b92"},"source":["### ConversationTokenBufferMemory"]},{"cell_type":"code","execution_count":15,"id":"9f6d063c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9f6d063c","outputId":"a7181294-90f6-4073-8448-5dac14db795a","executionInfo":{"status":"ok","timestamp":1758031967820,"user_tz":-330,"elapsed":4013,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"]}],"source":["!pip install tiktoken"]},{"cell_type":"code","execution_count":16,"id":"fb9020ed","metadata":{"id":"fb9020ed","executionInfo":{"status":"ok","timestamp":1758031978299,"user_tz":-330,"elapsed":3,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain.memory import ConversationTokenBufferMemory\n","#from langchain.llms import OpenAI\n","llm = ChatOpenAI(temperature=0.0)"]},{"cell_type":"code","execution_count":null,"id":"43582ee6","metadata":{"id":"43582ee6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750852174381,"user_tz":-330,"elapsed":3167,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"eecdccd7-8869-4887-d788-87b40d0e286b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-21-1453707750.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n"]}],"source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n","memory.save_context({\"input\": \"How is AI!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"How is Reinforcement Learning idea\"},\n","                    {\"output\": \"Out of the box !\"})\n","memory.save_context({\"input\": \"How are Chatbots\"},\n","                    {\"output\": \"Smart !!\"})"]},{"cell_type":"code","execution_count":null,"id":"284288e1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"284288e1","outputId":"a77ace5b-809c-4f41-ea00-34282ec6a409","executionInfo":{"status":"ok","timestamp":1750852174404,"user_tz":-330,"elapsed":22,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'AI: Out of the box !\\nHuman: How are Chatbots\\nAI: Smart !!'}"]},"metadata":{},"execution_count":22}],"source":["memory.load_memory_variables({})"]},{"cell_type":"markdown","id":"5ff55d5d","metadata":{"id":"5ff55d5d"},"source":["## ConversationSummaryMemory"]},{"cell_type":"code","execution_count":null,"id":"72dcf8b1","metadata":{"id":"72dcf8b1"},"outputs":[],"source":["from langchain.memory import ConversationSummaryBufferMemory"]},{"cell_type":"code","execution_count":null,"id":"4a5b238f","metadata":{"id":"4a5b238f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746956946624,"user_tz":-330,"elapsed":971,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"2a7d8abc-957c-4345-f057-bcafec5ccc78"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-28-4b510e4b434f>:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"]}],"source":["# create a long string\n","schedule = \"You have a conference call scheduled with your marketing team at 10 am. \\\n","            Before the call, make sure you finalize the marketing report presentation. \\\n","            From 11 am to 1 pm, you have a dedicated block of time to tackle your \\\n","            data analysis project using the advanced analytics tool. At 1:30 pm, \\\n","            you're meeting a potential client for a working lunch at the downtown bistro. \\\n","            Don't forget to bring your tablet to showcase the latest project portfolio.\"\n","\n","memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n","memory.save_context({\"input\": \"Hii !!\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"All good.\"},\n","                    {\"output\": \"Great !\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"},\n","                    {\"output\": f\"{schedule}\"})"]},{"cell_type":"code","execution_count":null,"id":"2e4ecabe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2e4ecabe","outputId":"8744859e-5110-4537-9b35-1f279448f082","executionInfo":{"status":"ok","timestamp":1746956947268,"user_tz":-330,"elapsed":3,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'System: The human greets the AI and asks about the schedule for the day. The AI informs the human about a conference call with the marketing team at 10 am, data analysis project work from 11 am to 1 pm, and a meeting with a potential client at 1:30 pm. The AI reminds the human to finalize the marketing report presentation and bring a tablet to showcase the project portfolio during the client meeting.'}"]},"metadata":{},"execution_count":29}],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"6728edba","metadata":{"id":"6728edba"},"outputs":[],"source":["conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"9a221b1d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"9a221b1d","outputId":"dc6daca8-da64-4924-b4a2-16272c2fa8fd","executionInfo":{"status":"ok","timestamp":1746956952263,"user_tz":-330,"elapsed":1962,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","System: The human greets the AI and asks about the schedule for the day. The AI informs the human about a conference call with the marketing team at 10 am, data analysis project work from 11 am to 1 pm, and a meeting with a potential client at 1:30 pm. The AI reminds the human to finalize the marketing report presentation and bring a tablet to showcase the project portfolio during the client meeting.\n","Human: How to showcase a perfect project portfolio?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"To showcase a perfect project portfolio, you should include a variety of projects that highlight your skills and expertise. Make sure to include a mix of different types of projects, such as case studies, client testimonials, and before-and-after examples. Use visuals like graphs, charts, and images to make your portfolio visually appealing. Be sure to highlight key metrics and results from each project to demonstrate your success. And don't forget to tailor your portfolio to the specific needs and interests of the potential client you are meeting with.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["conversation.predict(input=\"How to showcase a perfect project portfolio?\")"]},{"cell_type":"code","execution_count":null,"id":"bb582617","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bb582617","outputId":"e3a1730a-d4ee-4764-adaf-029c3a8c0f2f","executionInfo":{"status":"ok","timestamp":1746956953272,"user_tz":-330,"elapsed":12,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'System: The human greets the AI and asks about the schedule for the day. The AI informs the human about a conference call with the marketing team at 10 am, data analysis project work from 11 am to 1 pm, and a meeting with a potential client at 1:30 pm. The AI reminds the human to finalize the marketing report presentation and bring a tablet to showcase the project portfolio during the client meeting. To showcase a perfect project portfolio, the AI advises including a variety of projects that highlight skills and expertise, using visuals like graphs and charts, and tailoring the portfolio to the specific needs of the potential client.'}"]},"metadata":{},"execution_count":32}],"source":["memory.load_memory_variables({})"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}