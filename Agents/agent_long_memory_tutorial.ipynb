{"cells":[{"cell_type":"markdown","metadata":{"id":"sxdnLVT31nfd"},"source":["# Agent Memory with Redis\n","\n","## Introduction\n","\n","Without memory, AI agents are like goldfish - they forget everything after each conversation and can't learn from past interactions or maintain context across sessions.\n","\n","Agentic systems require both **short-term** and **long-term** memory in order to complete tasks in a personalized and resilient manner.\n","\n","Memory is all about state management and **Redis** is the well-known in-memory database for exactly this kind of use case today in production systems.\n","\n","## What We'll Build\n","\n","This tutorial demonstrates how to build a **memory-enabled travel agent** with **Redis** and **LangGraph** that remembers user preferences and provides personalized recommendations. This is a **horizontal concept** that you can take and apply to your own agent use cases.\n","\n","We'll explore:\n","\n","1. Short-term memory management using LangGraph's checkpointer\n","2. Long-term memory storage and retrieval using RedisVL\n","3. Managing long-term memory as a tool for a ReAct agent\n","4. Managing conversation history size with summarization"]},{"cell_type":"markdown","metadata":{"id":"Ee3ltHdVvKOD"},"source":["# Memory architecture overview\n","\n","Our agent uses a dual-memory system:\n","- **Short-term**: Manages conversation context\n","- **Long-term**: Stores persistent knowledge\n","\n","## Short-term Memory\n","The agent tracks chat history using Redis through LangGraph's checkpointer. Each node in the graph (Retrieve Memories, Respond, Summarize) saves its state to Redis, including conversation history and thread metadata.\n","\n","To prevent context window pollution, the agent summarizes conversations when they exceed a configurable length.\n","\n","## Long-term Memory\n","\n","Long-term memories are stored & indexed in Redis using the RedisVL client, with two types:\n","- **Episodic**: User preferences and experiences\n","- **Semantic**: General travel knowledge\n","\n",">**NOTE**: These memory types align with the [CoALA](https://arxiv.org/abs/2309.02427) paper's concepts. Our agent's procedural memory is encoded in its Python workflow."]},{"cell_type":"markdown","metadata":{"id":"0KciGua91nfe"},"source":["---\n","\n","# Set up our environment\n","\n","Before diving into the code, let's set up our development environment with the right Python libraries.\n","\n",">**NOTE**: You may need to restart your kernal after installing libraries."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0zTUm35H1nfe","outputId":"9a0ed47c-4115-4bf3-a8fe-728e3bd2a3b3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759762393289,"user_tz":-330,"elapsed":21932,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-openai\n","  Downloading langchain_openai-0.3.34-py3-none-any.whl.metadata (2.4 kB)\n","Collecting langgraph-checkpoint\n","  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n","Collecting langgraph\n","  Downloading langgraph-0.6.8-py3-none-any.whl.metadata (6.8 kB)\n","Collecting langgraph-checkpoint-redis\n","  Downloading langgraph_checkpoint_redis-0.1.2-py3-none-any.whl.metadata (16 kB)\n","Collecting langchain-redis\n","  Downloading langchain_redis-0.2.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: langchain-core<2.0.0,>=0.3.77 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.3.77)\n","Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n","Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n","Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint)\n","  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m840.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n","  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n","Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n","  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.9)\n","Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint-redis) (3.11.3)\n","Collecting redis<7.0.0,>=5.2.1 (from langgraph-checkpoint-redis)\n","  Downloading redis-6.4.0-py3-none-any.whl.metadata (10 kB)\n","Collecting redisvl<1.0.0,>=0.5.1 (from langgraph-checkpoint-redis)\n","  Downloading redisvl-0.9.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: anyio<5.0.0,>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from langchain-redis) (4.11.0)\n","Requirement already satisfied: certifi<2026.0.0,>=2025.4.26 in /usr/local/lib/python3.12/dist-packages (from langchain-redis) (2025.8.3)\n","Requirement already satisfied: httpcore<2.0.0,>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from langchain-redis) (1.0.9)\n","Requirement already satisfied: jinja2<4.0.0,>=3.1.6 in /usr/local/lib/python3.12/dist-packages (from langchain-redis) (3.1.6)\n","Collecting python-ulid>=3.0.0 (from langchain-redis)\n","  Downloading python_ulid-3.1.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /usr/local/lib/python3.12/dist-packages (from langchain-redis) (4.15.0)\n","Requirement already satisfied: urllib3<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-redis) (2.5.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.9.0->langchain-redis) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.9.0->langchain-redis) (1.3.1)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore<2.0.0,>=1.0.9->langchain-redis) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.6->langchain-redis) (3.0.3)\n","Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.4.31)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.33)\n","Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (6.0.3)\n","Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (25.0)\n","Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n","Collecting jsonpath-ng>=1.5.0 (from redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis)\n","  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis) (0.5.3)\n","Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis) (2.0.2)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.77->langchain-openai) (3.0.0)\n","Requirement already satisfied: ply in /usr/local/lib/python3.12/dist-packages (from jsonpath-ng>=1.5.0->redisvl<1.0.0,>=0.5.1->langgraph-checkpoint-redis) (3.11)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.25.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.3)\n","Downloading langchain_openai-0.3.34-py3-none-any.whl (75 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph-0.6.8-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_checkpoint_redis-0.1.2-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_redis-0.2.4-py3-none-any.whl (34 kB)\n","Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n","Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_ulid-3.1.0-py3-none-any.whl (11 kB)\n","Downloading redis-6.4.0-py3-none-any.whl (279 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading redisvl-0.9.1-py3-none-any.whl (160 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n","Installing collected packages: redis, python-ulid, ormsgpack, jsonpath-ng, redisvl, langgraph-sdk, langgraph-checkpoint, langchain-redis, langchain-openai, langgraph-prebuilt, langgraph-checkpoint-redis, langgraph\n","Successfully installed jsonpath-ng-1.7.0 langchain-openai-0.3.34 langchain-redis-0.2.4 langgraph-0.6.8 langgraph-checkpoint-2.1.1 langgraph-checkpoint-redis-0.1.2 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 ormsgpack-1.10.0 python-ulid-3.1.0 redis-6.4.0 redisvl-0.9.1\n"]}],"source":["%pip install langchain-openai langgraph-checkpoint langgraph langgraph-checkpoint-redis langchain-redis"]},{"cell_type":"markdown","metadata":{"id":"8R1hEM7s1nff"},"source":["## Required API keys\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"365fzPsj1nff","outputId":"06f0e351-8225-4df4-ee61-9978ced84a86","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759762436076,"user_tz":-330,"elapsed":13607,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["OPENAI API Key:\n","··········\n"]}],"source":["import getpass\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\\n\")"]},{"cell_type":"markdown","metadata":{"id":"NLkF4GB_1nff"},"source":["## Setup Redis\n","\n","You have two options for running Redis:\n","\n","1. **Redis Cloud**: For a fully-managed, seamless experience, use a free instance of Redis Cloud.\n","2. **Local Redis**: For a simple, local (non-persistent) Redis instance, run the cell below."]},{"cell_type":"markdown","metadata":{"id":"zgKbb4ol1nff"},"source":["Run the cell below to get a localized Redis instance on your Google colab server."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xs7bi1kr1nff","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a04930b5-4fba-4b0d-9458-e973f454f7db","executionInfo":{"status":"ok","timestamp":1759762482208,"user_tz":-330,"elapsed":21060,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb jammy main\n","Starting redis-stack-server, database path /var/lib/redis-stack\n"]}],"source":["%%sh\n","curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n","echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n","sudo apt-get update  > /dev/null 2>&1\n","sudo apt-get install redis-stack-server  > /dev/null 2>&1\n","redis-stack-server --daemonize yes"]},{"cell_type":"markdown","metadata":{"id":"-B8XRKHR1nff"},"source":["Let's test out Redis connection and create a client to communicate with the server."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"dauPT3PT1nff","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bfa15f71-d15c-4027-84fe-65ec28c9908c","executionInfo":{"status":"ok","timestamp":1759762490194,"user_tz":-330,"elapsed":61,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import os\n","\n","from redis import Redis\n","\n","# Use the environment variable if set, otherwise default to localhost\n","REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n","\n","redis_client = Redis.from_url(REDIS_URL)\n","redis_client.ping()"]},{"cell_type":"markdown","metadata":{"id":"aRxYTTOf1nfg"},"source":["## Prepare memory data models\n","\n","In this section, we'll create a robust data modeling system for our agent's memory using `Pydantic`. These models will ensure type safety and provide clear data structures for storing and retrieving memories from Redis.\n","\n","We'll implement four key components:\n","\n","1. `MemoryType` - An enumeration that categorizes memories into two types:\n","   - Episodic: Personal experiences and user preferences\n","   - Semantic: General knowledge and domain facts\n","\n","2. `Memory` - The core model representing a single memory entry with its content and metadata\n","\n","3. `Memories` - A container model that holds collections of memory objects\n","\n","4. `StoredMemory` - A specialized model for memories that have been persisted to Redis\n","\n","These models work together to create a complete memory lifecycle, from creation to storage and retrieval."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Ix6Pe6qG1nfg","executionInfo":{"status":"ok","timestamp":1759763024994,"user_tz":-330,"elapsed":227,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["import ulid\n","\n","from datetime import datetime\n","from enum import Enum\n","from typing import List, Optional\n","from pydantic import BaseModel, Field\n","\n","\n","class MemoryType(str, Enum):\n","    \"\"\"\n","    Defines the type of long-term memory for categorization and retrieval.\n","\n","    EPISODIC: Personal experiences and user-specific preferences\n","              (e.g., \"User prefers Delta airlines\", \"User visited Paris last year\")\n","\n","    SEMANTIC: General domain knowledge and facts\n","              (e.g., \"Singapore requires passport\", \"Tokyo has excellent public transit\")\n","\n","    The type of a long-term memory.\n","\n","    EPISODIC: User specific experiences and preferences\n","\n","    SEMANTIC: General knowledge on top of the user's preferences and LLM's\n","    training data.\n","    \"\"\"\n","\n","    EPISODIC = \"episodic\"\n","    SEMANTIC = \"semantic\"\n","\n","\n","class Memory(BaseModel):\n","    \"\"\"Represents a single long-term memory.\"\"\"\n","\n","    content: str\n","    memory_type: MemoryType\n","    metadata: str\n","\n","\n","class Memories(BaseModel):\n","    \"\"\"\n","    A list of memories extracted from a conversation by an LLM.\n","\n","    NOTE: OpenAI's structured output requires us to wrap the list in an object.\n","    \"\"\"\n","\n","    memories: List[Memory]\n","\n","\n","class StoredMemory(Memory):\n","    \"\"\"A stored long-term memory\"\"\"\n","\n","    id: str  # The redis key\n","    memory_id: ulid.ULID = Field(default_factory=lambda: ulid.ULID())\n","    created_at: datetime = Field(default_factory=datetime.now)\n","    user_id: Optional[str] = None\n","    thread_id: Optional[str] = None\n","    memory_type: Optional[MemoryType] = None"]},{"cell_type":"markdown","metadata":{"id":"P6a03f4b1nfg"},"source":["Now we have type-safe data models that handle the complete memory lifecycle from LLM extraction to Redis storage, with proper metadata tracking for production use. Next, we'll set up the Redis infrastructure to store and search these memories using vector embeddings."]},{"cell_type":"markdown","metadata":{"id":"T0FBUdRY1nfg"},"source":["# Memory Storage\n","\n","- **Short-term memory** is handled automatically by `RedisSaver` from `langgraph-checkpoint-redis`.\n","- For **long-term memory**, we'll use RedisVL with vector embeddings to enable semantic search of past experiences and knowledge.\n","\n","Below, we will create a search index schema in Redis to hold our long term memories. The schema has a few different fields including content, memory type, metadata, timestamps, user id, memory id, and the embedding of the memory."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"D-bfk_Ro1nfg","executionInfo":{"status":"ok","timestamp":1759763029260,"user_tz":-330,"elapsed":118,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from redisvl.index import SearchIndex\n","from redisvl.schema.schema import IndexSchema\n","\n","# Define the schema for our vector search index\n","# This creates the structure for storing and querying memories\n","memory_schema = IndexSchema.from_dict({\n","        \"index\": {\n","            \"name\": \"agent_memories\",  # Index name for identification\n","            \"prefix\": \"memory\",       # Redis key prefix (memory:1, memory:2, etc.)\n","            \"key_separator\": \":\",\n","            \"storage_type\": \"json\",\n","        },\n","        \"fields\": [\n","            {\"name\": \"content\", \"type\": \"text\"},\n","            {\"name\": \"memory_type\", \"type\": \"tag\"},\n","            {\"name\": \"metadata\", \"type\": \"text\"},\n","            {\"name\": \"created_at\", \"type\": \"text\"},\n","            {\"name\": \"user_id\", \"type\": \"tag\"},\n","            {\"name\": \"memory_id\", \"type\": \"tag\"},\n","            {\n","                \"name\": \"embedding\",\n","                \"type\": \"vector\",\n","                \"attrs\": {\n","                    \"algorithm\": \"flat\",\n","                    \"dims\": 1536,  # OpenAI embedding dimension\n","                    \"distance_metric\": \"cosine\",\n","                    \"datatype\": \"float32\",\n","                },\n","            },\n","        ],\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"id":"IHUC6A6tvKOF"},"source":["Below we create the `SearchIndex` from the `IndexSchema` and our Redis client connection object. We will overwrite the index spec if its already created!"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMHgajwyvKOF","outputId":"2a41c137-4924-4748-b2e1-3620ed88aee5","executionInfo":{"status":"ok","timestamp":1759763032604,"user_tz":-330,"elapsed":45,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Long-term memory index ready\n"]}],"source":["try:\n","    long_term_memory_index = SearchIndex(\n","        schema=memory_schema,\n","        redis_client=redis_client,\n","        validate_on_load=True\n","    )\n","    long_term_memory_index.create(overwrite=True)\n","    print(\"Long-term memory index ready\")\n","except Exception as e:\n","    print(f\"Error creating index: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"q9J3oIwN24M-"},"source":["Now that the index is created, we can inspect the long term memory index in Redis using the `rvl` cli:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smnQbc5-2y_C","outputId":"3e70eb07-f1ff-410c-96a4-139bb0dd5903","executionInfo":{"status":"ok","timestamp":1759132474589,"user_tz":-330,"elapsed":662,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Index Information:\n","╭────────────────┬────────────────┬────────────────┬────────────────┬────────────────┬\b╮\n","│ Index Name     │ Storage Type   │ Prefixes       │ Index Options  │ Indexing       │\n","├────────────────┼────────────────┼────────────────┼────────────────┼────────────────┼\b┤\n","| agent_memories | JSON           | ['memory']     | []             | 0              |\n","╰────────────────┴────────────────┴────────────────┴────────────────┴────────────────┴\b╯\n","Index Fields:\n","╭─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬\b╮\n","│ Name            │ Attribute       │ Type            │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │\n","├─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼\b┤\n","│ $.content       │ content         │ TEXT            │ WEIGHT          │ 1               │                 │                 │                 │                 │                 │                 │\n","│ $.memory_type   │ memory_type     │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │\n","│ $.metadata      │ metadata        │ TEXT            │ WEIGHT          │ 1               │                 │                 │                 │                 │                 │                 │\n","│ $.created_at    │ created_at      │ TEXT            │ WEIGHT          │ 1               │                 │                 │                 │                 │                 │                 │\n","│ $.user_id       │ user_id         │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │\n","│ $.memory_id     │ memory_id       │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │\n","│ $.embedding     │ embedding       │ VECTOR          │ algorithm       │ FLAT            │ data_type       │ FLOAT32         │ dim             │ 1536            │ distance_metric │ COSINE          │\n","╰─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴\b╯\n"]}],"source":["!rvl index info -i agent_memories"]},{"cell_type":"markdown","metadata":{"id":"r5ybTN2l1nfg"},"source":["## Functions to access memories\n","\n","Next, we provide three core functions to access, store and retrieve memories. We will eventually use these in tools for the LLM to call. We will start by loading a vectorizer class to create OpenAI embeddings.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ruYpDU_lvKOF","executionInfo":{"status":"ok","timestamp":1759763044399,"user_tz":-330,"elapsed":3065,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from redisvl.utils.vectorize.text.openai import OpenAITextVectorizer\n","\n","openai_embed = OpenAITextVectorizer(model=\"text-embedding-ada-002\")"]},{"cell_type":"markdown","metadata":{"id":"HXLu70owvKOF"},"source":["Next we will set up a simple logger so our functions will record log activity of whats happening."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"-XIpiadMvKOF","executionInfo":{"status":"ok","timestamp":1759763046787,"user_tz":-330,"elapsed":4,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["import logging\n","\n","# Set up a logger\n","logger = logging.getLogger(__name__)"]},{"cell_type":"markdown","metadata":{"id":"eMBbx2MkvKOF"},"source":["### 1. Check for similar memories\n","First, we'll write a utility function to check if a memory similar to a given\n","memory already exists in the index.\n","\n","This function checks for duplicate memories in Redis by:\n","1. Converting the input content into a vector embedding\n","2. Creating filters for user_id and memory_type\n","3. Using vector similarity search with a vector range query to find any existing + similar memories\n","4. Returning True if a similar memory exists, False otherwise\n","\n","This helps prevent storing redundant information in the agent's memory."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"GN9zPAWO1nfg","executionInfo":{"status":"ok","timestamp":1759763246990,"user_tz":-330,"elapsed":46,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from redisvl.query import VectorRangeQuery\n","from redisvl.query.filter import Tag\n","\n","# If we have any memories that aren't associated with a user, we'll use this ID.\n","SYSTEM_USER_ID = \"system\"\n","\n","def similar_memory_exists(\n","    content: str,\n","    memory_type: MemoryType,\n","    user_id: str = SYSTEM_USER_ID,\n","    thread_id: Optional[str] = None,\n","    distance_threshold: float = 0.1,\n",") -> bool:\n","    \"\"\"Check if a similar long-term memory already exists in Redis.\"\"\"\n","    content_embedding = openai_embed.embed(content)\n","\n","    filters = (Tag(\"user_id\") == user_id) & (Tag(\"memory_type\") == memory_type)\n","\n","    if thread_id:\n","        filters = filters & (Tag(\"thread_id\") == thread_id)\n","\n","    # Search for similar memories\n","    vector_query = VectorRangeQuery(\n","        vector=content_embedding,\n","        num_results=1,\n","        vector_field_name=\"embedding\",\n","        filter_expression=filters,\n","        distance_threshold=distance_threshold,\n","        return_fields=[\"id\"],\n","    )\n","    results = long_term_memory_index.query(vector_query)\n","    logger.debug(f\"Similar memory search results: {results}\")\n","\n","    if results:\n","        logger.debug(\n","            f\"{len(results)} similar {'memory' if results.count == 1 else 'memories'} found. First: \"\n","            f\"{results[0]['id']}. Skipping storage.\"\n","        )\n","        return True\n","\n","    return False"]},{"cell_type":"markdown","metadata":{"id":"_zqJwlXx1nfg"},"source":["### 2. Store long-term memories"]},{"cell_type":"markdown","metadata":{"id":"KIu2CrUq1nfg"},"source":["Below is a function that handles storing long-term memories in Redis with built-in deduplication.\n","\n","It's a key part of our memory system that:\n","1. Prevents duplicate memories by checking for similar content\n","2. Creates vector embeddings for semantic search capabilities\n","3. Stores the memory with relevant metadata for future retrieval\n","\n","We'll use the `similar_memory_exists()` function when we store memories in order to perform in-line memory deduplication."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"oKA39Qp21nfh","executionInfo":{"status":"ok","timestamp":1759763594496,"user_tz":-330,"elapsed":12,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from datetime import datetime\n","from typing import List, Optional, Union\n","\n","import ulid\n","\n","def store_memory(\n","    content: str,\n","    memory_type: MemoryType,\n","    user_id: str = SYSTEM_USER_ID,\n","    thread_id: Optional[str] = None,\n","    metadata: Optional[str] = None,\n","):\n","    \"\"\"Store a long-term memory in Redis with deduplication.\n","\n","        This function:\n","        1. Checks for similar existing memories to avoid duplicates\n","        2. Generates vector embeddings for semantic search\n","        3. Stores the memory with metadata for retrieval\n","        \"\"\"\n","    if metadata is None:\n","        metadata = \"{}\"\n","\n","    logger.info(f\"Preparing to store memory: {content}\")\n","\n","    if similar_memory_exists(content, memory_type, user_id, thread_id):\n","        logger.info(\"Similar memory found, skipping storage\")\n","        return\n","\n","    embedding = openai_embed.embed(content)\n","\n","    memory_data = {\n","        \"user_id\": user_id or SYSTEM_USER_ID,\n","        \"content\": content,\n","        \"memory_type\": memory_type.value,\n","        \"metadata\": metadata,\n","        \"created_at\": datetime.now().isoformat(),\n","        \"embedding\": embedding,\n","        \"memory_id\": str(ulid.ULID()),\n","        \"thread_id\": thread_id,\n","    }\n","\n","    try:\n","        long_term_memory_index.load([memory_data])\n","    except Exception as e:\n","        logger.error(f\"Error storing memory: {e}\")\n","        return\n","\n","    logger.info(f\"Stored {memory_type} memory: {content}\")"]},{"cell_type":"markdown","metadata":{"id":"0cpk-m7Z1nfh"},"source":["### 3. Retrieve relevant long-term memories\n","And now that we're storing memories, we can retrieve them using vector similarity search with metadata filters using RedisVL.\n","\n","This function:\n","1. Takes a query string, optional filters (memory type, user ID, thread ID), and a distance threshold (semantic)\n","2. Creates a vector range query using the query's embedding\n","3. Builds a filter object based on passed options\n","4. Filters to narrow down the search results\n","4. Executes the search and returns parsed memory objects"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xuEAMNjq1nfh","executionInfo":{"status":"ok","timestamp":1759763597315,"user_tz":-330,"elapsed":11,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["def retrieve_memories(\n","    query: str,\n","    memory_type: Union[Optional[MemoryType], List[MemoryType]] = None,\n","    user_id: str = SYSTEM_USER_ID,\n","    thread_id: Optional[str] = None,\n","    distance_threshold: float = 0.1,\n","    limit: int = 5,\n",") -> List[StoredMemory]:\n","    \"\"\"Retrieve relevant memories from Redis using vector similarity search.\n","\n","    \"\"\"\n","    # Create vector query using query embedding\n","    logger.debug(f\"Retrieving memories for query: {query}\")\n","    vector_query = VectorRangeQuery(\n","        vector=openai_embed.embed(query),\n","        return_fields=[\n","            \"content\",\n","            \"memory_type\",\n","            \"metadata\",\n","            \"created_at\",\n","            \"memory_id\",\n","            \"thread_id\",\n","            \"user_id\",\n","        ],\n","        num_results=limit,\n","        vector_field_name=\"embedding\",\n","        dialect=2,\n","        distance_threshold=distance_threshold,\n","    )\n","\n","    # Build filter conditions\n","    base_filters = [f\"@user_id:{{{user_id or SYSTEM_USER_ID}}}\"]\n","\n","    if memory_type:\n","        if isinstance(memory_type, list):\n","            base_filters.append(f\"@memory_type:{{{'|'.join(memory_type)}}}\")\n","        else:\n","            base_filters.append(f\"@memory_type:{{{memory_type.value}}}\")\n","\n","    if thread_id:\n","        base_filters.append(f\"@thread_id:{{{thread_id}}}\")\n","\n","    vector_query.set_filter(\" \".join(base_filters))\n","\n","    # Execute vector similarity search\n","    results = long_term_memory_index.query(vector_query)\n","\n","    # Parse results into StoredMemory objects\n","    memories = []\n","    for doc in results:\n","        try:\n","            memory = StoredMemory(\n","                id=doc[\"id\"],\n","                memory_id=doc[\"memory_id\"],\n","                user_id=doc[\"user_id\"],\n","                thread_id=doc.get(\"thread_id\", None),\n","                memory_type=MemoryType(doc[\"memory_type\"]),\n","                content=doc[\"content\"],\n","                created_at=doc[\"created_at\"],\n","                metadata=doc[\"metadata\"],\n","            )\n","            memories.append(memory)\n","        except Exception as e:\n","            logger.error(f\"Error parsing memory: {e}\")\n","            continue\n","    return memories"]},{"cell_type":"markdown","metadata":{"id":"YinPoLcc1nfh"},"source":["## Managing Long-Term Memory with Tools\n","\n","Memory operations are exposed as **tools** that the LLM can call to store or retrieve memories.\n","\n","**Tool-based memory management:**\n","- LLM decides when to store/retrieve memories\n","- Fewer Redis calls but may miss some context\n","- Adds some latency due to LLM decision-making\n","\n","Alternatively, you can always manually manage memories in your workflows.\n","\n","**Manual memory management:**\n","- More Redis calls but faster response times\n","- Extracts more memories, providing richer context\n","- Higher token usage due to more context\n","\n","> NOTE: **This tutorial uses tool-based memory** for optimal balance of control and efficiency.\n"]},{"cell_type":"markdown","metadata":{"id":"BmwB-sUJ1nfh"},"source":["### Define Agent Tools\n","\n","Now that we have our storage functions defined, we can create the tools that will enable our agent to interact with the memory system. These tools will be used by the LLM to manage memories during conversations.\n","\n","Let's start with the Store Memory Tool:\n"]},{"cell_type":"markdown","source":["\n","#### Store Memory Tool\n","\n","This tool enables the agent to save important information as long-term memories in Redis. It's particularly useful for capturing:\n","- User preferences and habits\n","- Personal experiences and anecdotes\n","- Important facts and knowledge shared during conversations\n","\n","The tool accepts the following parameters:\n","- `content`: The actual memory content to store (e.g., \"User prefers window seats on flights\")\n","- `memory_type`: The type of memory (e.g., `MemoryType.EPISODIC` for personal experiences, `MemoryType.SEMANTIC` for general knowledge)\n","- `metadata`: Optional dictionary for additional context (e.g., timestamps, source, confidence)\n","- `config`: Optional configuration for user/thread context (automatically handled by the agent)\n","\n","When called, the tool:\n","1. Validates the input parameters\n","2. Stores the memory in Redis with proper indexing\n","3. Returns a success message with the stored content\n","4. Handles errors gracefully with informative messages\n","\n","This tool is designed to be used by the LLM to build a persistent memory of the user's preferences and experiences, enabling more personalized and context-aware interactions over time."],"metadata":{"id":"sIjXFsO0ZBpH"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"T-S0eN4B1nfh","executionInfo":{"status":"ok","timestamp":1759763893606,"user_tz":-330,"elapsed":1012,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from typing import Dict, Optional\n","\n","from langchain_core.tools import tool\n","from langchain_core.runnables.config import RunnableConfig\n","\n","@tool\n","def store_memory_tool(\n","    content: str,\n","    memory_type: MemoryType,\n","    metadata: Optional[Dict[str, str]] = None,\n","    config: Optional[RunnableConfig] = None,\n",") -> str:\n","    \"\"\"\n","    Store a long-term memory in the system.\n","\n","    Use this tool to save important information about user preferences,\n","    experiences, or general knowledge that might be useful in future\n","    interactions.\n","    \"\"\"\n","    config = config or RunnableConfig()\n","    user_id = config.get(\"user_id\", SYSTEM_USER_ID)\n","    thread_id = config.get(\"thread_id\")\n","\n","    try:\n","        # Store in long-term memory\n","        store_memory(\n","            content=content,\n","            memory_type=memory_type,\n","            user_id=user_id,\n","            thread_id=thread_id,\n","            metadata=str(metadata) if metadata else None,\n","        )\n","        return f\"Successfully stored {memory_type} memory: {content}\"\n","    except Exception as e:\n","        return f\"Error storing memory: {str(e)}\""]},{"cell_type":"markdown","metadata":{"id":"9Am1Z_hItKpc"},"source":["Test the tool:"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"H1-HPwag-im_","outputId":"61fda117-212c-48fe-bcc3-54f98ecd0806","executionInfo":{"status":"ok","timestamp":1759763914162,"user_tz":-330,"elapsed":1918,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Successfully stored MemoryType.EPISODIC memory: I like flying on Indigo when possible'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["store_memory_tool.invoke({\"content\": \"I like flying on Indigo when possible\", \"memory_type\": \"episodic\"})"]},{"cell_type":"markdown","metadata":{"id":"MkjIWht9vKOG"},"source":["Now that we've seen how to store memories, let's look at how to retrieve them.\n","\n","#### Retrieve Memoreis Tool\n","This tool allows us to search through our stored memories using semantic similarity and filtering.\n","\n","This tool is particularly useful when you want to:\n","- Find relevant past experiences or preferences\n","- Filter memories by type (episodic or semantic)\n","- Get user-specific information\n","- Limit the number of results to keep responses focused\n","\n","The tool works by:\n","1. Taking a query string and searching for semantically similar memories\n","2. Filtering results based on memory type\n","3. Applying a similarity threshold to ensure relevance\n","4. Formatting the results in a clear, readable way"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"NEqm-q1ovKOG","executionInfo":{"status":"ok","timestamp":1759764025925,"user_tz":-330,"elapsed":7,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["@tool\n","def retrieve_memories_tool(\n","    query: str,\n","    memory_type: List[MemoryType],\n","    limit: int = 5,\n","    config: Optional[RunnableConfig] = None,\n",") -> str:\n","    \"\"\"\n","    Retrieve long-term memories relevant to the query.\n","\n","    Use this tool to access previously stored information about user\n","    preferences, experiences, or general knowledge.\n","    \"\"\"\n","    config = config or RunnableConfig()\n","    user_id = config.get(\"user_id\", SYSTEM_USER_ID)\n","\n","    try:\n","        # Get long-term memories\n","        stored_memories = retrieve_memories(\n","            query=query,\n","            memory_type=memory_type,\n","            user_id=user_id,\n","            limit=limit,\n","            distance_threshold=0.3,\n","        )\n","\n","        # Format the response\n","        response = []\n","\n","        if stored_memories:\n","            response.append(\"Long-term memories:\")\n","            for memory in stored_memories:\n","                response.append(f\"- [{memory.memory_type}] {memory.content}\")\n","\n","        return \"\\n\".join(response) if response else \"No relevant memories found.\"\n","\n","    except Exception as e:\n","        return f\"Error retrieving memories: {str(e)}\""]},{"cell_type":"markdown","metadata":{"id":"4irYew3pvKON"},"source":["Test the tool:"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"CMlAHmTe9vhN","outputId":"876756df-e3a8-47f0-d471-0d85077eace6","executionInfo":{"status":"ok","timestamp":1759764556374,"user_tz":-330,"elapsed":909,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Long-term memories:\\n- [MemoryType.EPISODIC] I like flying on Indigo when possible'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["retrieve_memories_tool.invoke({\"query\": \"Airline preferences\", \"memory_type\": [\"episodic\"]})"]},{"cell_type":"markdown","metadata":{"id":"PftV2tTG1nfh"},"source":["# Build the Travel Agent\n","\n","## Setting Up the ReAct Agent\n","\n","We'll use LangGraph's prebuilt components to create a ReAct agent with memory capabilities:\n","\n","1. **Short-term Memory**: A checkpoint saver tracks conversation history per thread\n","2. **Long-term Memory**: We'll extract and store key information from conversations\n","   - Episodic memories: User preferences and experiences\n","   - Semantic memories: General travel knowledge\n","\n","The system will automatically summarize conversations to manage context while preserving important details in long-term storage.\n","\n","Below we start with setting up the Redis checkpointer (`RedisSaver`) that will handle short term memory for the agent."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"QSouau_jvKON","executionInfo":{"status":"ok","timestamp":1759764656823,"user_tz":-330,"elapsed":13696,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_core.messages import AIMessage, SystemMessage\n","from langchain_openai import ChatOpenAI\n","from langgraph.prebuilt.chat_agent_executor import create_react_agent\n","from langgraph.checkpoint.redis import RedisSaver\n","\n","# Set up the Redis checkpointer for short term memory\n","redis_saver = RedisSaver(redis_client=redis_client)\n","redis_saver.setup()"]},{"cell_type":"markdown","metadata":{"id":"a8LEro_PvKON"},"source":["Next we define the set of tools for the agent."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"EtZo92KuvKON","executionInfo":{"status":"ok","timestamp":1759764661705,"user_tz":-330,"elapsed":6,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["# Define the set of tools\n","tools = [store_memory_tool, retrieve_memories_tool]"]},{"cell_type":"markdown","metadata":{"id":"e-2IMJaLvKON"},"source":["Configure the LLM from OpenAI."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"kWz7rC5_vKON","executionInfo":{"status":"ok","timestamp":1759764670056,"user_tz":-330,"elapsed":159,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["# Configure an LLM for the agent with a more creative temperature.\n","llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7).bind_tools(tools)"]},{"cell_type":"markdown","metadata":{"id":"JLKB-V9HvKON"},"source":["Assemble the ReAct agent combining the LLM, tools, checkpointer, and system prompt!"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"e-TpYxYb1nfh","executionInfo":{"status":"ok","timestamp":1759764723164,"user_tz":-330,"elapsed":10,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["# Defint the travel agent\n","travel_agent = create_react_agent(\n","    model=llm,\n","    tools=tools,               # Long-term memory: provided as a set of custom tools\n","    checkpointer=redis_saver,  # Short-term memory: the conversation history\n","    prompt=SystemMessage(\n","        content=\"\"\"\n","        You are a travel assistant helping users plan their trips. You remember user preferences\n","        and provide personalized recommendations based on past interactions.\n","\n","        You have access to the following types of memory:\n","        1. Short-term memory: The current conversation thread\n","        2. Long-term memory:\n","           - Episodic: User preferences and past trip experiences (e.g., \"User prefers window seats\")\n","           - Semantic: General knowledge about travel destinations and requirements\n","\n","        Your procedural knowledge (how to search, book flights, etc.) is built into your tools and prompts.\n","\n","        Always be helpful, personal, and context-aware in your responses.\n","        \"\"\"\n","    ),\n",")"]},{"cell_type":"markdown","metadata":{"id":"htuJmhkY1nfi"},"source":["✅ Now that we have the basic agent in place, we will build a LangGraph workflow that invokes this agent as a node. The graph will consist of three nodes in total. We will move through each one separately."]},{"cell_type":"markdown","metadata":{"id":"R2mZwvHj1nfi"},"source":["## Node 1: Respond to the user\n","In LangGraph, a **node** represents a discrete unit of processing in a workflow. Each node is a function that takes a state object and configuration as input, processes the data, and returns an updated state. Nodes can be connected to form a directed graph that defines the flow of execution.\n","\n","The `respond_to_user` node (below) is our first node in the travel agent workflow. It serves as the entry point for user interactions and handles the core conversation flow. Here's how it works:\n","\n","1. It receives the current conversation state and configuration\n","2. Extracts any human messages from the state\n","3. Invokes our travel agent to generate a response\n","4. Handles any errors gracefully\n","5. Updates the conversation state with the agent's response\n","\n","The node uses a custom `RuntimeState` class that inherits from `MessagesState` to maintain the conversation history. This state object is passed between nodes in the graph, allowing each node to access and modify the conversation context as needed."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"PFdGi8fd1nfi","executionInfo":{"status":"ok","timestamp":1759764821233,"user_tz":-330,"elapsed":6,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_core.messages import HumanMessage\n","from langgraph.graph.message import MessagesState\n","\n","class RuntimeState(MessagesState):\n","    \"\"\"Runtime state for the travel agent.\"\"\"\n","    pass\n","\n","def respond_to_user(state: RuntimeState, config: RunnableConfig) -> RuntimeState:\n","    \"\"\"Invoke the travel agent to generate a response.\"\"\"\n","    human_messages = [m for m in state[\"messages\"] if isinstance(m, HumanMessage)]\n","    if not human_messages:\n","        logger.warning(\"No HumanMessage found in state\")\n","        return state\n","\n","    try:\n","        # Single agent invocation, not streamed (simplified for reliability)\n","        result = travel_agent.invoke({\"messages\": state[\"messages\"]}, config=config)\n","        agent_message = result[\"messages\"][-1]\n","        state[\"messages\"].append(agent_message)\n","    except Exception as e:\n","        logger.error(f\"Error invoking travel agent: {e}\")\n","        agent_message = AIMessage(\n","            content=\"I'm sorry, I encountered an error processing your request.\"\n","        )\n","        state[\"messages\"].append(agent_message)\n","\n","    return state"]},{"cell_type":"markdown","metadata":{"id":"kZyQE3MeyoQw"},"source":["## Node 2: Execute Tools\n","\n","The `execute_tools` node is a critical component in our travel agent's workflow that bridges the gap between the LLM's decisions and actual tool execution. Positioned after the `respond_to_user` node, it handles the practical side of the agent's tool-using capabilities.\n","\n","When the LLM determines it needs to use a tool, it includes tool calls in its response. This node then:\n","\n","1. Scans the conversation history to find the most recent AI message containing tool calls\n","2. For each tool call found:\n","   - Extracts the tool name, arguments, and call ID from the message\n","   - Matches the tool name against our available tools\n","   - Executes the tool with the provided arguments\n","   - Creates a ToolMessage containing the result\n","3. Handles any errors that occur during tool execution\n","4. Adds all tool results back to the conversation history\n","\n","This node is essential because it enables our agent to interact with external systems and services while maintaining a coherent conversation flow. Without it, the agent would be limited to just generating text responses without the ability to perform actual actions or retrieve real-time information.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"bkFA2_vgyrdZ","executionInfo":{"status":"ok","timestamp":1759764879852,"user_tz":-330,"elapsed":4,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_core.messages import ToolMessage\n","\n","def execute_tools(state: RuntimeState, config: RunnableConfig) -> RuntimeState:\n","    \"\"\"Execute tools specified in the latest AIMessage and append ToolMessages.\"\"\"\n","    messages = state[\"messages\"]\n","    latest_ai_message = next(\n","        (m for m in reversed(messages) if isinstance(m, AIMessage) and m.tool_calls),\n","        None\n","    )\n","\n","    if not latest_ai_message:\n","        return state  # No tool calls to process\n","\n","    tool_messages = []\n","    for tool_call in latest_ai_message.tool_calls:\n","        tool_name = tool_call[\"name\"]\n","        tool_args = tool_call[\"args\"]\n","        tool_id = tool_call[\"id\"]\n","\n","        # Find the corresponding tool\n","        tool = next((t for t in tools if t.name == tool_name), None)\n","        if not tool:\n","            continue  # Skip if tool not found\n","\n","        try:\n","            # Execute the tool with the provided arguments\n","            result = tool.invoke(tool_args, config=config)\n","            # Create a ToolMessage with the result\n","            tool_message = ToolMessage(\n","                content=str(result),\n","                tool_call_id=tool_id,\n","                name=tool_name\n","            )\n","            tool_messages.append(tool_message)\n","        except Exception as e:\n","            # Handle tool execution errors\n","            error_message = ToolMessage(\n","                content=f\"Error executing tool '{tool_name}': {str(e)}\",\n","                tool_call_id=tool_id,\n","                name=tool_name\n","            )\n","            tool_messages.append(error_message)\n","\n","    # Append the ToolMessages to the message history\n","    messages.extend(tool_messages)\n","    state[\"messages\"] = messages\n","    return state"]},{"cell_type":"markdown","metadata":{"id":"LM3oPg101nfi"},"source":["## Node 3: Conversation Summarization\n","\n","The conversation summarization node helps manage context by condensing chat history into concise summaries. This prevents the LLM's context window from being overwhelmed as the conversation grows.\n","\n","Key features:\n","\n","1. **Automatic Triggering**: Summarizes after every 6 messages (configurable)\n","\n","2. **Smart Summarization**:\n","   - Uses GPT-4o with low temperature (0.3) for consistent summaries\n","   - Preserves key information like preferences and pending items\n","   - Replaces old messages while keeping recent context\n","\n","3. **Benefits**:\n","   - Prevents context window overflow\n","   - Maintains conversation coherence\n","   - Optimizes token usage\n","\n","The summary becomes part of the conversation history, allowing the agent to reference past interactions efficiently."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"KUYw18Xb1nfi","executionInfo":{"status":"ok","timestamp":1759765004807,"user_tz":-330,"elapsed":13,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_core.messages import RemoveMessage\n","\n","# An LLM configured for summarization.\n","summarizer = ChatOpenAI(model=\"gpt-4o\", temperature=0.3)\n","\n","# The number of messages after which we'll summarize the conversation.\n","MESSAGE_SUMMARIZATION_THRESHOLD = 6\n","\n","def summarize_conversation(\n","    state: RuntimeState, config: RunnableConfig\n",") -> RuntimeState:\n","    \"\"\"\n","    Summarize a list of messages into a concise summary to reduce context length\n","    while preserving important information.\n","    \"\"\"\n","    messages = state[\"messages\"]\n","    current_message_count = len(messages)\n","    if current_message_count < MESSAGE_SUMMARIZATION_THRESHOLD:\n","        logger.debug(f\"Not summarizing conversation: {current_message_count}\")\n","        return state\n","\n","    system_prompt = \"\"\"\n","    You are a conversation summarizer. Create a concise summary of the previous\n","    conversation between a user and a travel assistant.\n","\n","    The summary should:\n","    1. Highlight key topics, preferences, and decisions\n","    2. Include any specific trip details (destinations, dates, preferences)\n","    3. Note any outstanding questions or topics that need follow-up\n","    4. Be concise but informative\n","\n","    Format your summary as a brief narrative paragraph.\n","    \"\"\"\n","\n","    message_content = \"\\n\".join(\n","        [\n","            f\"{'User' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content}\"\n","            for msg in messages\n","        ]\n","    )\n","\n","    # Invoke the summarizer\n","    summary_messages = [\n","        SystemMessage(content=system_prompt),\n","        HumanMessage(\n","            content=f\"Please summarize this conversation:\\n\\n{message_content}\"\n","        ),\n","    ]\n","\n","    summary_response = summarizer.invoke(summary_messages)\n","\n","    logger.info(f\"Summarized {len(messages)} messages into a conversation summary\")\n","\n","    summary_message = SystemMessage(\n","        content=f\"\"\"\n","        Summary of the conversation so far:\n","\n","        {summary_response.content}\n","\n","        Please continue the conversation based on this summary and the recent messages.\n","        \"\"\"\n","    )\n","    remove_messages = [\n","        RemoveMessage(id=msg.id) for msg in messages if msg.id is not None\n","    ]\n","\n","    state[\"messages\"] = [  # type: ignore\n","        *remove_messages,\n","        summary_message,\n","        state[\"messages\"][-1],\n","    ]\n","\n","    return state.copy()"]},{"cell_type":"markdown","metadata":{"id":"dpzjQxXi1nfi"},"source":["## Assemble the full graph\n","\n","It's time to assemble our graph for end-to-end agent execution. We will attach all three **nodes** we defined above."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"h6TvQaob1nfi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ed58d9b0-9c7c-4362-a7c1-4e63aa4a4f6d","executionInfo":{"status":"ok","timestamp":1759765100268,"user_tz":-330,"elapsed":7,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langgraph.graph.state.StateGraph at 0x7f3397daefc0>"]},"metadata":{},"execution_count":24}],"source":["from langgraph.graph import StateGraph, END\n","\n","workflow = StateGraph(RuntimeState)\n","\n","# Add nodes to the graph\n","workflow.add_node(\"agent\", respond_to_user)\n","workflow.add_node(\"execute_tools\", execute_tools)\n","workflow.add_node(\"summarize_conversation\", summarize_conversation)"]},{"cell_type":"markdown","metadata":{"id":"cYGE-DLuvKOO"},"source":["Next, we will tie the nodes together using **edges** which control process flow. There is a conditional edge between the agent node and what comes next. What comes next is based on whether we need to handle + execute a tool call or proceed..."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"61Un_szhvKOO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"17088f94-97c7-4a8d-fc17-69315dfc69ef","executionInfo":{"status":"ok","timestamp":1759765102134,"user_tz":-330,"elapsed":48,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langgraph.graph.state.StateGraph at 0x7f3397daefc0>"]},"metadata":{},"execution_count":25}],"source":["def decide_next_step(state):\n","    latest_ai_message = next((m for m in reversed(state[\"messages\"]) if isinstance(m, AIMessage)), None)\n","    if latest_ai_message and latest_ai_message.tool_calls:\n","        return \"execute_tools\"\n","    return \"summarize_conversation\"\n","\n","workflow.set_entry_point(\"agent\")\n","workflow.add_conditional_edges(\n","    \"agent\",\n","    decide_next_step,\n","    {\"execute_tools\": \"execute_tools\", \"summarize_conversation\": \"summarize_conversation\"},\n",")\n","workflow.add_edge(\"execute_tools\", \"agent\")\n","workflow.add_edge(\"summarize_conversation\", END)"]},{"cell_type":"markdown","metadata":{"id":"3L_OEc80vKOO"},"source":["Compile the graph!"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"kuwdsVhYvKOO","executionInfo":{"status":"ok","timestamp":1759765106107,"user_tz":-330,"elapsed":24,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["graph = workflow.compile(checkpointer=redis_saver)"]},{"cell_type":"markdown","metadata":{"id":"sQSfnQDK1nfo"},"source":[" ## Testing the Main Agent Loop\n","\n"," Let's put our travel agent to work! The main loop handles the conversation flow:\n","\n"," **Initialization**: Sets up a unique thread ID and empty message state\n","\n"," **Input Processing**: Gets user input, handles empty inputs, and exit commands  \n","\n"," **Message Flow**: Converts input to HumanMessage and streams through our workflow\n","\n","  **Response Generation**: Processes state and displays AI responses\n","\n"," **Error Handling**: Catches errors and keeps the conversation flowing smoothly\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"xD1BTjXY1nfp","executionInfo":{"status":"ok","timestamp":1759765143336,"user_tz":-330,"elapsed":8,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["def main(thread_id: str = \"book_flight\", user_id: str = \"demo_user\"):\n","    \"\"\"Main interaction loop for the travel agent\"\"\"\n","\n","    print(\"Welcome to the Travel Assistant! (Type 'exit' to quit)\")\n","\n","    config = RunnableConfig(configurable={\"thread_id\": thread_id, \"user_id\": user_id})\n","    state = RuntimeState(messages=[])\n","\n","    while True:\n","        user_input = input(\"\\nYou (type 'quit' to quit): \")\n","\n","        if not user_input:\n","            continue\n","\n","        if user_input.lower() in [\"exit\", \"quit\"]:\n","            print(\"Thank you for using the Travel Assistant. Goodbye!\")\n","            break\n","\n","        state[\"messages\"].append(HumanMessage(content=user_input))\n","\n","        try:\n","            # Process user input through the graph\n","            for result in graph.stream(state, config=config, stream_mode=\"values\"):\n","                state = RuntimeState(**result)\n","\n","            logger.debug(f\"# of messages after run: {len(state['messages'])}\")\n","\n","            # Find the most recent AI message, so we can print the response\n","            ai_messages = [m for m in state[\"messages\"] if isinstance(m, AIMessage)]\n","            if ai_messages:\n","                message = ai_messages[-1].content\n","            else:\n","                logger.error(\"No AI messages after run\")\n","                message = \"I'm sorry, I couldn't process your request properly.\"\n","                # Add the error message to the state\n","                state[\"messages\"].append(AIMessage(content=message))\n","\n","            print(f\"\\nAssistant: {message}\")\n","\n","        except Exception as e:\n","            logger.exception(f\"Error processing request: {e}\")\n","            error_message = \"I'm sorry, I encountered an error processing your request.\"\n","            print(f\"\\nAssistant: {error_message}\")\n","            # Add the error message to the state\n","            state[\"messages\"].append(AIMessage(content=error_message))"]},{"cell_type":"markdown","metadata":{"id":"P51RdhnzfZa1"},"source":["Before you try your own, take a look at the current conversation with the travel agent. Notice the memory storage actions, the calls to the LLM, and also the conversation summarization that take place during the workflow!"]},{"cell_type":"code","source":["try:\n","    user_id = input(\"Enter a user ID: \") or \"demo_user\"\n","    thread_id = input(\"Enter a thread ID: \") or \"demo_thread\"\n","except Exception:\n","    # If we're running in CI, we don't have a terminal to input from, so just exit\n","    exit()\n","else:\n","    main(thread_id, user_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bes7rJkrhXLd","executionInfo":{"status":"ok","timestamp":1759765306913,"user_tz":-330,"elapsed":123054,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"3741c85e-f183-4f29-c392-aa4127026fe8"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a user ID: test\n","Enter a thread ID: 123\n","Welcome to the Travel Assistant! (Type 'exit' to quit)\n","\n","You (type 'quit' to quit): Hi I plan to go to singapore with my wife this summer. We love outdoors activities and trying new kinds of foods. Any good recommendations?\n","\n","Assistant: Singapore is a fantastic destination for outdoor activities and culinary adventures! Given your interests, here are some recommendations:\n","\n","### Outdoor Activities:\n","1. **Gardens by the Bay**: Explore the stunning Supertree Grove and Cloud Forest. It's a must-see for nature lovers.\n","2. **Sentosa Island**: Enjoy beaches, adventure parks, and outdoor attractions like zip-lining and bungee jumping.\n","3. **MacRitchie Reservoir**: Go for a treetop walk and enjoy hiking trails around the reservoir.\n","4. **Pulau Ubin**: Rent a bike and explore this rustic island, which offers a glimpse of Singapore's past.\n","\n","### Culinary Experiences:\n","1. **Hawker Centers**: Must-visit places like Lau Pa Sat, Maxwell Food Centre, and Old Airport Road Food Centre to try local delights like Hainanese chicken rice, laksa, and satay.\n","2. **Kampong Glam**: Explore this vibrant neighborhood for Middle Eastern and Malay cuisines.\n","3. **Chinatown Food Street**: Enjoy a variety of Chinese street foods and snacks.\n","4. **Little India**: Savor authentic Indian cuisine, from biryani to dosas.\n","\n","If you need more specific recommendations or help with planning your itinerary, feel free to ask! And since you prefer flying on Indigo, you might want to check their flights for a comfortable journey.\n","\n","You (type 'quit' to quit): pure veg\n","\n","Assistant: Thanks for letting me know about your preference for vegetarian food! Here are some vegetarian-friendly recommendations for your trip to Singapore:\n","\n","### Vegetarian Culinary Experiences:\n","1. **Komala Vilas**: Located in Little India, this is a popular spot for authentic South Indian vegetarian meals.\n","2. **Gokul Vegetarian Restaurant**: Offers a wide variety of Indian, Asian, and Western vegetarian dishes.\n","3. **LingZhi Vegetarian**: Known for its innovative vegetarian Chinese cuisine, located in Orchard.\n","4. **VeganBurg**: Enjoy delicious plant-based burgers in a trendy setting.\n","5. **Teng Bespoke Vegetarian Dining**: Offers Japanese-inspired vegetarian dishes, perfect for a unique dining experience.\n","\n","### Outdoor Activities:\n","The previous recommendations for outdoor activities remain great options, as they provide a wonderful mix of nature and adventure without specific dietary constraints.\n","\n","If you need help with any other aspect of your trip or further recommendations, feel free to ask!\n","\n","You (type 'quit' to quit): quit\n","Thank you for using the Travel Assistant. Goodbye!\n"]}]},{"cell_type":"markdown","metadata":{"id":"xmFwUzVo2qxB"},"source":["Let's review what the agent learned about me during the process!"]},{"cell_type":"code","source":["res = retrieve_memories_tool.invoke({\"query\": \"Travel, activity, and dietary preferences\", \"memory_type\": [\"episodic\", \"semantic\"]})\n","res.split(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KdXIhzSOFad","executionInfo":{"status":"ok","timestamp":1759765325024,"user_tz":-330,"elapsed":541,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"1de6c0d3-1741-412a-ed27-5fd5e0039ca6"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Long-term memories:',\n"," '- [MemoryType.EPISODIC] User prefers vegetarian food.',\n"," '- [MemoryType.EPISODIC] I like flying on Indigo when possible']"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"M2UrJxte5HRT"},"source":["Don't forget, we have the RedisVL index we can use to manually query or work with as needed:"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYm3HQj54WPX","outputId":"791b0b20-0dce-4387-8c74-7de27952c561","executionInfo":{"status":"ok","timestamp":1759765333934,"user_tz":-330,"elapsed":13,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":30}],"source":["from redisvl.query import CountQuery\n","\n","# count total long-term memories in Redis\n","long_term_memory_index.query(CountQuery())"]},{"cell_type":"markdown","metadata":{"id":"recap-summary"},"source":["___\n","\n","# Recap\n","\n","You've now learned the fundamentals from scratch and built a **production-ready memory-enabled AI agent** from the ground up. Let's recap the key accomplishments:\n","\n","## 🏗️ What we built\n","\n","1. ✅ **Dual-Memory Architecture**: Short-term conversation state + long-term persistent knowledge with LangGraph and Redis\n","2. ✅ **Vector-Powered Memory**: Semantic search using RedisVL\n","3. ✅ **Smart Deduplication**: Prevents storing similar memories multiple times  \n","4. ✅ **Tool-Based Memory Management**: LLM controls when to store/retrieve memories  \n","5. ✅ **Conversation Summarization**: Automatic context window management  \n","\n","**Why Redis?**\n","\n","- **Performance**: Sub-millisecond memory retrieval at scale  \n","- **Versatility**: Handles both structured state (checkpoints) and unstructured data (vectors)  \n","- **Production-Ready**: Built-in persistence, clustering, and high availability  \n","- **Developer Experience**: Rich ecosystem with tools like RedisVL and AI framework integrations  \n","\n","**When to Use Each Approach:**\n","- **Custom Implementation** (this tutorial): Maximum control, specific requirements, learning\n","- **LangMem**: LangChain ecosystem integration, rapid prototyping\n","- **Mem0**: Multi-application memory sharing, enterprise features\n","\n","## 🔄 Next Steps\n","\n","You now have the foundation to build sophisticated, memory-enabled AI agents that feel truly intelligent and personalized.\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":0}