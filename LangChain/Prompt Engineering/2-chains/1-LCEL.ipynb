{"cells":[{"cell_type":"markdown","id":"c8067ff8-c55f-4186-afcf-029b45e7406f","metadata":{"id":"c8067ff8-c55f-4186-afcf-029b45e7406f"},"source":["# LangChain Expression Language and Chains"]},{"cell_type":"markdown","id":"28013522","metadata":{"id":"28013522"},"source":["In this notebook you will learn about LangChain runnables, and the ability to compose them into chains using LangChain Expression Language (LCEL)."]},{"cell_type":"markdown","id":"69366671-11a4-4439-b6ad-cb89497ef5d4","metadata":{"id":"69366671-11a4-4439-b6ad-cb89497ef5d4"},"source":["---"]},{"cell_type":"markdown","id":"c08054f2","metadata":{"id":"c08054f2"},"source":["## Objectives"]},{"cell_type":"markdown","id":"a023bc7a-47b5-4508-957c-f3354c9fb363","metadata":{"id":"a023bc7a-47b5-4508-957c-f3354c9fb363"},"source":["By the time you complete this notebook you will:\n","\n","- Understand LangChain runnables as units of work in LangChain.\n","- Intentionally use LLM instances and prompt templates as runnables.\n","- Create and use runnable output parsers.\n","- Compose runnables into LangChain chains using LCEL pipe syntax."]},{"cell_type":"markdown","id":"f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb","metadata":{"id":"f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb"},"source":["---"]},{"cell_type":"code","source":["!pip install groq langchain-groq grandalf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bxq6uJnvOa_p","executionInfo":{"status":"ok","timestamp":1757423235854,"user_tz":0,"elapsed":13615,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"986045d5-b3b6-478a-c8fb-0f12449a8f48","collapsed":true},"id":"bxq6uJnvOa_p","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n","Collecting langchain-groq\n","  Downloading langchain_groq-0.3.7-py3-none-any.whl.metadata (2.6 kB)\n","Collecting grandalf\n","  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.75)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from grandalf) (3.2.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.23)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.4)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.24.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n","Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_groq-0.3.7-py3-none-any.whl (16 kB)\n","Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: grandalf, groq, langchain-groq\n","Successfully installed grandalf-0.8 groq-0.31.1 langchain-groq-0.3.7\n"]}]},{"cell_type":"markdown","id":"327550d4","metadata":{"id":"327550d4"},"source":["## Imports"]},{"cell_type":"code","execution_count":2,"id":"75febe51","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75febe51","executionInfo":{"status":"ok","timestamp":1757423301929,"user_tz":0,"elapsed":16642,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"3dfd8dba-446e-42fd-dfc1-d541c6858f58"},"outputs":[{"name":"stdout","output_type":"stream","text":["GROQ API Key:\n","··········\n"]}],"source":["import os\n","import getpass\n","\n","os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key:\\n\")"]},{"cell_type":"markdown","id":"9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d","metadata":{"id":"9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d"},"source":["---"]},{"cell_type":"markdown","id":"5c0e1244","metadata":{"id":"5c0e1244"},"source":["## Create a Model Instance"]},{"cell_type":"code","execution_count":3,"id":"c2c05c82","metadata":{"id":"c2c05c82","executionInfo":{"status":"ok","timestamp":1757423312900,"user_tz":0,"elapsed":2001,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_groq import ChatGroq\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)"]},{"cell_type":"markdown","id":"64d304e5-6bbe-4beb-ba92-fa208fb4c2f4","metadata":{"id":"64d304e5-6bbe-4beb-ba92-fa208fb4c2f4"},"source":["---"]},{"cell_type":"markdown","id":"a7255020-07f1-402a-98dd-5489d153b9c6","metadata":{"id":"a7255020-07f1-402a-98dd-5489d153b9c6"},"source":["## LangChain Runnables"]},{"cell_type":"markdown","id":"84af1c42-fa16-4cfb-b6d1-d8997cbd07e8","metadata":{"id":"84af1c42-fa16-4cfb-b6d1-d8997cbd07e8"},"source":["In the previous notebook you learned to create simple LangChain prompt templates, and to instantiate them with specific values for their template placeholders with the `invoke` method."]},{"cell_type":"code","execution_count":4,"id":"70d017e6-fdc4-49e8-b261-7aad5a2c4cf4","metadata":{"id":"70d017e6-fdc4-49e8-b261-7aad5a2c4cf4","executionInfo":{"status":"ok","timestamp":1757423334924,"user_tz":0,"elapsed":49,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["template = ChatPromptTemplate.from_template(\"Answer the following question: {question}\")\n","prompt = template.invoke({\"question\": \"In what city is NVIDIA world headquarters?\"})"]},{"cell_type":"markdown","id":"fdffbbf7-9867-4989-ab88-13ec696519b7","metadata":{"id":"fdffbbf7-9867-4989-ab88-13ec696519b7"},"source":["You also know, that when sending a prompt to an LLM instance that we have created in LangChain, that we use the model instances's `invoke` method."]},{"cell_type":"code","execution_count":5,"id":"3cecc7c8-cb9f-484e-ad4b-4f96bf738d1c","metadata":{"id":"3cecc7c8-cb9f-484e-ad4b-4f96bf738d1c","executionInfo":{"status":"ok","timestamp":1757423346285,"user_tz":0,"elapsed":540,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["response = llm.invoke(prompt)"]},{"cell_type":"code","execution_count":6,"id":"b4040531-44e5-4bae-85a8-0d5a29436ee2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4040531-44e5-4bae-85a8-0d5a29436ee2","executionInfo":{"status":"ok","timestamp":1757423348700,"user_tz":0,"elapsed":21,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"58692824-79e3-4d50-8f91-3ff6839c387a"},"outputs":[{"output_type":"stream","name":"stdout","text":["The NVIDIA world headquarters is located in Santa Clara, California.\n"]}],"source":["print(response.content)"]},{"cell_type":"markdown","id":"a0f52c24-8bf1-421b-a43c-7b6f8dc71765","metadata":{"id":"a0f52c24-8bf1-421b-a43c-7b6f8dc71765"},"source":["The presence of the `invoke` method on both LLM instances, and on prompt templates is not coincidental, they are both LangChain **runnables**.\n","\n","In LangChain, a **runnable** is a unit of work that can be invoked (as we've done with both LLM instances and prompt templates), batched and streamed (as we have done with LLM instances).\n","\n","Just to sanity check this, let's try the `batch` method, which we've used with LLM instances, but not on our prompt templates. Since prompt templates are runnables, like LLM instances, and since runnables can be batched, the following ought to work."]},{"cell_type":"code","execution_count":7,"id":"21a5d236-6bad-4c8a-b0c1-d6a2243edb25","metadata":{"id":"21a5d236-6bad-4c8a-b0c1-d6a2243edb25","executionInfo":{"status":"ok","timestamp":1757423431217,"user_tz":0,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["questions = [\n","    {\"question\": \"In what city is NVIDIA world headquarters?\"},\n","    {\"question\": \"When was NVIDIA founded?\"},\n","    {\"question\": \"Who is the CEO of NVIDIA?\"},\n","]"]},{"cell_type":"code","execution_count":8,"id":"7604cf22-1d01-4872-bea0-75765f4a1969","metadata":{"id":"7604cf22-1d01-4872-bea0-75765f4a1969","executionInfo":{"status":"ok","timestamp":1757423434549,"user_tz":0,"elapsed":42,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompts = template.batch(questions)"]},{"cell_type":"code","execution_count":9,"id":"3f662654-38be-4bab-aeab-8349addb41f0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3f662654-38be-4bab-aeab-8349addb41f0","executionInfo":{"status":"ok","timestamp":1757423436361,"user_tz":0,"elapsed":41,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"7cc9e009-2561-45d2-a7ca-349f78c99677"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[ChatPromptValue(messages=[HumanMessage(content='Answer the following question: In what city is NVIDIA world headquarters?', additional_kwargs={}, response_metadata={})]),\n"," ChatPromptValue(messages=[HumanMessage(content='Answer the following question: When was NVIDIA founded?', additional_kwargs={}, response_metadata={})]),\n"," ChatPromptValue(messages=[HumanMessage(content='Answer the following question: Who is the CEO of NVIDIA?', additional_kwargs={}, response_metadata={})])]"]},"metadata":{},"execution_count":9}],"source":["prompts"]},{"cell_type":"markdown","id":"54b56ebf-bca5-4677-aa9d-3dfcdada3240","metadata":{"id":"54b56ebf-bca5-4677-aa9d-3dfcdada3240"},"source":["---"]},{"cell_type":"markdown","id":"54a8d73c-afe3-491a-a204-a7f2cc58a87b","metadata":{"id":"54a8d73c-afe3-491a-a204-a7f2cc58a87b"},"source":["## LangChain Expression Language (LCEL)"]},{"cell_type":"markdown","id":"abdb34ce-2eb9-440a-b8ca-46a2c502ea3f","metadata":{"id":"abdb34ce-2eb9-440a-b8ca-46a2c502ea3f"},"source":["LCEL is a declaritive way to compose runnables into **chains**: reusable compositions of functionality. We chain runnables together through LCEL's pipe `|` operator, which at a high level, will pipe the output of one runnable to the next.\n","\n","For those of you who have worked with the Unix command line, you'll be familar with the `|` operator as a way to chain together the functionality of various programs in service of a larger goal.\n","\n","If you don't know any Bash, don't worry too much about the following cell, but for those of you who do, you'll see we create a chain via the pipe operator to print \"hello pipes\" with `echo`, reverse the string with `rev` and then uppercase the reversed string with `tr`."]},{"cell_type":"code","execution_count":10,"id":"5f79f32a-43fe-42d3-8ef2-990d8301e1d9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5f79f32a-43fe-42d3-8ef2-990d8301e1d9","executionInfo":{"status":"ok","timestamp":1757423795546,"user_tz":0,"elapsed":48,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"7e9a9354-6c06-4154-8be6-5955f6ea2832"},"outputs":[{"output_type":"stream","name":"stdout","text":["SEPIP OLLEH\n"]}],"source":["%%bash\n","echo hello pipes | rev | tr 'a-z' 'A-Z'"]},{"cell_type":"markdown","id":"279292de-2938-4e47-9c77-4ccda357ecf2","metadata":{"id":"279292de-2938-4e47-9c77-4ccda357ecf2"},"source":["Similarly, and just as conveniently and powerfully, we can pipe together a great deal of LangChain functionality using LCEL's pipe operator."]},{"cell_type":"markdown","id":"9a41d3f0-a868-47a0-9540-eb682a2099f6","metadata":{"id":"9a41d3f0-a868-47a0-9540-eb682a2099f6"},"source":["---"]},{"cell_type":"markdown","id":"45a8546b-7394-4346-8443-142b17f8619f","metadata":{"id":"45a8546b-7394-4346-8443-142b17f8619f"},"source":["## A Simple Chain"]},{"cell_type":"markdown","id":"11e074e8-6a11-46f9-b8a5-310194a7a4ff","metadata":{"id":"11e074e8-6a11-46f9-b8a5-310194a7a4ff"},"source":["Let's begin with a simple chain, relevant to the work you've already been doing. Just to keep everything where we can see it we will define here again our LLM instance and a prompt template."]},{"cell_type":"code","execution_count":11,"id":"e27df471-5d1e-43ea-9e8e-1b22d3505a77","metadata":{"id":"e27df471-5d1e-43ea-9e8e-1b22d3505a77","executionInfo":{"status":"ok","timestamp":1757423968311,"user_tz":0,"elapsed":120,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)\n","template = ChatPromptTemplate.from_template(\"Answer the following question: {question}\")"]},{"cell_type":"markdown","id":"91b8d15d-4db4-4999-92f8-5670e0581d79","metadata":{"id":"91b8d15d-4db4-4999-92f8-5670e0581d79"},"source":["Now we'll create our first LCEL chain by composing these 2 together with a pipe. It makes sense intuitively that we'll want to utilize the prompt template first, and then send the resulting prompt to the LLM, so in our pipe we will put the template first."]},{"cell_type":"code","execution_count":12,"id":"2c27fd51-bac6-4187-a386-b7eabef0328b","metadata":{"id":"2c27fd51-bac6-4187-a386-b7eabef0328b","executionInfo":{"status":"ok","timestamp":1757423992895,"user_tz":0,"elapsed":16,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chain = template | llm"]},{"cell_type":"markdown","id":"189a6e3f-76f3-4a4f-8d6e-374f96b72f34","metadata":{"id":"189a6e3f-76f3-4a4f-8d6e-374f96b72f34"},"source":["We can visualize the computational graph represented by `chain` using the following helper method on the chain."]},{"cell_type":"code","execution_count":13,"id":"b06f85cd-cd02-425e-8110-719deb38c720","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b06f85cd-cd02-425e-8110-719deb38c720","executionInfo":{"status":"ok","timestamp":1757423998429,"user_tz":0,"elapsed":46,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"791ba8d2-0164-4ae7-8ddc-fdb81ca6f8bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["    +-------------+    \n","    | PromptInput |    \n","    +-------------+    \n","           *           \n","           *           \n","           *           \n","+--------------------+ \n","| ChatPromptTemplate | \n","+--------------------+ \n","           *           \n","           *           \n","           *           \n","     +----------+      \n","     | ChatGroq |      \n","     +----------+      \n","           *           \n","           *           \n","           *           \n","  +----------------+   \n","  | ChatGroqOutput |   \n","  +----------------+   \n"]}],"source":["print(chain.get_graph().draw_ascii())"]},{"cell_type":"markdown","id":"aaba768b-60f7-4708-9a6f-041be552fd0d","metadata":{"id":"aaba768b-60f7-4708-9a6f-041be552fd0d"},"source":["As you can see, the chain will expect a `PromptInput` which be piped into our `ChatPromptTemplate` which will then be piped into our model which will finally produce the output.\n","\n","Additionally, we can ascertain what kinds of inputs our chain expects, this time using a different helper method."]},{"cell_type":"code","execution_count":14,"id":"0c581d4d-78af-46dc-8e09-d3f895c209c7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0c581d4d-78af-46dc-8e09-d3f895c209c7","executionInfo":{"status":"ok","timestamp":1757424035826,"user_tz":0,"elapsed":14,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"97ea8d60-1fe7-4bde-99c1-002bfffcc727"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3226659032.py:1: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n","  chain.input_schema.schema()\n"]},{"output_type":"execute_result","data":{"text/plain":["{'properties': {'question': {'title': 'Question', 'type': 'string'}},\n"," 'required': ['question'],\n"," 'title': 'PromptInput',\n"," 'type': 'object'}"]},"metadata":{},"execution_count":14}],"source":["chain.input_schema.schema()"]},{"cell_type":"markdown","id":"262baa27-37ce-4eb4-8e89-3eafe878aa77","metadata":{"id":"262baa27-37ce-4eb4-8e89-3eafe878aa77"},"source":["The above is a [Pydantic](https://docs.pydantic.dev/latest/) object, which we're not going to cover in depth right now, but you'll notice immediately its `required` field states explicitly the name of any properties we are required to pass into `chain`."]},{"cell_type":"markdown","id":"844e9fb8-9e89-47f5-8f00-5b7383148f32","metadata":{"id":"844e9fb8-9e89-47f5-8f00-5b7383148f32"},"source":["Chains are composed of runnables, but are also runnables themselves. Thus, just like we would with any other runnable, we can use its `invoke` method.\n","\n","We know the beginning of our chain expects a prompt input, and that the prompt template expects us to supply a value for `question`, so we'll invoke the chain while providing the expected value."]},{"cell_type":"code","execution_count":15,"id":"fb6588bb-2886-43cd-ae0c-400f1836af4c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fb6588bb-2886-43cd-ae0c-400f1836af4c","executionInfo":{"status":"ok","timestamp":1757424346140,"user_tz":0,"elapsed":374,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"08bbf11c-8bc8-4d0a-c625-85289ecb3b92"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='NVIDIA was founded in 1993 by Jensen Huang, Chris Malachowsky, and Curtis Priem.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 44, 'total_tokens': 68, 'completion_time': 0.029951625, 'prompt_time': 0.018138718, 'queue_time': 0.210777398, 'total_time': 0.048090343}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5fe24cfc-c8a3-40cb-8fd1-623a4b4ad3db-0', usage_metadata={'input_tokens': 44, 'output_tokens': 24, 'total_tokens': 68})"]},"metadata":{},"execution_count":15}],"source":["chain.invoke({\"question\": \"Who founded NVIDIA?\"})"]},{"cell_type":"markdown","id":"d845ac6f-4553-4e9b-85ec-11b0ebbda437","metadata":{"id":"d845ac6f-4553-4e9b-85ec-11b0ebbda437"},"source":["It looks like we received a message from the model just like we have when invoking the model instance directly. Let's save the response and see if we view its `content` field as we've been able to do previously."]},{"cell_type":"code","execution_count":16,"id":"a4acdc6d-a35f-4c5d-b110-20ed1b02031b","metadata":{"id":"a4acdc6d-a35f-4c5d-b110-20ed1b02031b","executionInfo":{"status":"ok","timestamp":1757424374974,"user_tz":0,"elapsed":461,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["answer = chain.invoke({\"question\": \"Who founded NVIDIA?\"})"]},{"cell_type":"code","execution_count":17,"id":"9c5adb2f-876f-4fff-8f18-85b669f5dc4a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9c5adb2f-876f-4fff-8f18-85b669f5dc4a","executionInfo":{"status":"ok","timestamp":1757424375863,"user_tz":0,"elapsed":22,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"bd7bd03c-899e-4fd6-c0f3-91619377d32e"},"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA was founded in 1993 by Jensen Huang, Chris Malachowsky, and Curtis Priem.\n"]}],"source":["print(answer.content)"]},{"cell_type":"markdown","id":"2a70e4ce-6567-4ff2-ae4f-8cbcae1d3466","metadata":{"id":"2a70e4ce-6567-4ff2-ae4f-8cbcae1d3466"},"source":["---"]},{"cell_type":"markdown","id":"e5b3fdeb-ce2d-4892-b90e-c05b35105d4a","metadata":{"id":"e5b3fdeb-ce2d-4892-b90e-c05b35105d4a"},"source":["## Output Parsers"]},{"cell_type":"markdown","id":"f0a5b509-40ab-485f-a333-9ed25d9a1fe4","metadata":{"id":"f0a5b509-40ab-485f-a333-9ed25d9a1fe4"},"source":["Another core LangChain component is **output parsers**, which are classes to help structure LLM responses. Output parsers are, like LLM instances and prompt templates, runnables, which means we can use them in chains.\n","\n","Let's begin with perhaps the most straightforward output parser, `StrOutputParser`, which is going to save us all the repetitive boilerplate of fishing the `content` field out of our model responses.\n","\n","First we import the `StrOutputParser` class."]},{"cell_type":"code","execution_count":18,"id":"6922ac5c-2a99-417b-a921-69bbda8b5c83","metadata":{"id":"6922ac5c-2a99-417b-a921-69bbda8b5c83","executionInfo":{"status":"ok","timestamp":1757424625773,"user_tz":0,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_core.output_parsers import StrOutputParser"]},{"cell_type":"markdown","id":"f620b737-fac1-4b1d-964d-33bd4446fa1e","metadata":{"id":"f620b737-fac1-4b1d-964d-33bd4446fa1e"},"source":["Next we create an instance of the parser. For more advanced parsing techniques, some of which we'll see later, we can instantiate the parser with a variety of arguments, but for this simple parser we don't need to pass in any arguments when instantiating it."]},{"cell_type":"code","execution_count":19,"id":"2d169dfc-fce2-4677-bcc8-f6a20a7deb03","metadata":{"id":"2d169dfc-fce2-4677-bcc8-f6a20a7deb03","executionInfo":{"status":"ok","timestamp":1757424657627,"user_tz":0,"elapsed":40,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["parser = StrOutputParser()"]},{"cell_type":"markdown","id":"128e1748-4e53-45c3-9f7d-5d7b2507ad14","metadata":{"id":"128e1748-4e53-45c3-9f7d-5d7b2507ad14"},"source":["Given our claims above about all runnables having `invoke`, `batch` and `stream` methods, we would expect to be able to call them on `parser`."]},{"cell_type":"code","execution_count":20,"id":"e25a173d-ab7e-47ce-b722-8a8108389589","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"e25a173d-ab7e-47ce-b722-8a8108389589","executionInfo":{"status":"ok","timestamp":1757424686987,"user_tz":0,"elapsed":15,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"a5cada51-3d6b-4887-bb82-83a2926b334b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'parse this string'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["parser.invoke('parse this string')"]},{"cell_type":"code","execution_count":21,"id":"1c076244-834f-4a24-98de-c014c3fb9bd8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1c076244-834f-4a24-98de-c014c3fb9bd8","executionInfo":{"status":"ok","timestamp":1757424690437,"user_tz":0,"elapsed":13,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"db2ba8ed-09b1-457e-b49e-b0e20b761516"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['parse this string', 'and this string too']"]},"metadata":{},"execution_count":21}],"source":["parser.batch(['parse this string', 'and this string too'])"]},{"cell_type":"markdown","id":"0e2d7735-3884-4626-baa9-cb621c462558","metadata":{"id":"0e2d7735-3884-4626-baa9-cb621c462558"},"source":["Additionally, and most importantly, we would also expect to be able to use `parser` in a chain. Let's recreate the chain from earlier but extend it by piping the model ouput into the output parser."]},{"cell_type":"code","execution_count":22,"id":"5fc5037d-69bd-4e74-beaa-56b7496faf62","metadata":{"id":"5fc5037d-69bd-4e74-beaa-56b7496faf62","executionInfo":{"status":"ok","timestamp":1757424717936,"user_tz":0,"elapsed":43,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chain = template | llm | parser"]},{"cell_type":"markdown","id":"2bd483e1-b284-43bc-ae2a-b06371668fa9","metadata":{"id":"2bd483e1-b284-43bc-ae2a-b06371668fa9"},"source":["Again, we can visualize the computational graph represented by `chain` using the following helper method on the chain."]},{"cell_type":"code","execution_count":null,"id":"a17febaa-f8c2-479d-9751-a328d50348f2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a17febaa-f8c2-479d-9751-a328d50348f2","executionInfo":{"status":"ok","timestamp":1756779874181,"user_tz":-330,"elapsed":23,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"c3626171-c0d3-456c-d56c-ac59a33bde76"},"outputs":[{"output_type":"stream","name":"stdout","text":["     +-------------+       \n","     | PromptInput |       \n","     +-------------+       \n","            *              \n","            *              \n","            *              \n","  +--------------------+   \n","  | ChatPromptTemplate |   \n","  +--------------------+   \n","            *              \n","            *              \n","            *              \n","      +----------+         \n","      | ChatGroq |         \n","      +----------+         \n","            *              \n","            *              \n","            *              \n","   +-----------------+     \n","   | StrOutputParser |     \n","   +-----------------+     \n","            *              \n","            *              \n","            *              \n","+-----------------------+  \n","| StrOutputParserOutput |  \n","+-----------------------+  \n"]}],"source":["print(chain.get_graph().draw_ascii())"]},{"cell_type":"markdown","id":"e64fe4c1-c608-4eb6-84b2-6867e0e624cd","metadata":{"id":"e64fe4c1-c608-4eb6-84b2-6867e0e624cd"},"source":["And now let's invoke the chain, passing in the expected arguments."]},{"cell_type":"code","execution_count":23,"id":"c32470b0-65bc-4d1d-8e01-0a29887a2902","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"c32470b0-65bc-4d1d-8e01-0a29887a2902","executionInfo":{"status":"ok","timestamp":1757424752689,"user_tz":0,"elapsed":666,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"4f265a07-29ca-4d05-bd00-16075480e74c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The pipe symbol, denoted by \"|\", was first introduced in Unix systems by Doug McIlroy, an American computer scientist. He is often credited with inventing the concept of piping, which allows the output of one command to be used as the input for another command. This innovation significantly enhanced the flexibility and power of the Unix command-line interface. McIlroy\\'s work on piping dates back to the early 1970s, during the development of the first Unix operating system at Bell Labs.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}],"source":["chain.invoke({\"question\": \"Who invented the use of the pipe symbol in Unix systems?\"})"]},{"cell_type":"markdown","id":"552b14ef-5a32-4bf0-b212-a6836c72ae35","metadata":{"id":"552b14ef-5a32-4bf0-b212-a6836c72ae35"},"source":["---"]},{"cell_type":"markdown","id":"935c5c73-d0c4-468e-b59f-5ccd71ca2e44","metadata":{"id":"935c5c73-d0c4-468e-b59f-5ccd71ca2e44"},"source":["## Exercise: Translation Revisited"]},{"cell_type":"markdown","id":"1d2beea7-f91f-46ed-9977-85916f74bc28","metadata":{"id":"1d2beea7-f91f-46ed-9977-85916f74bc28"},"source":["Create a chain that is able to translate a given statement, source language, and target languages you specify.\n","\n","If you get stuck, check out the _Solution_ below."]},{"cell_type":"markdown","id":"4ce32867-d017-4610-8b53-29d35ce75915","metadata":{"id":"4ce32867-d017-4610-8b53-29d35ce75915"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"6767b5e4-faef-4273-9d3b-fdc9093873ae","metadata":{"id":"6767b5e4-faef-4273-9d3b-fdc9093873ae"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"89b76682-3b4b-4b63-a8aa-a437d65994f8","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"89b76682-3b4b-4b63-a8aa-a437d65994f8"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"d3dd817a-1525-435f-8d4a-72d4cb2f0529","metadata":{"id":"d3dd817a-1525-435f-8d4a-72d4cb2f0529"},"outputs":[],"source":["translate_template = ChatPromptTemplate.from_template(\"\"\"Translate the following statement from {from_language} to {to_language}. \\\n","Provide only the translated text: {statement}\"\"\")"]},{"cell_type":"code","execution_count":null,"id":"282648fb-0a35-429f-8400-491850b91c7a","metadata":{"id":"282648fb-0a35-429f-8400-491850b91c7a"},"outputs":[],"source":["translation_chain = translate_template | llm | parser"]},{"cell_type":"code","execution_count":null,"id":"f4efe1b4-dda1-483a-9df8-e1de42953255","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4efe1b4-dda1-483a-9df8-e1de42953255","executionInfo":{"status":"ok","timestamp":1756779893544,"user_tz":-330,"elapsed":22,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"00946f94-6a77-4ed2-bd4f-71088f106954"},"outputs":[{"output_type":"stream","name":"stdout","text":["     +-------------+       \n","     | PromptInput |       \n","     +-------------+       \n","            *              \n","            *              \n","            *              \n","  +--------------------+   \n","  | ChatPromptTemplate |   \n","  +--------------------+   \n","            *              \n","            *              \n","            *              \n","      +----------+         \n","      | ChatGroq |         \n","      +----------+         \n","            *              \n","            *              \n","            *              \n","   +-----------------+     \n","   | StrOutputParser |     \n","   +-----------------+     \n","            *              \n","            *              \n","            *              \n","+-----------------------+  \n","| StrOutputParserOutput |  \n","+-----------------------+  \n"]}],"source":["print(translation_chain.get_graph().draw_ascii())"]},{"cell_type":"code","execution_count":null,"id":"0fcba0cd-19c5-4210-96e2-fe2e4b0daed8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fcba0cd-19c5-4210-96e2-fe2e4b0daed8","executionInfo":{"status":"ok","timestamp":1756779897759,"user_tz":-330,"elapsed":28,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"2ba89fa6-490c-464e-9c55-e49771354461"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3249423841.py:1: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n","  translation_chain.input_schema.schema()\n"]},{"output_type":"execute_result","data":{"text/plain":["{'properties': {'from_language': {'title': 'From Language', 'type': 'string'},\n","  'statement': {'title': 'Statement', 'type': 'string'},\n","  'to_language': {'title': 'To Language', 'type': 'string'}},\n"," 'required': ['from_language', 'statement', 'to_language'],\n"," 'title': 'PromptInput',\n"," 'type': 'object'}"]},"metadata":{},"execution_count":30}],"source":["translation_chain.input_schema.schema()"]},{"cell_type":"code","execution_count":null,"id":"f14ca60a-9a3e-4cac-a193-f1f9c71452a2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"f14ca60a-9a3e-4cac-a193-f1f9c71452a2","executionInfo":{"status":"ok","timestamp":1756779900398,"user_tz":-330,"elapsed":372,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"637cde6e-44a9-4447-9c9d-bfe292d78004"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Egal wer du bist, es macht Spaß, neue Dinge zu lernen.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["translation_chain.invoke({\n","    \"from_language\": \"English\",\n","    \"to_language\": \"German\",\n","    \"statement\": \"No matter who you are it's fun to learn new things.\"\n","})"]},{"cell_type":"markdown","id":"db6f7b38-4ff7-402b-878f-85de44bb7bf8","metadata":{"id":"db6f7b38-4ff7-402b-878f-85de44bb7bf8"},"source":["---"]},{"cell_type":"markdown","id":"0e0180ae-c224-41ca-8111-e2e0091e123a","metadata":{"id":"0e0180ae-c224-41ca-8111-e2e0091e123a"},"source":["## Summary"]},{"cell_type":"markdown","id":"a5cb5233-a5f4-491c-9db9-ca7d5a31bb95","metadata":{"id":"a5cb5233-a5f4-491c-9db9-ca7d5a31bb95"},"source":["In this notebook you learned how to work with runnables, and in particular, 3 of the core LangChain runnables: LLM instances, prompt templates, and output parsers.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}