{"cells":[{"cell_type":"markdown","id":"485cbad4-ff67-4314-930c-91b12f926cfc","metadata":{"id":"485cbad4-ff67-4314-930c-91b12f926cfc"},"source":["# Parallel Chains"]},{"cell_type":"markdown","id":"28013522","metadata":{"id":"28013522"},"source":["In this notebook you'll learn how to create and use parallel chains."]},{"cell_type":"markdown","id":"69366671-11a4-4439-b6ad-cb89497ef5d4","metadata":{"id":"69366671-11a4-4439-b6ad-cb89497ef5d4"},"source":["---"]},{"cell_type":"markdown","id":"c08054f2","metadata":{"id":"c08054f2"},"source":["## Objectives"]},{"cell_type":"markdown","id":"a023bc7a-47b5-4508-957c-f3354c9fb363","metadata":{"id":"a023bc7a-47b5-4508-957c-f3354c9fb363"},"source":["By the time you complete this notebook you will:\n","\n","- Be able to create and use chains that execute in parallel\n","- Think about and identify opportunities for parallelism in your chains"]},{"cell_type":"markdown","id":"f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb","metadata":{"id":"f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb"},"source":["---"]},{"cell_type":"code","source":["!pip install groq langchain-groq grandalf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stDBKvk2SKKf","executionInfo":{"status":"ok","timestamp":1757428860432,"user_tz":0,"elapsed":18754,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"170e4ba3-a529-499e-c18b-ba4375d2f916"},"id":"stDBKvk2SKKf","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n","Collecting langchain-groq\n","  Downloading langchain_groq-0.3.7-py3-none-any.whl.metadata (2.6 kB)\n","Collecting grandalf\n","  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.75)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from grandalf) (3.2.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.23)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.4)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.24.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n","Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_groq-0.3.7-py3-none-any.whl (16 kB)\n","Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: grandalf, groq, langchain-groq\n","Successfully installed grandalf-0.8 groq-0.31.1 langchain-groq-0.3.7\n"]}]},{"cell_type":"markdown","id":"327550d4","metadata":{"id":"327550d4"},"source":["## Imports"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key:\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAj-KUy4Sf7n","executionInfo":{"status":"ok","timestamp":1757428867931,"user_tz":0,"elapsed":7494,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"84f01024-8434-48c8-c383-54c5736f2efa"},"id":"KAj-KUy4Sf7n","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["GROQ API Key:\n","··········\n"]}]},{"cell_type":"code","execution_count":null,"id":"7e6ed9ec-054e-4a6d-9849-836e47924658","metadata":{"id":"7e6ed9ec-054e-4a6d-9849-836e47924658"},"outputs":[],"source":["from langchain_groq import ChatGroq\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableLambda, RunnableParallel"]},{"cell_type":"markdown","id":"9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d","metadata":{"id":"9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d"},"source":["---"]},{"cell_type":"markdown","id":"5c0e1244","metadata":{"id":"5c0e1244"},"source":["## Create a Model Instance"]},{"cell_type":"code","execution_count":null,"id":"c2c05c82","metadata":{"id":"c2c05c82"},"outputs":[],"source":["llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)"]},{"cell_type":"markdown","id":"64d304e5-6bbe-4beb-ba92-fa208fb4c2f4","metadata":{"id":"64d304e5-6bbe-4beb-ba92-fa208fb4c2f4"},"source":["---"]},{"cell_type":"markdown","id":"8aadfbd8-2ee2-485c-8484-bef95aa0db11","metadata":{"id":"8aadfbd8-2ee2-485c-8484-bef95aa0db11"},"source":["## Parallel Chain Execution"]},{"cell_type":"markdown","id":"911a9777-8998-436d-9b29-be2ad80cec39","metadata":{"id":"911a9777-8998-436d-9b29-be2ad80cec39"},"source":["For the previous set of exercises you composed multiple chains to operate on a set of inputs **serially**, or in order. As a matter of fact, our task required us to do so: we needed to perform the spell and grammar checking prior to generating the additional text.\n","\n","At times, when thinking about a chain of tasks we wish to perform, we might ascertain that some of the tasks can be performed in parallel. The great news is that LCEL provides us with easy to use syntax for the parallel execution of runnables in our chains.\n","\n","We're going to begin our study of parallel chain execution with a non-LLM-related set of tasks, just to get familiar with the syntax, after which we'll apply what we've learned to create a chain that utilizes multiple LLM chains working in parallel."]},{"cell_type":"markdown","id":"d7ca3ea6-6ad7-4a3a-afc0-dc4e5d563803","metadata":{"id":"d7ca3ea6-6ad7-4a3a-afc0-dc4e5d563803"},"source":["---"]},{"cell_type":"markdown","id":"355b78b9-541e-4707-ac11-812c7c6d8fd8","metadata":{"id":"355b78b9-541e-4707-ac11-812c7c6d8fd8"},"source":["## Identifying Opportunites for Parallel Execution"]},{"cell_type":"markdown","id":"c7cc33c9-0fd1-426e-9e43-d9019b633ec1","metadata":{"id":"c7cc33c9-0fd1-426e-9e43-d9019b633ec1"},"source":["It may seem obvious, but before we can perform parallel execution, we need to identify when it is possible. You may already be well-practiced thinking about the possibility of parallel execution from work you've done in other programming settings, but if not, it's a skill you'll pick up quickly.\n","\n","In general, you just need to consider when the ouput of one process is required as the input to another. In such a case you've discovered the need for serial execution between the two process. If, however, both (or many) processes can operate without relying on the work of other processes, you likely have an opportunity to do parallel work.\n","\n","Let's construct a toy example to explore this a little further. Let's say we have a piece of text..."]},{"cell_type":"code","execution_count":null,"id":"725126cd-de26-446b-b1c1-9672c54ad12d","metadata":{"id":"725126cd-de26-446b-b1c1-9672c54ad12d"},"outputs":[],"source":["text = 'building large language models with prompt engineering'"]},{"cell_type":"markdown","id":"593fc920-50c0-4ade-afc5-45f0109c46ca","metadata":{"id":"593fc920-50c0-4ade-afc5-45f0109c46ca"},"source":["...and we would like to perform two pieces of work on it:\n","1. Title case the text\n","2. Count the number of words the text contains\n","\n","If we ask ourselves whether or not the output to either of these tasks is needed as inputs to the other, we quicky understand that the answer is \"no\". Therefore, these tasks can be performed independently and because they can be performed independently they could also be exectuted in parallel."]},{"cell_type":"markdown","id":"1f406339-b7ac-4922-8966-3a82197678e4","metadata":{"id":"1f406339-b7ac-4922-8966-3a82197678e4"},"source":["---"]},{"cell_type":"markdown","id":"c8505b64-bc91-4f28-b8ad-5fa74ee343cf","metadata":{"id":"c8505b64-bc91-4f28-b8ad-5fa74ee343cf"},"source":["## Constructing Parallel Runnables"]},{"cell_type":"markdown","id":"d0e0ed7d-e2f7-448c-98bc-a52534f9a5e2","metadata":{"id":"d0e0ed7d-e2f7-448c-98bc-a52534f9a5e2"},"source":["Understanding that for our toy problem we could perform the 2 constituent tasks in parallel, let's look at how to accomplish this with LangChain.\n","\n","To begin, let's create two runnables using `RunnableLambda` that each capture one of the sub-tasks we wish to perform.\n","\n","First we define a runnable for title casing input text."]},{"cell_type":"code","execution_count":null,"id":"874e3c8f-264f-428a-b65f-37b885b2ae37","metadata":{"id":"874e3c8f-264f-428a-b65f-37b885b2ae37"},"outputs":[],"source":["title_case = RunnableLambda(lambda text: text.title())"]},{"cell_type":"markdown","id":"56144124-7cca-447b-88ec-c453b617494b","metadata":{"id":"56144124-7cca-447b-88ec-c453b617494b"},"source":["Next we define a runnable for counting the number of words in a given text."]},{"cell_type":"code","execution_count":null,"id":"894df238-4c67-48e1-999f-9a85bb4a3ec1","metadata":{"id":"894df238-4c67-48e1-999f-9a85bb4a3ec1"},"outputs":[],"source":["count_words = RunnableLambda(lambda text: len(text.split()))"]},{"cell_type":"markdown","id":"23dd783f-aaa8-4475-bba3-70ebc0e42355","metadata":{"id":"23dd783f-aaa8-4475-bba3-70ebc0e42355"},"source":["We can invoke both runnables with our text sample and see that they behave as expected."]},{"cell_type":"code","execution_count":null,"id":"e3500024-ab44-4d4e-839c-6c5cc5ab885c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"e3500024-ab44-4d4e-839c-6c5cc5ab885c","executionInfo":{"status":"ok","timestamp":1757429038433,"user_tz":0,"elapsed":109,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"a8a84d66-d66e-4428-8133-cc38e573e710"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Building Large Language Models With Prompt Engineering'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["title = title_case.invoke(text)\n","title"]},{"cell_type":"code","execution_count":null,"id":"bd370f91-e806-4099-8c43-0d7d14b912ff","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bd370f91-e806-4099-8c43-0d7d14b912ff","executionInfo":{"status":"ok","timestamp":1757429047962,"user_tz":0,"elapsed":17,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"2173d0bc-c2bb-4ae4-9846-917e37c20f90"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":9}],"source":["word_count = count_words.invoke(text)\n","word_count"]},{"cell_type":"markdown","id":"db5a3b22-4980-494c-bdb6-9ba1a75ce6a5","metadata":{"id":"db5a3b22-4980-494c-bdb6-9ba1a75ce6a5"},"source":["If we wanted to crate a chain to execute these 2 steps serially, we could pipe them together, though care needs to be taken about the order of the serial pipe."]},{"cell_type":"code","execution_count":null,"id":"42b12519-2ad8-4b0a-ada5-fea4d27fab4f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42b12519-2ad8-4b0a-ada5-fea4d27fab4f","executionInfo":{"status":"ok","timestamp":1757429071482,"user_tz":0,"elapsed":27,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"b20d461f-0840-4f14-90b9-7b5c13390775"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":10}],"source":["serial_chain = title_case | count_words # And NOT count_words | title_case\n","serial_chain.invoke(text)"]},{"cell_type":"markdown","id":"55e980e0-a988-4b30-8d2a-dcd63ca4c1ab","metadata":{"id":"55e980e0-a988-4b30-8d2a-dcd63ca4c1ab"},"source":["In order to create a parallel chain in LCEL we can use `RunnableParallel` which expects as input a dictionary where each property is one of the runnables that we would like to execute in parallel.\n","\n","As with any Python dictionary, ours needs to contain key/value pairs. In the case of parallel chain execution the key is an arbitrary value we set, and the value is the runnable itself.\n","\n","The parallel chain will return a dictionary with the same properties we specify, with the keys we set being mapped to the results of the runnables we passed in.\n","\n","An example will clarify."]},{"cell_type":"code","execution_count":null,"id":"9b6b6c06-29a2-4f5b-812c-7dff53ddb6a9","metadata":{"id":"9b6b6c06-29a2-4f5b-812c-7dff53ddb6a9"},"outputs":[],"source":["parallel_chain = RunnableParallel({'title': title_case, 'word_count': count_words})"]},{"cell_type":"code","execution_count":null,"id":"a1609b71-63a0-41bc-9cc3-11c9a8c10356","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1609b71-63a0-41bc-9cc3-11c9a8c10356","executionInfo":{"status":"ok","timestamp":1757429165723,"user_tz":0,"elapsed":30,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"689e2c6b-24bc-4727-ea61-1816133378e0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': 'Building Large Language Models With Prompt Engineering',\n"," 'word_count': 7}"]},"metadata":{},"execution_count":12}],"source":["parallel_chain.invoke(text)"]},{"cell_type":"markdown","id":"4788f996-ddce-4fa3-ba03-16ec9986efcf","metadata":{"id":"4788f996-ddce-4fa3-ba03-16ec9986efcf"},"source":["If we take a look at the computational graph for `parallel_chain` we can see that it indicates the two runnables (`title_case` and `count_words`) are being executed in parallel."]},{"cell_type":"code","execution_count":null,"id":"e8f2dbbc-e466-462e-b031-3d1a7355e6eb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8f2dbbc-e466-462e-b031-3d1a7355e6eb","executionInfo":{"status":"ok","timestamp":1756780752355,"user_tz":-330,"elapsed":56,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"00f90708-73d3-4ce9-bfc2-4598fc19ebf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------------+  \n","| Parallel<title,word_count>Input |  \n","+---------------------------------+  \n","             *         *             \n","           **           **           \n","          *               *          \n","   +--------+          +--------+    \n","   | Lambda |          | Lambda |    \n","   +--------+          +--------+    \n","             *         *             \n","              **     **              \n","                *   *                \n","+----------------------------------+ \n","| Parallel<title,word_count>Output | \n","+----------------------------------+ \n"]}],"source":["print(parallel_chain.get_graph().draw_ascii())"]},{"cell_type":"markdown","id":"21ed3bcc-cf15-40ca-8a39-8d1709ae0022","metadata":{"id":"21ed3bcc-cf15-40ca-8a39-8d1709ae0022"},"source":["---"]},{"cell_type":"markdown","id":"4c17ad6c-8027-4def-8a96-65bb34a7c2f2","metadata":{"id":"4c17ad6c-8027-4def-8a96-65bb34a7c2f2"},"source":["## Using Parallel Outputs"]},{"cell_type":"markdown","id":"485c1842-a5be-4c7c-a89a-69f32f1ff9fa","metadata":{"id":"485c1842-a5be-4c7c-a89a-69f32f1ff9fa"},"source":["Parallel chains are runnables and therefore can be composed with other runnables.\n","\n","We should keep in mind that the output of a parallel chain is a dictionary. Some runnables, like prompt templates, expect dictionary values as their input, but others may not.\n","\n","Of course, we can construct custom runnables to handle the output of parallel runnables, if needed.\n","\n","With our current toy exercise, for example, should we wish to create a simple printout for our title-cased title and its word count, we could create a runnable that expects a dictionary as its input and uses its values to construct the output."]},{"cell_type":"code","execution_count":null,"id":"ef4a5d0c-bada-499b-98f7-601c5b6f7cc9","metadata":{"id":"ef4a5d0c-bada-499b-98f7-601c5b6f7cc9"},"outputs":[],"source":["describe_title = RunnableLambda(lambda x: f\"'{x['title']}' has {x['word_count']} words.\")"]},{"cell_type":"markdown","id":"2de4f597-3e36-4d16-ba8e-032674a90fc1","metadata":{"id":"2de4f597-3e36-4d16-ba8e-032674a90fc1"},"source":["Just to test `describe_title` out, let's invoke it with a dictionary."]},{"cell_type":"code","execution_count":null,"id":"f24c724b-d5a4-4042-907d-6f56abd1a383","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"f24c724b-d5a4-4042-907d-6f56abd1a383","executionInfo":{"status":"ok","timestamp":1757429265232,"user_tz":0,"elapsed":63,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"1e94ed51-fdfa-4212-9b9f-a8b330836f08"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"'Building Large Language Models With Prompt Engineering' has 7 words.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["describe_title.invoke({'title': title, 'word_count': word_count})"]},{"cell_type":"markdown","id":"96175f2e-73fc-42df-9417-1b3c984fa370","metadata":{"id":"96175f2e-73fc-42df-9417-1b3c984fa370"},"source":["Seeing that this works as expected, let's add it to our existing chain."]},{"cell_type":"code","execution_count":null,"id":"f745aab8-8337-4e02-a479-fbd9db7a6838","metadata":{"id":"f745aab8-8337-4e02-a479-fbd9db7a6838"},"outputs":[],"source":["final_chain = parallel_chain | describe_title"]},{"cell_type":"markdown","id":"8c05cab4-6738-4ecf-a756-3b2ecf6b5ae5","metadata":{"id":"8c05cab4-6738-4ecf-a756-3b2ecf6b5ae5"},"source":["Looking at our final chain, we can see it has been extended, and now contains parallel and serial components."]},{"cell_type":"code","execution_count":null,"id":"11eaf08c-f955-4770-b94a-c79f4ce62c2c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11eaf08c-f955-4770-b94a-c79f4ce62c2c","executionInfo":{"status":"ok","timestamp":1757429284849,"user_tz":0,"elapsed":72,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"88f4a458-3770-43ff-a15b-db786b23d6fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------------+  \n","| Parallel<title,word_count>Input |  \n","+---------------------------------+  \n","             *         *             \n","           **           **           \n","          *               *          \n","   +--------+          +--------+    \n","   | Lambda |          | Lambda |    \n","   +--------+          +--------+    \n","             *         *             \n","              **     **              \n","                *   *                \n","+----------------------------------+ \n","| Parallel<title,word_count>Output | \n","+----------------------------------+ \n","                  *                  \n","                  *                  \n","                  *                  \n","             +--------+              \n","             | Lambda |              \n","             +--------+              \n","                  *                  \n","                  *                  \n","                  *                  \n","          +--------------+           \n","          | LambdaOutput |           \n","          +--------------+           \n"]}],"source":["print(final_chain.get_graph().draw_ascii())"]},{"cell_type":"markdown","id":"70825e5f-7881-4a4e-8c59-9581cc3960af","metadata":{"id":"70825e5f-7881-4a4e-8c59-9581cc3960af"},"source":["And we can invoke it with a given title."]},{"cell_type":"code","execution_count":null,"id":"e537e1cb-62a8-4ef4-84fb-cb219d67df83","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"e537e1cb-62a8-4ef4-84fb-cb219d67df83","executionInfo":{"status":"ok","timestamp":1757429330900,"user_tz":0,"elapsed":66,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"531770b6-0a9c-4ebb-a707-8d23fa56e44d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"'Building Large Language Models With Prompt Engineering' has 7 words.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["final_chain.invoke(title)"]},{"cell_type":"markdown","id":"33fb2e5b-7c0e-440b-b5bb-cdc671db5b58","metadata":{"id":"33fb2e5b-7c0e-440b-b5bb-cdc671db5b58"},"source":["---"]},{"cell_type":"markdown","id":"5b172f09-43c0-4258-8568-23dfabe280c6","metadata":{"id":"5b172f09-43c0-4258-8568-23dfabe280c6"},"source":["## Dictionary Literal Syntax for Parallel Runnables"]},{"cell_type":"markdown","id":"a95532e2-e4ce-4247-b113-3d7f69936093","metadata":{"id":"a95532e2-e4ce-4247-b113-3d7f69936093"},"source":["As a matter of convenience, LCEL allows us to replace calls to `RunnableParallel` with simply the dictionary literal we would have passed into `RunnableParallel`.\n","\n","As an example, here's a definition of `final_chain` fully articulated, and using a call to `RunnableParallel`."]},{"cell_type":"code","execution_count":null,"id":"2b22ba20-d688-4ba2-adc9-a07e86e9f00a","metadata":{"id":"2b22ba20-d688-4ba2-adc9-a07e86e9f00a"},"outputs":[],"source":["final_chain = RunnableParallel({'title': title_case, 'word_count': count_words}) | describe_title"]},{"cell_type":"markdown","id":"0753666b-338b-432e-a1f6-f51279142f1f","metadata":{"id":"0753666b-338b-432e-a1f6-f51279142f1f"},"source":["Should we wish, we could rewrite the chain as follows, removing the call to `RunnableParallel` and leaving only the dictionary literal:"]},{"cell_type":"code","execution_count":null,"id":"5050d224-06a0-4eeb-96fd-9ef116a647c7","metadata":{"id":"5050d224-06a0-4eeb-96fd-9ef116a647c7"},"outputs":[],"source":["final_chain = {'title': title_case, 'word_count': count_words} | describe_title"]},{"cell_type":"code","execution_count":null,"id":"b4dab60b-29fe-4423-a1a9-b538bb602529","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"b4dab60b-29fe-4423-a1a9-b538bb602529","executionInfo":{"status":"ok","timestamp":1757429379149,"user_tz":0,"elapsed":73,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"e40dc935-e738-4c7e-9960-79f77c12f4c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"'Building Large Language Models With Prompt Engineering' has 7 words.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["final_chain.invoke(title)"]},{"cell_type":"markdown","id":"8b327896-3b3d-4d99-a825-7f10f374ca92","metadata":{"id":"8b327896-3b3d-4d99-a825-7f10f374ca92"},"source":["There are some gotcha's to this syntax, however. For example, if we tried to rewrite our earlier `parallel_chain`..."]},{"cell_type":"code","execution_count":null,"id":"f51cb680-dc2f-4c9f-84c2-931970a063fc","metadata":{"id":"f51cb680-dc2f-4c9f-84c2-931970a063fc"},"outputs":[],"source":["parallel_chain = RunnableParallel({'title': title_case, 'word_count': count_words})"]},{"cell_type":"markdown","id":"3a951b61-7670-4f7b-a8a5-0415ef79dceb","metadata":{"id":"3a951b61-7670-4f7b-a8a5-0415ef79dceb"},"source":["...which contains no pipe characters and is only the single parallel runnable, it would look like this."]},{"cell_type":"code","execution_count":null,"id":"4e5eaed7-9ab0-4360-addf-4fba1206deb2","metadata":{"id":"4e5eaed7-9ab0-4360-addf-4fba1206deb2"},"outputs":[],"source":["parallel_chain = {'title': title_case, 'word_count': count_words}"]},{"cell_type":"markdown","id":"ed41c87a-2449-4e2c-b1c0-269caeb68223","metadata":{"id":"ed41c87a-2449-4e2c-b1c0-269caeb68223"},"source":["This looks good and didn't throw any errors when we defined it, but if now we try to invoke it, you'll see we get an attribute error about dict objects not having invoke attributes, which is true."]},{"cell_type":"code","execution_count":null,"id":"aa55c3c4-6966-4b76-a1bf-e257f397e30e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aa55c3c4-6966-4b76-a1bf-e257f397e30e","executionInfo":{"status":"ok","timestamp":1757429401567,"user_tz":0,"elapsed":55,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"502f9fb9-24dc-4148-d168-39c9d8e0826a"},"outputs":[{"output_type":"stream","name":"stdout","text":["'dict' object has no attribute 'invoke'\n"]}],"source":["try:\n","    parallel_chain.invoke(title)\n","except AttributeError as e:\n","    print(e)"]},{"cell_type":"markdown","id":"ab2f03ee-4867-4d69-96ba-e987bd031c2e","metadata":{"id":"ab2f03ee-4867-4d69-96ba-e987bd031c2e"},"source":["The takeaway is that you're always safe using `RunnableParallel`, and even if you prefer the dict literal syntax, use `RunnableParallel` when the Python interpreter would not be able to understand that your runnable isn't simply a Python dict."]},{"cell_type":"markdown","id":"b3a043bc-81ae-4b5c-b698-54eb7a2698eb","metadata":{"id":"b3a043bc-81ae-4b5c-b698-54eb7a2698eb"},"source":["---"]},{"cell_type":"markdown","id":"40015f36-9355-4646-aae9-f1b16b40657f","metadata":{"id":"40015f36-9355-4646-aae9-f1b16b40657f"},"source":["## Exercise: Create a Chain with Parallel LLM Tasks"]},{"cell_type":"markdown","id":"99bb974a-6f95-46db-97e7-ab31a0431de5","metadata":{"id":"99bb974a-6f95-46db-97e7-ab31a0431de5"},"source":["We are going to revisit an exercise you completed several notebooks ago when we introduced you to prompt templates, but before you learned about LCEL chains.\n","\n","If you recall you were given the following list of statements..."]},{"cell_type":"code","execution_count":null,"id":"ae8917fa-b01c-455d-9a64-199c260d3e47","metadata":{"id":"ae8917fa-b01c-455d-9a64-199c260d3e47"},"outputs":[],"source":["statements = [\n","    \"I had a fantastic time hiking up the mountain yesterday.\",\n","    \"The new restaurant downtown serves delicious vegetarian dishes.\",\n","    \"I am feeling quite stressed about the upcoming project deadline.\",\n","    \"Watching the sunset at the beach was a calming experience.\",\n","    \"I recently started reading a fascinating book about space exploration.\"\n","]"]},{"cell_type":"markdown","id":"a847ed40-a63c-4f67-81d5-8d9426324ee4","metadata":{"id":"a847ed40-a63c-4f67-81d5-8d9426324ee4"},"source":["...after which you created prompts for sentiment analysis, main topic extraction, and followup question generation and ultimately created a summary output that looked like the following:"]},{"cell_type":"markdown","id":"3dffa678-ffc6-4ef0-bd87-0d3bf961000c","metadata":{"id":"3dffa678-ffc6-4ef0-bd87-0d3bf961000c"},"source":["```\n","Statement: I had a fantastic time hiking up the mountain yesterday.\n","Overall sentiment: Positive\n","Main topic: Hiking\n","Followup question: What were some of the most challenging or memorable parts of your hiking experience?\n","\n","Statement: The new restaurant downtown serves delicious vegetarian dishes.\n","Overall sentiment: Positive\n","Main topic: Vegetarian restaurants.\n","Followup question: What types of vegetarian dishes are served at the new downtown restaurant that are worth trying?\n","\n","Statement: I am feeling quite stressed about the upcoming project deadline.\n","Overall sentiment: Negative\n","Main topic: Project deadline stress\n","Followup question: How do you typically manage stress and pressure when working towards a significant deadline in a project?\n","\n","Statement: Watching the sunset at the beach was a calming experience.\n","Overall sentiment: Positive\n","Main topic: The experience of watching a sunset at the beach.\n","Followup question: What are some other activities that people often do at the beach at sunset?\n","\n","Statement: I recently started reading a fascinating book about space exploration.\n","Overall sentiment: Positive\n","Main topic: Space exploration\n","Followup question: What are some of the most significant discoveries or achievements made in the field of space exploration that the book might touch on?\n","```"]},{"cell_type":"markdown","id":"da3a53d7-ce82-4180-a4b0-dac76d13822e","metadata":{"id":"da3a53d7-ce82-4180-a4b0-dac76d13822e"},"source":["For this exercise you are going again to produce the same summary output given the same list, however, you are going to do so using chains, and especially, using parallel chains where possible."]},{"cell_type":"markdown","id":"83f46cde-3d77-47a0-b816-09dc7afc11a4","metadata":{"id":"83f46cde-3d77-47a0-b816-09dc7afc11a4"},"source":["### Runnables for Your Work"]},{"cell_type":"markdown","id":"b50fcf08-98c5-4387-bbb9-9465065864cc","metadata":{"id":"b50fcf08-98c5-4387-bbb9-9465065864cc"},"source":["To get you started, and to avoid you repeating work you've already done elsewhere, we'll provide you with several runnables that you are going to use in your solution.\n","\n","First are the 3 prompt templates needed for your LLM-related tasks."]},{"cell_type":"code","execution_count":null,"id":"87ccd15a-1c4c-4da8-a110-cb2285d4870a","metadata":{"id":"87ccd15a-1c4c-4da8-a110-cb2285d4870a"},"outputs":[],"source":["sentiment_template = ChatPromptTemplate.from_template(\"\"\"In a single word, either 'positive' or 'negative', \\\n","provide the overall sentiment of the following piece of text: {text}\"\"\")"]},{"cell_type":"code","execution_count":null,"id":"fdd3575a-bfda-4302-bffe-ccf53e999e3d","metadata":{"id":"fdd3575a-bfda-4302-bffe-ccf53e999e3d"},"outputs":[],"source":["main_topic_template = ChatPromptTemplate.from_template(\"\"\"Identify and state, as concisely as possible, the main topic \\\n","of the following piece of text. Only provide the main topic and no other helpful comments. Text: {text}\"\"\")"]},{"cell_type":"code","execution_count":null,"id":"3304c984-50a2-416c-95cc-0c75bc900664","metadata":{"id":"3304c984-50a2-416c-95cc-0c75bc900664"},"outputs":[],"source":["followup_template = ChatPromptTemplate.from_template(\"\"\"What is an appropriate and interesting followup question that would help \\\n","me learn more about the provided text? Only supply the question. Text: {text}\"\"\")"]},{"cell_type":"markdown","id":"0b992d27-1cf2-4722-825e-faf09cccf0e6","metadata":{"id":"0b992d27-1cf2-4722-825e-faf09cccf0e6"},"source":["Next is an output parser."]},{"cell_type":"code","execution_count":null,"id":"f01d049f-96a9-418e-a637-adaeab555cb0","metadata":{"id":"f01d049f-96a9-418e-a637-adaeab555cb0"},"outputs":[],"source":["parser = StrOutputParser()"]},{"cell_type":"markdown","id":"543670ca-6053-424d-bf98-28fb9ac02d49","metadata":{"id":"543670ca-6053-424d-bf98-28fb9ac02d49"},"source":["Finally is a custom runnable which expects a dictionary as input with 4 values(`statement`, `sentiment`, `main_topic`, `followup`), and produces the text output we desire for this task."]},{"cell_type":"code","execution_count":null,"id":"747ac5fb-e941-482a-8c9d-7d2e66f80dcb","metadata":{"id":"747ac5fb-e941-482a-8c9d-7d2e66f80dcb"},"outputs":[],"source":["output_formatter = RunnableLambda(lambda responses: (\n","    f\"Statement: {responses['statement']}\\n\"\n","    f\"Overall sentiment: {responses['sentiment']}\\n\"\n","    f\"Main topic: {responses['main_topic']}\\n\"\n","    f\"Followup question: {responses['followup']}\\n\"\n","))"]},{"cell_type":"markdown","id":"0ca51815-3edd-4efc-8be5-34a33b590224","metadata":{"id":"0ca51815-3edd-4efc-8be5-34a33b590224"},"source":["### Plan Your Chain(s)"]},{"cell_type":"markdown","id":"d0ec47f9-0285-4106-ba4c-4a77602bb973","metadata":{"id":"d0ec47f9-0285-4106-ba4c-4a77602bb973"},"source":["Before doing any additional coding, spend some time thinking through how you ought to compose your chain, including any sub-chains. In particular, consider where in our task we can utilize parallel execution.\n","\n","Feel free to use the cell immediately below to write down your thoughts and articulate a plan of action. When you're done compare it to our *Solution Plan* below."]},{"cell_type":"markdown","id":"cd4af23c-608d-43ab-bf26-433d10b575cb","metadata":{"id":"cd4af23c-608d-43ab-bf26-433d10b575cb"},"source":["### Your Plan Here"]},{"cell_type":"markdown","id":"d644240a-185e-4cc3-8387-6937bb248767","metadata":{"id":"d644240a-185e-4cc3-8387-6937bb248767"},"source":[]},{"cell_type":"markdown","id":"12d62145-669a-4f23-ad99-a2b83f36653a","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"12d62145-669a-4f23-ad99-a2b83f36653a"},"source":["### Solution Plan"]},{"cell_type":"markdown","id":"cf114d2b-38f4-4414-bd87-4c2a90d6586d","metadata":{"id":"cf114d2b-38f4-4414-bd87-4c2a90d6586d"},"source":["The inputs and outputs for our full chain will be:\n","\n","```\n","statements -> formatted_output\n","```\n","\n","Working backwards from `formatted_output` we know that we are going to need 4 values to provide it:\n","```\n","statements ->\n","[statement, sentiment, main_topic, followup_question] ->\n","formatted_output\n","```\n","\n","We ought to be able to capture `statement` from `statements` somehow and just pipe it through the chain, but for `sentiment`, `main_topic`, and `followup_question` these each will need to be their own LLM chain:\n","\n","```\n","statements ->\n","[\n","    statement,\n","    sentiment_template -> llm -> parser,\n","    main_topic_template -> llm -> parser,\n","    followup_question_template -> llm -> parser\n","] ->\n","formatted_output\n","```\n","\n","As an observation, it looks like everything in my outline in between the brackets `[` and `]` can be done independently of each other, so it's likely I'll be able to perform parallel execution there.\n","\n","In order to move from my 4 parallel chains, which will return a dictionary to `formatted_output` I'll need to use the `output_formatter` runnable defined above.\n","\n","```\n","statements ->\n","[\n","    statement,\n","    sentiment_template -> llm -> parser,\n","    main_topic_template -> llm -> parser,\n","    followup_question_template -> llm -> parser\n","] ->\n","output_formatter ->\n","formatted_output\n","```\n","\n","As a final detail, I need to prepare my inputs (`statments`), which are currently strings, for use in the prompt templates, which each expect a dictionary with a `text` property.\n","\n","```\n","statements ->\n","prep_statements_for_templates ->\n","[\n","    statement,\n","    sentiment_template -> llm -> parser,\n","    main_topic_template -> llm -> parser,\n","    followup_question_template -> llm -> parser\n","] ->\n","output_formatter ->\n","formatted_output\n","```\n","\n","This looks like a good description and with it I'm ready to plan my next steps forward:\n","\n","\n","1) Create a custom runnable to convert my string inputs to dictionaries with a text field for use by the prompt templates.\n","2) Create 1 chain each for sentiment analysis, main topic extraction, and followup question generation (3 chains total)\n","3) Create a parallel chain with the 3 chains you just created, plus one more runnable (a custom runnable) that captures the passed-in text under the key `statement` (as is expected by the output formatter).\n","4) Chain together the `prep_for_inputs` runnable, the parallel chain, and the `output_formatter` runnable.\n","5) Use the full chain to batch process `statements`."]},{"cell_type":"markdown","id":"36a1335a-1b72-4169-bcdc-06565aed73de","metadata":{"id":"36a1335a-1b72-4169-bcdc-06565aed73de"},"source":["### Your Work Here"]},{"cell_type":"markdown","id":"a3e82bc9-891e-4f13-88eb-ae210837ec4b","metadata":{"id":"a3e82bc9-891e-4f13-88eb-ae210837ec4b"},"source":["If you're up for it, jump right in and implement the desired functionality.\n","\n","If you'd like to work through the exercise with some step-by-step guidance, open up the *Walkthrough* section immediately below."]},{"cell_type":"code","execution_count":null,"id":"473f9062-461f-43ff-820d-6b95b1e09ea7","metadata":{"id":"473f9062-461f-43ff-820d-6b95b1e09ea7"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"953fcde0-c354-4fd5-8aea-8af8dd0b5d3a","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"953fcde0-c354-4fd5-8aea-8af8dd0b5d3a"},"source":["## Walkthrough"]},{"cell_type":"markdown","id":"75eaa1e3-ba6d-4622-86be-d1ab4db78d43","metadata":{"id":"75eaa1e3-ba6d-4622-86be-d1ab4db78d43"},"source":["In this walkthrough we'll follow the course of action laid out in the *Solution Plan* above, namely:\n","\n","1) Create a custom runnable to convert your string inputs to dictionaries with a `text` field for use by the prompt templates.\n","2) Create 1 chain each for sentiment analysis, main topic extraction, and followup question generation (3 chains total)\n","3) Create a parallel chain with the 3 chains you just created, plus one more runnable (a custom runnable) that captures the passed-in text under the key `statement` (as is expected by the output formatter).\n","4) Chain together the `prep_for_inputs` runnable, the parallel chain, and the `output_formatter` runnable.\n","5) Use the full chain to batch process `statements`."]},{"cell_type":"markdown","id":"68864983-4ef8-475d-882a-39b23b2625b6","metadata":{"id":"68864983-4ef8-475d-882a-39b23b2625b6"},"source":["### Prepare Inpute for Prompt Templates"]},{"cell_type":"markdown","id":"bfd14c81-eaa3-4c8a-b1f4-23f2d07a9a5e","metadata":{"id":"bfd14c81-eaa3-4c8a-b1f4-23f2d07a9a5e"},"source":["Create a custom runnable to convert your string inputs to dictionaries with a `text` field for use by the prompt templates.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"3ec1949f-7602-4645-b34d-e85f6abea6d2","metadata":{"id":"3ec1949f-7602-4645-b34d-e85f6abea6d2"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"bdea2068-66a4-443c-a80a-2c1d187654d8","metadata":{"id":"bdea2068-66a4-443c-a80a-2c1d187654d8"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ef87ad0a-0e6d-4d2e-b505-228e61782d80","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"ef87ad0a-0e6d-4d2e-b505-228e61782d80"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"5c320ba9-ada4-4312-8a35-17b7ed58ebb2","metadata":{"id":"5c320ba9-ada4-4312-8a35-17b7ed58ebb2"},"outputs":[],"source":["prep_for_template = RunnableLambda(lambda text: {\"text\": text})"]},{"cell_type":"markdown","id":"bbbb657d-7a74-4b66-af76-55d62bf1dffc","metadata":{"id":"bbbb657d-7a74-4b66-af76-55d62bf1dffc"},"source":["### Create LLM Chains"]},{"cell_type":"markdown","id":"5f3badcd-3887-4365-9cf4-e57c34c76210","metadata":{"id":"5f3badcd-3887-4365-9cf4-e57c34c76210"},"source":["Create 1 chain each for sentiment analysis, main topic extraction, and followup question generation (3 chains total). Each chain should use the appropriate prompt template, along with our LLM instance and output parser.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"d5b01397-b46f-424d-a858-2e9f20ebd68c","metadata":{"id":"d5b01397-b46f-424d-a858-2e9f20ebd68c"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"74fbbde7-613e-46dd-9ab6-28d07cf6f9ee","metadata":{"id":"74fbbde7-613e-46dd-9ab6-28d07cf6f9ee"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e0756f5c-b652-4234-91d9-bcdc734be320","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"e0756f5c-b652-4234-91d9-bcdc734be320"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"317f4c98-7420-4d46-94f2-93d2287e3794","metadata":{"id":"317f4c98-7420-4d46-94f2-93d2287e3794"},"outputs":[],"source":["sentiment_chain = sentiment_template | llm | parser\n","main_topic_chain = main_topic_template | llm | parser\n","followup_chain = followup_template | llm | parser"]},{"cell_type":"markdown","id":"c1179cab-aeda-46f7-9691-228e41390980","metadata":{"id":"c1179cab-aeda-46f7-9691-228e41390980"},"source":["### Create a Parallel Chain"]},{"cell_type":"markdown","id":"6f215a11-9173-4e02-9a64-ba36925982af","metadata":{"id":"6f215a11-9173-4e02-9a64-ba36925982af"},"source":["Create a parallel chain with the 3 chains you just created, plus one more runnable (a custom runnable) that captures the passed-in text under the key `statement` (as is expected by the output formatter).\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"a1c23bbd-045d-4bf1-8bf9-7195c0165f47","metadata":{"id":"a1c23bbd-045d-4bf1-8bf9-7195c0165f47"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"9a3f5a94-c4c9-41e3-a398-ba3122828d8d","metadata":{"id":"9a3f5a94-c4c9-41e3-a398-ba3122828d8d"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"6762dad3-cc1d-4886-af81-95a847284847","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"6762dad3-cc1d-4886-af81-95a847284847"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"715a2bce-8a4e-4a22-b3c2-b155ebe53c86","metadata":{"id":"715a2bce-8a4e-4a22-b3c2-b155ebe53c86"},"outputs":[],"source":["parallel_chain = RunnableParallel({\n","    \"sentiment\": sentiment_chain,\n","    \"main_topic\": main_topic_chain,\n","    \"followup\": followup_chain,\n","    \"statement\": RunnableLambda(lambda x: x['text'])\n","})"]},{"cell_type":"markdown","id":"28f30171-42ed-4ee7-8c43-b22402876390","metadata":{"id":"28f30171-42ed-4ee7-8c43-b22402876390"},"source":["### Compose Main Chain Out of Sub-chains"]},{"cell_type":"markdown","id":"c1d4c03e-6ab2-4666-86a2-32da020a1331","metadata":{"id":"c1d4c03e-6ab2-4666-86a2-32da020a1331"},"source":["Chain together the `prep_for_inputs` runnable, the parallel chain, and the `output_formatter` runnable.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"6ceda6fa-6e30-48fa-9fff-d4635735672d","metadata":{"id":"6ceda6fa-6e30-48fa-9fff-d4635735672d"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"7bb50473-cb16-4549-91c3-76446bac5d95","metadata":{"id":"7bb50473-cb16-4549-91c3-76446bac5d95"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"f5ea2a4a-0083-4206-bdde-cb6b7f96524d","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"f5ea2a4a-0083-4206-bdde-cb6b7f96524d"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"545f1951-ff37-4f6a-baec-baabb1ad5316","metadata":{"id":"545f1951-ff37-4f6a-baec-baabb1ad5316"},"outputs":[],"source":["chain = prep_for_template | parallel_chain | output_formatter"]},{"cell_type":"code","execution_count":null,"id":"81389788-c7ac-48c1-8a93-8a1104740e40","metadata":{"id":"81389788-c7ac-48c1-8a93-8a1104740e40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757429830374,"user_tz":0,"elapsed":70,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"07deaab6-acd3-46b8-f236-4bf6f0015453"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   +-------------+                                               \n","                                                   | LambdaInput |                                               \n","                                                   +-------------+                                               \n","                                                          *                                                      \n","                                                          *                                                      \n","                                                          *                                                      \n","                                                     +--------+                                                  \n","                                                     | Lambda |                                                  \n","                                                     +--------+                                                  \n","                                                          *                                                      \n","                                                          *                                                      \n","                                                          *                                                      \n","                             +--------------------------------------------------------+                          \n","                             | Parallel<sentiment,main_topic,followup,statement>Input |                          \n","                             +--------------------------------------------------------+                          \n","                               ********           ***             **          *********                          \n","                       ********                 **                  ***                *******                   \n","                   ****                       **                       **                     *********          \n","+--------------------+          +--------------------+          +--------------------+                 ****      \n","| ChatPromptTemplate |          | ChatPromptTemplate |          | ChatPromptTemplate |                    *      \n","+--------------------+          +--------------------+          +--------------------+                    *      \n","           *                               *                               *                              *      \n","           *                               *                               *                              *      \n","           *                               *                               *                              *      \n","     +----------+                    +----------+                    +----------+                         *      \n","     | ChatGroq |                    | ChatGroq |                    | ChatGroq |                         *      \n","     +----------+                    +----------+                    +----------+                         *      \n","           *                               *                               *                              *      \n","           *                               *                               *                              *      \n","           *                               *                               *                              *      \n","  +-----------------+             +-----------------+             +-----------------+               +--------+   \n","  | StrOutputParser |**           | StrOutputParser |             | StrOutputParser |           ****| Lambda |   \n","  +-----------------+  ********   +-----------------+             +-----------------+   ********    +--------+   \n","                               ********           ***             **           *********                         \n","                                       ********      **        ***     ********                                  \n","                                               ****    **    **   *****                                          \n","                             +---------------------------------------------------------+                         \n","                             | Parallel<sentiment,main_topic,followup,statement>Output |                         \n","                             +---------------------------------------------------------+                         \n","                                                          *                                                      \n","                                                          *                                                      \n","                                                          *                                                      \n","                                                     +--------+                                                  \n","                                                     | Lambda |                                                  \n","                                                     +--------+                                                  \n","                                                          *                                                      \n","                                                          *                                                      \n","                                                          *                                                      \n","                                                  +--------------+                                               \n","                                                  | LambdaOutput |                                               \n","                                                  +--------------+                                               \n"]}],"source":["print(chain.get_graph().draw_ascii())"]},{"cell_type":"markdown","id":"201a2a0e-fa83-4169-9133-85d72426450b","metadata":{"id":"201a2a0e-fa83-4169-9133-85d72426450b"},"source":["### Execute the Chain"]},{"cell_type":"markdown","id":"4bd5e6f5-177f-4499-b323-cfaa5f2448d6","metadata":{"id":"4bd5e6f5-177f-4499-b323-cfaa5f2448d6"},"source":["Use the full chain to batch process `statements`.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"e82cfbf4-b7fd-4fcb-afb2-4ba874425cc0","metadata":{"id":"e82cfbf4-b7fd-4fcb-afb2-4ba874425cc0"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"1e964459-2cc0-4f42-8200-e0acda41f402","metadata":{"id":"1e964459-2cc0-4f42-8200-e0acda41f402"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"d05108d0-37dd-4d98-bc5c-33201ab221c9","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"d05108d0-37dd-4d98-bc5c-33201ab221c9"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"c6f813fb-af6c-4b23-ae29-7b65d39559d4","metadata":{"id":"c6f813fb-af6c-4b23-ae29-7b65d39559d4"},"outputs":[],"source":["formatted_outputs = chain.batch(statements)"]},{"cell_type":"code","execution_count":null,"id":"5d1c657a-e316-4127-b261-94f446fe8af3","metadata":{"id":"5d1c657a-e316-4127-b261-94f446fe8af3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757429849735,"user_tz":0,"elapsed":37,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"2428edb1-6026-4f70-8154-0100f97c458a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Statement: I had a fantastic time hiking up the mountain yesterday.\n","Overall sentiment: Positive.\n","Main topic: Hiking\n","Followup question: What made the hike so fantastic, was it the scenery, the challenge, or something else?\n","\n","Statement: The new restaurant downtown serves delicious vegetarian dishes.\n","Overall sentiment: Positive.\n","Main topic: New restaurant downtown\n","Followup question: What inspired the chef to create a menu focused on vegetarian cuisine?\n","\n","Statement: I am feeling quite stressed about the upcoming project deadline.\n","Overall sentiment: Negative.\n","Main topic: Project deadline stress.\n","Followup question: What specific aspects of the project are causing you the most stress and anxiety?\n","\n","Statement: Watching the sunset at the beach was a calming experience.\n","Overall sentiment: Positive.\n","Main topic: Beach sunset experience\n","Followup question: What specific aspects of the beach at sunset contributed to the calming atmosphere?\n","\n","Statement: I recently started reading a fascinating book about space exploration.\n","Overall sentiment: Positive.\n","Main topic: Space exploration\n","Followup question: What sparked your interest in reading about space exploration, and what specific aspects of the book have you found most fascinating so far?\n","\n"]}],"source":["for output in formatted_outputs:\n","    print(output)"]},{"cell_type":"markdown","id":"3e6813c3-1072-4e23-8835-d5e9bebbad4c","metadata":{"id":"3e6813c3-1072-4e23-8835-d5e9bebbad4c"},"source":["---"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}