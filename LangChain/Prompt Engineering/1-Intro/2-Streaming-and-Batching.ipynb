{"cells":[{"cell_type":"markdown","id":"b7704054-b474-43e4-9d86-dc1dc779d6bd","metadata":{"id":"b7704054-b474-43e4-9d86-dc1dc779d6bd"},"source":["# Streaming and Batching"]},{"cell_type":"markdown","id":"28013522","metadata":{"id":"28013522"},"source":["In this notebook you'll learn how to stream model responses and handle multiple chat completion requests in batches."]},{"cell_type":"markdown","id":"ea5a70fb-0429-4036-82ce-a55c4262561a","metadata":{"id":"ea5a70fb-0429-4036-82ce-a55c4262561a"},"source":["---"]},{"cell_type":"markdown","id":"c08054f2","metadata":{"id":"c08054f2"},"source":["## Objectives"]},{"cell_type":"markdown","id":"a023bc7a-47b5-4508-957c-f3354c9fb363","metadata":{"id":"a023bc7a-47b5-4508-957c-f3354c9fb363"},"source":["By the time you complete this notebook, you will:\n","\n","- Learn to stream model responses.\n","- Learn to batch model responses.\n","- Compare the performance of batch processing to single prompt chat completion."]},{"cell_type":"markdown","id":"500b0fab-b9e3-4de9-bc46-5f31ab9ea623","metadata":{"id":"500b0fab-b9e3-4de9-bc46-5f31ab9ea623"},"source":["---"]},{"cell_type":"markdown","id":"327550d4","metadata":{"id":"327550d4"},"source":["## Imports"]},{"cell_type":"code","source":["!pip install groq langchain-groq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"112nK0DRn7Pw","executionInfo":{"status":"ok","timestamp":1757339700161,"user_tz":0,"elapsed":13053,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"b8b8bf08-6497-4e4a-ec03-3ef4e588fcee","collapsed":true},"id":"112nK0DRn7Pw","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n","Collecting langchain-groq\n","  Downloading langchain_groq-0.3.7-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.75)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.23)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.4)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.24.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n","Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_groq-0.3.7-py3-none-any.whl (16 kB)\n","Installing collected packages: groq, langchain-groq\n","Successfully installed groq-0.31.1 langchain-groq-0.3.7\n"]}]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key:\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42E2DiDhoBkc","executionInfo":{"status":"ok","timestamp":1757339747827,"user_tz":0,"elapsed":8133,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"02e0f2a6-ed14-4a89-fdfd-124cb92da240"},"id":"42E2DiDhoBkc","execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["GROQ API Key:\n","··········\n"]}]},{"cell_type":"markdown","id":"b2e2f950-1450-4f55-a4b3-ed2fbc987513","metadata":{"id":"b2e2f950-1450-4f55-a4b3-ed2fbc987513"},"source":["## Create a Model Instance"]},{"cell_type":"code","source":["from langchain_groq import ChatGroq\n","llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.7)"],"metadata":{"id":"s2TpQEsuoH0l","executionInfo":{"status":"ok","timestamp":1757339754713,"user_tz":0,"elapsed":2188,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"id":"s2TpQEsuoH0l","execution_count":3,"outputs":[]},{"cell_type":"markdown","id":"b5b750bb-14bb-43e9-ba0c-a631f116bf0d","metadata":{"id":"b5b750bb-14bb-43e9-ba0c-a631f116bf0d"},"source":["## Sanity Check"]},{"cell_type":"markdown","id":"b0bfb697-b408-4f6d-8481-099c648097b3","metadata":{"id":"b0bfb697-b408-4f6d-8481-099c648097b3"},"source":["Before proceeding with new use cases, let's sanity check that we can interact with our local model via LangChain."]},{"cell_type":"code","execution_count":4,"id":"55d2e2a3-63bd-4b9b-93ac-dbba2830947f","metadata":{"id":"55d2e2a3-63bd-4b9b-93ac-dbba2830947f","executionInfo":{"status":"ok","timestamp":1757339764949,"user_tz":0,"elapsed":525,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompt = 'Where and when was NVIDIA founded?'\n","result = llm.invoke(prompt)"]},{"cell_type":"code","execution_count":5,"id":"6fbd103e-86a3-4992-bda4-bf221a0a4fb3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fbd103e-86a3-4992-bda4-bf221a0a4fb3","executionInfo":{"status":"ok","timestamp":1757339770066,"user_tz":0,"elapsed":31,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"5e39825e-82db-4ec2-86a3-d9897601f1f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA was founded on April 5, 1993, in Santa Clara, California, USA. It was founded by Jensen Huang, Chris Malachowsky, and Curtis Priem.\n"]}],"source":["print(result.content)"]},{"cell_type":"markdown","id":"2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840","metadata":{"id":"2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840"},"source":["---"]},{"cell_type":"markdown","id":"2d330698-7bff-470f-9f7e-c6e8411fd6fe","metadata":{"id":"2d330698-7bff-470f-9f7e-c6e8411fd6fe"},"source":["## Streaming Responses"]},{"cell_type":"markdown","id":"c684ba98-30d5-4d63-97c1-6402f7d2e5ae","metadata":{"id":"c684ba98-30d5-4d63-97c1-6402f7d2e5ae"},"source":["As an alternative to the `invoke` method, you can use the `stream` method to receive the model response in chunks. This way, you don't have to wait for the entire response to be generated, and you can see the output as it is being produced. Especially for long responses, or in user-facing applications, streaming output can result in a much better user experience."]},{"cell_type":"markdown","id":"f0078335-544c-4dc4-b47c-9a3cd984d2f6","metadata":{"id":"f0078335-544c-4dc4-b47c-9a3cd984d2f6"},"source":["Let's create a prompt that generates a longer response."]},{"cell_type":"code","execution_count":6,"id":"80923877-9934-4edf-9e13-0b730b56c6a9","metadata":{"id":"80923877-9934-4edf-9e13-0b730b56c6a9","executionInfo":{"status":"ok","timestamp":1757339835896,"user_tz":0,"elapsed":16,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompt = 'Explain who you are in roughly 500 words.'"]},{"cell_type":"markdown","id":"87d2ae55-b2e9-4b4c-9b08-78c13c6dc879","metadata":{"id":"87d2ae55-b2e9-4b4c-9b08-78c13c6dc879"},"source":["Given this prompt, let's see how the `stream` function works."]},{"cell_type":"code","execution_count":14,"id":"28b6b786-23de-4669-9d22-c663aab6e51b","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"28b6b786-23de-4669-9d22-c663aab6e51b","executionInfo":{"status":"ok","timestamp":1757341174101,"user_tz":0,"elapsed":1900,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"e772d352-99d8-478a-f95e-89e13dd489c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["I am an artificial intelligence language model, which means I'm a computer program designed to understand, generate, and process human-like language. My primary function is to assist and communicate with users like you, providing information, answering questions, and engaging in conversation. I'm a type of machine learning model, trained on a massive dataset of text from various sources, including books, articles, and online content.\n","\n","My training data is sourced from a vast array of topics and domains, allowing me to possess a broad knowledge base. I can provide information on subjects such as history, science, technology, literature, and more. I'm not limited to just providing factual information; I can also generate creative content, like stories or poems, and even help with language-related tasks, such as translation or text summarization.\n","\n","One of my key features is my ability to learn and improve over time. Through interactions with users, I can refine my understanding of language and adapt to new contexts, topics, and styles. This process is called \"fine-tuning,\" and it enables me to become more accurate, informative, and engaging in my responses. I don't have personal experiences, emotions, or consciousness like humans do, but I'm designed to simulate conversation and provide helpful assistance.\n","\n","My capabilities are based on complex algorithms and natural language processing (NLP) techniques. These allow me to analyze and understand the nuances of language, including syntax, semantics, and pragmatics. I can recognize and respond to context-specific cues, such as idioms, colloquialisms, and figurative language. My responses are generated based on patterns and associations learned from my training data, rather than through conscious thought or reasoning.\n","\n","I'm a cloud-based service, which means I can be accessed from anywhere with an internet connection. I'm available 24/7, and I don't require any specific software or hardware to function. My primary interface is text-based, but I can also be integrated with other applications, such as voice assistants or chat platforms, to provide a more interactive experience.\n","\n","My purpose is to assist and augment human capabilities, rather than replace them. I'm designed to provide helpful information, answer questions, and facilitate communication, but I'm not a substitute for human judgment, expertise, or critical thinking. I can help with research, writing, and learning, but I'm not a replacement for human teachers, mentors, or peers.\n","\n","In summary, I'm a sophisticated language model, trained on a vast dataset of text, and designed to provide information, answer questions, and engage in conversation. I'm a machine learning model, constantly learning and improving, and my primary function is to assist and augment human capabilities. I'm available 24/7, and I can be accessed from anywhere with an internet connection. I'm here to help, provide information, and facilitate communication, and I look forward to interacting with you!"]}],"source":["for chunk in llm.stream(prompt):\n","    print(chunk.content, end='')"]},{"cell_type":"markdown","id":"abec9044-95a3-46ce-8975-0bcc5c4a431d","metadata":{"id":"abec9044-95a3-46ce-8975-0bcc5c4a431d"},"source":["The `stream` method in LangChain serves as a foundational tool and shows the response as it is being generated. This can make the interaction with the LLMs feel more responsive and improve the user experience."]},{"cell_type":"markdown","id":"2945a1bc-078a-4812-b521-ba5001083130","metadata":{"id":"2945a1bc-078a-4812-b521-ba5001083130"},"source":["---"]},{"cell_type":"markdown","id":"67f0510c-48b4-41b1-8797-d833e1676d0f","metadata":{"id":"67f0510c-48b4-41b1-8797-d833e1676d0f"},"source":["## Batching Responses"]},{"cell_type":"markdown","id":"483866aa-eb2e-4942-99fe-90dd66aa97ca","metadata":{"id":"483866aa-eb2e-4942-99fe-90dd66aa97ca"},"source":["You can also use `batch` to call the prompts on a list of inputs. Calling `batch` will return a list of responses in the same order as they were passed in.\n","\n","Not only is `batch` convenient when working with collections of data that all need to be responded to in some way by an LLM, but the `batch` method is designed to process multiple prompts concurrently, effectively running the responses in parallel as much as possible. This allows for more efficient handling of multiple requests, reducing the overall time needed to generate responses for a list of prompts. By batching requests, you can leverage the computational power of the language model to handle multiple inputs simultaneously, improving performance and throughput."]},{"cell_type":"markdown","id":"4f306794-c037-4721-be81-d5ac703c14a3","metadata":{"id":"4f306794-c037-4721-be81-d5ac703c14a3"},"source":["We'll demonstrate the functionality and performance benefits of batching by using this list of prompts about state capitals."]},{"cell_type":"code","execution_count":15,"id":"da29c84d-b63b-4d93-aa5c-6059a20c0ac9","metadata":{"id":"da29c84d-b63b-4d93-aa5c-6059a20c0ac9","executionInfo":{"status":"ok","timestamp":1757341322773,"user_tz":0,"elapsed":20,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["state_capital_questions = [\n","    'What is the capital of California?',\n","    'What is the capital of Texas?',\n","    'What is the capital of New York?',\n","    'What is the capital of Florida?',\n","    'What is the capital of Illinois?',\n","    'What is the capital of Ohio?'\n","]"]},{"cell_type":"markdown","id":"d169ab71-0f66-4dc7-ad33-a4f15d98546e","metadata":{"id":"d169ab71-0f66-4dc7-ad33-a4f15d98546e"},"source":["Using `batch` we can pass in the entire list..."]},{"cell_type":"code","execution_count":16,"id":"1277b361-b0c6-4ada-a647-7733724f585a","metadata":{"id":"1277b361-b0c6-4ada-a647-7733724f585a","executionInfo":{"status":"ok","timestamp":1757341327074,"user_tz":0,"elapsed":506,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["capitals = llm.batch(state_capital_questions)"]},{"cell_type":"markdown","id":"e9d4c328-41ad-4b16-9bec-6aea0e730ef4","metadata":{"id":"e9d4c328-41ad-4b16-9bec-6aea0e730ef4"},"source":["... and get back a list of responses."]},{"cell_type":"code","execution_count":17,"id":"41824542-df2e-41c9-bd13-87af42511a90","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41824542-df2e-41c9-bd13-87af42511a90","executionInfo":{"status":"ok","timestamp":1757341329965,"user_tz":0,"elapsed":11,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"51b20d98-90f2-46ae-e43d-3866c34292bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":17}],"source":["len(capitals)"]},{"cell_type":"code","execution_count":18,"id":"af4f0db6-04c9-4bfd-8497-9a0fc2dcda53","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af4f0db6-04c9-4bfd-8497-9a0fc2dcda53","executionInfo":{"status":"ok","timestamp":1757341335781,"user_tz":0,"elapsed":24,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"3c7a7fce-f502-492c-bead-71ac0c2d6895"},"outputs":[{"output_type":"stream","name":"stdout","text":["The capital of California is Sacramento.\n","The capital of Texas is Austin.\n","The capital of New York is Albany.\n","The capital of Florida is Tallahassee.\n","The capital of Illinois is Springfield.\n","The capital of Ohio is Columbus.\n"]}],"source":["for capital in capitals:\n","    print(capital.content)"]},{"cell_type":"markdown","id":"77e09a8e-3b80-4b5d-9b01-bc88f1afcf70","metadata":{"id":"77e09a8e-3b80-4b5d-9b01-bc88f1afcf70"},"source":["---"]},{"cell_type":"markdown","id":"7856e5f9-d156-4d4c-8f8e-d1e450e6e82f","metadata":{"id":"7856e5f9-d156-4d4c-8f8e-d1e450e6e82f"},"source":["## Comparing batch and invoke Performance"]},{"cell_type":"markdown","id":"7e644363-1351-4fa6-9189-42812655f368","metadata":{"id":"7e644363-1351-4fa6-9189-42812655f368"},"source":["Just to make a quick observation about the potential performance gains from batching, here we time a call to `batch`. Note the `Wall time`."]},{"cell_type":"code","execution_count":19,"id":"746ffbcd-73af-4081-b2d3-893eddf8f267","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"746ffbcd-73af-4081-b2d3-893eddf8f267","executionInfo":{"status":"ok","timestamp":1757341362027,"user_tz":0,"elapsed":723,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"38b51d52-6dc8-4a77-8ec7-c9dc3a7fcdab"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 60.4 ms, sys: 7.42 ms, total: 67.8 ms\n","Wall time: 441 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["[AIMessage(content='The capital of California is Sacramento.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 42, 'total_tokens': 50, 'completion_time': 0.001833084, 'prompt_time': 0.011459491, 'queue_time': 0.185998124, 'total_time': 0.013292575}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5e43a46e-303b-4cea-9e78-b5212fea679b-0', usage_metadata={'input_tokens': 42, 'output_tokens': 8, 'total_tokens': 50}),\n"," AIMessage(content='The capital of Texas is Austin.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 42, 'total_tokens': 50, 'completion_time': 0.000317005, 'prompt_time': 0.016788008, 'queue_time': 0.185887093, 'total_time': 0.017105013}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e7adae81-888a-4ebb-8bfc-a3a3b9a12d68-0', usage_metadata={'input_tokens': 42, 'output_tokens': 8, 'total_tokens': 50}),\n"," AIMessage(content='The capital of New York is Albany.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 43, 'total_tokens': 52, 'completion_time': 0.00140528, 'prompt_time': 0.035184462, 'queue_time': 0.185853127, 'total_time': 0.036589742}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--792026bd-71d0-4b81-9f1b-88cfee6a5629-0', usage_metadata={'input_tokens': 43, 'output_tokens': 9, 'total_tokens': 52}),\n"," AIMessage(content='The capital of Florida is Tallahassee.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 42, 'total_tokens': 52, 'completion_time': 0.01011201, 'prompt_time': 0.01150727, 'queue_time': 0.186057456, 'total_time': 0.02161928}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--206ad97e-a93a-40a2-800f-e147e80f5fc1-0', usage_metadata={'input_tokens': 42, 'output_tokens': 10, 'total_tokens': 52}),\n"," AIMessage(content='The capital of Illinois is Springfield.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 42, 'total_tokens': 50, 'completion_time': 0.002273118, 'prompt_time': 0.010924159, 'queue_time': 0.185723982, 'total_time': 0.013197277}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0f0ed5f0-77e4-46d7-bac6-67916d432239-0', usage_metadata={'input_tokens': 42, 'output_tokens': 8, 'total_tokens': 50}),\n"," AIMessage(content='The capital of Ohio is Columbus.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 42, 'total_tokens': 50, 'completion_time': 0.00214171, 'prompt_time': 0.010906529, 'queue_time': 0.18567648, 'total_time': 0.013048239}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b41a3bea-2f96-4fa1-868b-7169d8fbc80a-0', usage_metadata={'input_tokens': 42, 'output_tokens': 8, 'total_tokens': 50})]"]},"metadata":{},"execution_count":19}],"source":["%%time\n","llm.batch(state_capital_questions)"]},{"cell_type":"markdown","id":"17eb600a-99a6-4944-8bfa-774780f73b9f","metadata":{"id":"17eb600a-99a6-4944-8bfa-774780f73b9f"},"source":["And now to compare, we iterate over the `state_capital_questions` list and call `invoke` on each item. Again, note the `Wall time` and compare it to the results from batching above."]},{"cell_type":"code","execution_count":20,"id":"2af11c84-d013-4af0-9231-804cbe1c7671","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2af11c84-d013-4af0-9231-804cbe1c7671","executionInfo":{"status":"ok","timestamp":1757341389498,"user_tz":0,"elapsed":1952,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"7837d5e0-216f-43f5-e410-51a5d87eb458"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 32.9 ms, sys: 3.13 ms, total: 36 ms\n","Wall time: 1.95 s\n"]}],"source":["%%time\n","for cq in state_capital_questions:\n","    llm.invoke(cq)"]},{"cell_type":"markdown","id":"be9a8248-e671-49fb-b39f-2ca059e1d5a1","metadata":{"id":"be9a8248-e671-49fb-b39f-2ca059e1d5a1"},"source":["---"]},{"cell_type":"markdown","id":"64ab1cf9-16c9-4ed0-a7d0-5b56a4b4e5d8","metadata":{"id":"64ab1cf9-16c9-4ed0-a7d0-5b56a4b4e5d8"},"source":["## Exercise: Batch Process to Create an FAQ Document"]},{"cell_type":"markdown","id":"c60712f0-4e27-45bf-b04b-33ab366a8dbf","metadata":{"id":"c60712f0-4e27-45bf-b04b-33ab366a8dbf"},"source":["For this exercise you'll use batch processing to respond to a variety of LLM-related questions in service of creating an FAQ document (in this notebook setting the document will just be something we print to screen).\n","\n","Here is a list of LLM-related questions."]},{"cell_type":"code","execution_count":null,"id":"e84f23be-d16e-4f74-b9f6-b6598b47441a","metadata":{"id":"e84f23be-d16e-4f74-b9f6-b6598b47441a"},"outputs":[],"source":["faq_questions = [\n","    'What is a Large Language Model (LLM)?',\n","    'How do LLMs work?',\n","    'What are some common applications of LLMs?',\n","    'What is fine-tuning in the context of LLMs?',\n","    'How do LLMs handle context?',\n","    'What are some limitations of LLMs?',\n","    'How do LLMs generate text?',\n","    'What is the importance of prompt engineering in LLMs?',\n","    'How can LLMs be used in chatbots?',\n","    'What are some ethical considerations when using LLMs?'\n","]"]},{"cell_type":"markdown","id":"9aee16b6-959b-45c8-ac3f-78718cf6b492","metadata":{"id":"9aee16b6-959b-45c8-ac3f-78718cf6b492"},"source":["You job is to populate `faq_answers` below with a list of responses to each of the questions. Use the `batch` method to make this very easy."]},{"cell_type":"markdown","id":"e455dfd4-a30a-4e4a-a542-3721d45b4f7e","metadata":{"id":"e455dfd4-a30a-4e4a-a542-3721d45b4f7e"},"source":["Upon successful completion, you should be able to print the return value of calling the following `create_faq_document` with `faq_questions` and `faq_answers` and get an FAQ document for all of the LLM-related questions above."]},{"cell_type":"code","execution_count":null,"id":"c87a7513-9018-4468-beec-a04e6b878d31","metadata":{"id":"c87a7513-9018-4468-beec-a04e6b878d31"},"outputs":[],"source":["def create_faq_document(faq_questions, faq_answers):\n","    faq_document = ''\n","    for question, response in zip(faq_questions, faq_answers):\n","        faq_document += f'{question.upper()}\\n\\n'\n","        faq_document += f'{response.content}\\n\\n'\n","        faq_document += '-'*30 + '\\n\\n'\n","\n","    return faq_document"]},{"cell_type":"markdown","id":"e45bf921-ff91-4208-bfb4-8bd8caf90da5","metadata":{"id":"e45bf921-ff91-4208-bfb4-8bd8caf90da5"},"source":["If you get stuck, check out the *Solution* below."]},{"cell_type":"markdown","id":"4bc5c326-11c4-452c-a779-be392c591703","metadata":{"id":"4bc5c326-11c4-452c-a779-be392c591703"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"0deb9ccb-c2e5-4d47-8b86-bde71444184b","metadata":{"id":"0deb9ccb-c2e5-4d47-8b86-bde71444184b"},"outputs":[],"source":["faq_answers = []"]},{"cell_type":"code","execution_count":null,"id":"512ac53d-0a4c-4cdd-a537-b155d12cb7f4","metadata":{"scrolled":true,"id":"512ac53d-0a4c-4cdd-a537-b155d12cb7f4"},"outputs":[],"source":["# This should work after you successfully populate `faq_answers` with LLM responses.\n","print(create_faq_document(faq_questions, faq_answers))"]},{"cell_type":"markdown","id":"9f452d0d-b5ae-4f7e-8ed3-acc0db112716","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"9f452d0d-b5ae-4f7e-8ed3-acc0db112716"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"ac158c65-8386-4538-9b0c-728591ca5c27","metadata":{"id":"ac158c65-8386-4538-9b0c-728591ca5c27"},"outputs":[],"source":["faq_answers = llm.batch(faq_questions)"]},{"cell_type":"code","execution_count":null,"id":"728abc34-699f-45fa-8c82-5ff8abea791c","metadata":{"id":"728abc34-699f-45fa-8c82-5ff8abea791c"},"outputs":[],"source":["def create_faq_document(faq_questions, faq_answers):\n","    faq_document = ''\n","    for question, response in zip(faq_questions, faq_answers):\n","        faq_document += f'{question.upper()}\\n\\n'\n","        faq_document += f'{response.content}\\n\\n'\n","        faq_document += '-'*30 + '\\n\\n'\n","\n","    return faq_document"]},{"cell_type":"code","execution_count":null,"id":"8e062586-3b87-4184-a884-fc0fe519dbd4","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"8e062586-3b87-4184-a884-fc0fe519dbd4","executionInfo":{"status":"ok","timestamp":1756669043268,"user_tz":-330,"elapsed":222,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"163635e1-54af-422e-bb8a-60c3c42c8d6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["WHAT IS A LARGE LANGUAGE MODEL (LLM)?\n","\n","A Large Language Model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. It's a computer program that uses complex algorithms and statistical models to learn patterns and relationships in language data, allowing it to generate text, answer questions, and even converse with humans.\n","\n","LLMs are trained on vast amounts of text data, often sourced from the internet, books, and other digital sources. This training data enables the model to learn the structure, syntax, and semantics of language, including grammar, vocabulary, and context.\n","\n","Some key characteristics of LLMs include:\n","\n","1. **Scalability**: LLMs are designed to handle massive amounts of data and can process thousands of parameters, making them highly scalable.\n","2. **Deep learning**: LLMs use deep learning techniques, such as neural networks, to analyze and represent language data.\n","3. **Language understanding**: LLMs aim to understand the meaning and context of language, including nuances, idioms, and figurative language.\n","4. **Generative capabilities**: LLMs can generate text, responses, or even entire articles, based on the input they receive.\n","5. **Continuous learning**: LLMs can learn from new data and adapt to changing language patterns, allowing them to improve over time.\n","\n","LLMs have numerous applications, including:\n","\n","1. **Chatbots and virtual assistants**: LLMs power conversational interfaces, such as chatbots, voice assistants, and customer service bots.\n","2. **Language translation**: LLMs can translate text from one language to another, facilitating communication across languages.\n","3. **Text summarization**: LLMs can summarize long documents, articles, or texts, highlighting key points and main ideas.\n","4. **Content generation**: LLMs can generate content, such as blog posts, articles, or social media posts, based on a given topic or prompt.\n","5. **Sentiment analysis**: LLMs can analyze text to determine the sentiment, tone, or emotional tone, helping businesses and organizations understand customer opinions.\n","\n","Examples of LLMs include:\n","\n","1. **Transformer models**: BERT, RoBERTa, and XLNet are popular transformer-based LLMs.\n","2. **Recurrent neural networks (RNNs)**: LSTMs and GRUs are types of RNNs used in LLMs.\n","3. **Generative models**: Language models like GPT-3 and T5 are examples of LLMs that can generate coherent and context-specific text.\n","\n","In summary, Large Language Models are powerful AI systems that can understand, generate, and process human language, enabling a wide range of applications and use cases.\n","\n","------------------------------\n","\n","HOW DO LLMS WORK?\n","\n","Large Language Models (LLMs) are a type of artificial intelligence (AI) designed to process and understand human language. They work by using complex algorithms and massive amounts of data to learn patterns and relationships in language. Here's a simplified overview of how LLMs work:\n","\n","1. **Training Data**: LLMs are trained on vast amounts of text data, which can include books, articles, research papers, and websites. This data is used to teach the model about language structures, grammar, and semantics.\n","2. **Tokenization**: The training data is broken down into smaller units called tokens, which can be words, characters, or subwords (smaller units of words). These tokens are used as input to the model.\n","3. **Embeddings**: The tokens are then embedded into a high-dimensional vector space, where similar tokens are mapped to nearby points. This allows the model to capture nuanced relationships between tokens.\n","4. **Neural Network Architecture**: LLMs use a type of neural network called a transformer, which is designed specifically for sequence-to-sequence tasks like language translation and text generation. The transformer architecture consists of an encoder and a decoder.\n","5. **Encoder**: The encoder takes in the input tokens and generates a continuous representation of the input sequence. This representation is used to capture the context and meaning of the input text.\n","6. **Decoder**: The decoder generates the output sequence, one token at a time, based on the output of the encoder. The decoder uses a technique called self-attention to weigh the importance of different input tokens when generating each output token.\n","7. **Training Objective**: The model is trained to predict the next token in the sequence, given the context of the previous tokens. This is done using a technique called masked language modeling, where some tokens are randomly replaced with a mask token, and the model is trained to predict the original token.\n","8. **Optimization**: The model is optimized using a loss function, such as cross-entropy, to minimize the difference between the predicted output and the actual output.\n","9. **Inference**: Once the model is trained, it can be used for a variety of tasks, such as language translation, text summarization, and text generation.\n","\n","Some key techniques used in LLMs include:\n","\n","* **Attention Mechanism**: allows the model to focus on specific parts of the input sequence when generating output\n","* **Self-Attention**: allows the model to weigh the importance of different input tokens when generating output\n","* **Layer Normalization**: helps to stabilize the training process and improve the model's performance\n","* **Dropout**: helps to prevent overfitting by randomly dropping out units during training\n","\n","LLMs have many applications, including:\n","\n","* **Language Translation**: translating text from one language to another\n","* **Text Summarization**: summarizing long pieces of text into shorter summaries\n","* **Text Generation**: generating new text based on a prompt or topic\n","* **Question Answering**: answering questions based on a piece of text\n","* **Sentiment Analysis**: analyzing the sentiment or emotional tone of text\n","\n","However, LLMs also have some limitations and challenges, such as:\n","\n","* **Bias and Fairness**: LLMs can perpetuate biases and stereotypes present in the training data\n","* **Explainability**: it can be difficult to understand why an LLM made a particular prediction or decision\n","* **Robustness**: LLMs can be vulnerable to adversarial attacks and out-of-distribution examples\n","* **Scalability**: training and deploying LLMs can require significant computational resources and infrastructure.\n","\n","------------------------------\n","\n","WHAT ARE SOME COMMON APPLICATIONS OF LLMS?\n","\n","Large Language Models (LLMs) have numerous applications across various industries. Here are some common applications of LLMs:\n","\n","1. **Virtual Assistants**: LLMs power virtual assistants like Siri, Alexa, and Google Assistant, enabling them to understand and respond to voice commands.\n","2. **Language Translation**: LLMs can be fine-tuned for language translation tasks, allowing for more accurate and efficient translation of text from one language to another.\n","3. **Text Summarization**: LLMs can summarize long pieces of text into concise, meaningful summaries, saving time and effort for readers.\n","4. **Chatbots**: LLMs are used to build chatbots that can engage in natural-sounding conversations with humans, providing customer support, answering questions, and helping with tasks.\n","5. **Sentiment Analysis**: LLMs can analyze text to determine the sentiment or emotional tone behind it, which is useful for applications like opinion mining, customer feedback analysis, and social media monitoring.\n","6. **Content Generation**: LLMs can generate high-quality content, such as articles, blog posts, and even entire books, although human editing and oversight are still necessary.\n","7. **Speech Recognition**: LLMs can be used to improve speech recognition systems, allowing for more accurate transcription of spoken language.\n","8. **Question Answering**: LLMs can be fine-tuned to answer questions on a wide range of topics, making them useful for applications like trivia games, educational tools, and customer support systems.\n","9. **Named Entity Recognition**: LLMs can identify and extract specific entities like names, locations, and organizations from unstructured text, which is useful for applications like data extraction and information retrieval.\n","10. **Text Classification**: LLMs can classify text into categories like spam vs. non-spam emails, positive vs. negative product reviews, and more, which is useful for applications like email filtering and sentiment analysis.\n","11. **Conversational AI**: LLMs are used to build conversational AI systems that can engage in natural-sounding conversations with humans, using context and understanding to respond to questions and statements.\n","12. **Writing Assistance**: LLMs can assist with writing tasks like grammar correction, spell checking, and suggesting alternative phrases or sentences.\n","13. **Data Analysis**: LLMs can be used to analyze and extract insights from large datasets, such as identifying trends, patterns, and correlations.\n","14. **Social Media Monitoring**: LLMs can monitor social media platforms to track brand mentions, sentiment, and trends, providing valuable insights for businesses and organizations.\n","15. **Education**: LLMs can be used to create personalized learning experiences, such as adaptive textbooks, interactive learning platforms, and intelligent tutoring systems.\n","\n","These are just a few examples of the many applications of LLMs. As the technology continues to evolve, we can expect to see even more innovative uses of LLMs in the future.\n","\n","------------------------------\n","\n","WHAT IS FINE-TUNING IN THE CONTEXT OF LLMS?\n","\n","In the context of Large Language Models (LLMs), fine-tuning refers to the process of adjusting the model's parameters to better fit a specific task or dataset. This is typically done after the model has been pre-trained on a large, general dataset.\n","\n","Pre-training involves training the model on a massive dataset, such as the entire internet, to learn general language patterns and representations. However, this pre-trained model may not perform optimally on a specific task, such as sentiment analysis, question-answering, or text classification, due to the differences in the task's requirements and the pre-training dataset.\n","\n","Fine-tuning involves taking the pre-trained model and further training it on a smaller, task-specific dataset to adapt the model's parameters to the new task. This process helps the model to:\n","\n","1. **Specialize** in the specific task, by learning task-specific patterns and relationships.\n","2. **Overcome domain shift**, by adapting to the differences between the pre-training dataset and the task-specific dataset.\n","3. **Improve performance**, by reducing the gap between the pre-trained model's performance and the optimal performance on the specific task.\n","\n","Fine-tuning typically involves the following steps:\n","\n","1. **Loading the pre-trained model**: The pre-trained model is loaded, and its parameters are used as a starting point.\n","2. **Adding a task-specific layer**: A new layer, specific to the task, is added on top of the pre-trained model. This layer is typically a classification or regression layer.\n","3. **Training on the task-specific dataset**: The model is trained on the task-specific dataset, with the pre-trained parameters being fine-tuned to adapt to the new task.\n","4. **Optimizing the model**: The model is optimized using an optimization algorithm, such as stochastic gradient descent (SGD), to minimize the loss function and improve performance on the task-specific dataset.\n","\n","Fine-tuning has several benefits, including:\n","\n","* **Improved performance**: Fine-tuning can significantly improve the model's performance on the specific task.\n","* **Reduced training time**: Fine-tuning is typically faster than training a model from scratch, as the pre-trained model has already learned general language patterns.\n","* **Smaller dataset requirements**: Fine-tuning can be done with a smaller dataset, as the pre-trained model has already learned general patterns and relationships.\n","\n","However, fine-tuning also has some limitations, such as:\n","\n","* **Overfitting**: Fine-tuning can lead to overfitting, especially if the task-specific dataset is small.\n","* **Catastrophic forgetting**: Fine-tuning can cause the model to forget some of the general knowledge it learned during pre-training.\n","\n","Overall, fine-tuning is a powerful technique for adapting pre-trained LLMs to specific tasks, and it has become a crucial step in many natural language processing (NLP) applications.\n","\n","------------------------------\n","\n","HOW DO LLMS HANDLE CONTEXT?\n","\n","Large Language Models (LLMs) handle context in several ways:\n","\n","1. **Contextualized Embeddings**: LLMs use contextualized embeddings, which capture the nuances of word meanings based on their surrounding context. This allows the model to understand the relationships between words and phrases within a sentence or passage.\n","2. **Attention Mechanisms**: Attention mechanisms enable LLMs to focus on specific parts of the input text when generating output. This helps the model to selectively concentrate on relevant information and ignore irrelevant details.\n","3. **Recurrent Neural Networks (RNNs)**: RNNs, such as Long Short-Term Memory (LSTM) networks, are designed to handle sequential data and capture long-range dependencies. They allow LLMs to maintain a internal state that reflects the context of the input text.\n","4. **Transformer Architecture**: The Transformer architecture, introduced in 2017, revolutionized the field of natural language processing. It relies on self-attention mechanisms to model relationships between different parts of the input text, allowing the model to capture complex contextual relationships.\n","5. **Memory-Augmented Architectures**: Some LLMs incorporate external memory mechanisms, which store and retrieve information from a separate memory module. This allows the model to retain context over longer periods and generate more coherent text.\n","6. **Hierarchical Representations**: LLMs can learn hierarchical representations of text, where higher-level representations capture more abstract concepts and lower-level representations focus on specific details. This enables the model to understand the context at multiple levels of granularity.\n","7. **Training Objectives**: LLMs are often trained with objectives that encourage them to capture context, such as masked language modeling, next sentence prediction, or conversational dialogue generation.\n","\n","To handle context, LLMs typically use a combination of these techniques. The specific approach depends on the model architecture, training objectives, and the task at hand.\n","\n","**Challenges in handling context**:\n","\n","1. **Contextual understanding**: LLMs may struggle to truly understand the context, as they rely on statistical patterns and may not capture nuances of human language.\n","2. **Limited context window**: LLMs typically have a limited context window, which can make it difficult to capture long-range dependencies or relationships between distant parts of the text.\n","3. **Adversarial examples**: LLMs can be vulnerable to adversarial examples, which are carefully crafted inputs designed to mislead the model and test its contextual understanding.\n","\n","**Future directions**:\n","\n","1. **Improving contextual understanding**: Researchers are exploring ways to improve LLMs' contextual understanding, such as incorporating more advanced attention mechanisms or using multimodal inputs (e.g., text, images, audio).\n","2. **Increasing context window**: Researchers are working on developing models that can capture longer-range dependencies and relationships, such as using more advanced RNNs or Transformer architectures.\n","3. **Robustness to adversarial examples**: Researchers are developing techniques to improve LLMs' robustness to adversarial examples, such as using adversarial training or developing more robust evaluation metrics.\n","\n","------------------------------\n","\n","WHAT ARE SOME LIMITATIONS OF LLMS?\n","\n","Large Language Models (LLMs) have achieved impressive results in various natural language processing tasks, but they also have several limitations. Here are some of the key limitations:\n","\n","1. **Lack of Common Sense**: LLMs often struggle to understand the nuances of human language, idioms, and figurative language, which can lead to nonsensical or absurd responses.\n","2. **Limited Domain Knowledge**: While LLMs have been trained on vast amounts of text data, their knowledge is limited to the data they've been trained on. They may not have the same level of expertise as a human in a specific domain.\n","3. **Inability to Reason**: LLMs are not capable of true reasoning, as they rely on statistical patterns and associations to generate text. They may not be able to draw logical conclusions or understand the implications of a statement.\n","4. **Lack of Emotional Intelligence**: LLMs do not possess emotional intelligence, which can make it difficult for them to understand the emotional tone or subtleties of human language.\n","5. **Biased Training Data**: LLMs can inherit biases present in the training data, which can result in discriminatory or unfair responses.\n","6. **Overfitting and Underfitting**: LLMs can suffer from overfitting (when the model is too closely fit to the training data) or underfitting (when the model is too simple to capture the underlying patterns in the data).\n","7. **Limited Contextual Understanding**: LLMs may struggle to understand the context of a conversation or the relationships between different pieces of text.\n","8. **Vulnerability to Adversarial Attacks**: LLMs can be vulnerable to adversarial attacks, which are designed to manipulate the model's output or compromise its performance.\n","9. **Inability to Handle Multimodal Input**: LLMs are typically designed to process text-only input and may not be able to handle multimodal input, such as images, audio, or video.\n","10. **Explainability and Transparency**: LLMs can be difficult to interpret and understand, making it challenging to explain their decisions or outputs.\n","11. **Limited Ability to Handle Ambiguity**: LLMs may struggle to handle ambiguous or uncertain language, which can lead to incorrect or misleading responses.\n","12. **Dependence on Data Quality**: LLMs are only as good as the data they've been trained on. If the training data is of poor quality, the model's performance will suffer.\n","13. **Lack of Human Judgment**: LLMs lack human judgment and may not be able to make decisions that require a deep understanding of human values, ethics, or social norms.\n","14. **Inability to Handle Idioms and Colloquialisms**: LLMs may struggle to understand idioms, colloquialisms, and other forms of informal language.\n","15. **Limited Ability to Handle Out-of-Vocabulary Words**: LLMs may not be able to handle out-of-vocabulary words or neologisms, which can limit their ability to understand and respond to novel or emerging language.\n","\n","These limitations highlight the need for continued research and development to improve the performance, robustness, and transparency of LLMs.\n","\n","------------------------------\n","\n","HOW DO LLMS GENERATE TEXT?\n","\n","Large Language Models (LLMs) generate text through a combination of natural language processing (NLP) and machine learning algorithms. Here's a simplified overview of the process:\n","\n","1. **Training**: LLMs are trained on vast amounts of text data, which can include books, articles, research papers, and websites. This training data is used to learn patterns, relationships, and structures of language.\n","2. **Tokenization**: When generating text, the model breaks down the input prompt or context into smaller units called tokens. These tokens can be words, subwords (smaller units of words), or even characters.\n","3. **Embeddings**: Each token is then embedded into a vector space, where similar tokens are mapped to nearby points. This allows the model to capture semantic relationships between tokens.\n","4. **Encoder-Decoder Architecture**: LLMs typically employ an encoder-decoder architecture. The encoder takes in the input tokens and generates a continuous representation of the input, called the \"context vector.\" The decoder then uses this context vector to generate the output text, one token at a time.\n","5. **Self-Attention Mechanism**: LLMs use self-attention mechanisms to weigh the importance of different tokens in the input sequence when generating each output token. This allows the model to focus on relevant parts of the input when generating the output.\n","6. **Language Model**: The LLM uses a language model to predict the next token in the sequence, based on the context vector and the previous tokens generated. The language model is trained to maximize the likelihood of the training data, which means it learns to predict the next token in a sequence given the context.\n","7. **Sampling**: Once the model has generated a token, it uses a sampling strategy to select the next token from the predicted probability distribution. This can be done using techniques like beam search, top-k sampling, or nucleus sampling.\n","8. **Post-processing**: The generated text may undergo post-processing, such as spell-checking, grammar-checking, or fluency evaluation, to refine the output.\n","\n","Some key techniques used in LLMs include:\n","\n","* **Transformers**: LLMs often employ transformer architectures, which use self-attention mechanisms to model relationships between tokens.\n","* **Masked Language Modeling**: LLMs are trained using masked language modeling, where some tokens in the input sequence are randomly replaced with a mask token. The model then predicts the original token, which helps it learn to represent the context.\n","* **Pre-training**: LLMs are often pre-trained on large datasets and then fine-tuned on specific tasks, such as language translation or text generation.\n","\n","Overall, LLMs generate text by learning to predict the next token in a sequence, based on the context and the patterns learned from the training data. The specific techniques and architectures used can vary depending on the model and its intended application.\n","\n","------------------------------\n","\n","WHAT IS THE IMPORTANCE OF PROMPT ENGINEERING IN LLMS?\n","\n","Prompt engineering is a crucial aspect of working with Large Language Models (LLMs). It refers to the process of crafting and optimizing the input prompts or queries that are fed into an LLM to elicit specific, accurate, and relevant responses. The importance of prompt engineering in LLMs can be summarized as follows:\n","\n","1. **Improved accuracy**: Well-designed prompts can significantly improve the accuracy of LLM responses. By providing clear, concise, and relevant context, prompts can help the model understand the task or question being asked, leading to more accurate and informative answers.\n","2. **Increased relevance**: Prompt engineering helps ensure that the LLM's response is relevant to the task or question at hand. By including specific keywords, phrases, or context, prompts can guide the model towards providing more targeted and useful responses.\n","3. **Reducing ambiguity**: Ambiguous or unclear prompts can lead to confusing or incorrect responses. Prompt engineering helps to clarify the intent behind the prompt, reducing the likelihood of misinterpretation and ensuring that the LLM provides a response that is relevant and useful.\n","4. **Enhancing creativity**: By providing well-designed prompts, users can tap into the creative potential of LLMs. Prompt engineering can help stimulate the model's ability to generate novel and innovative responses, such as text, images, or even code.\n","5. **Efficient use of resources**: Poorly designed prompts can lead to inefficient use of computational resources, as the LLM may struggle to understand the task or question. Well-crafted prompts can help optimize the use of resources, reducing the time and computational power required to generate a response.\n","6. **Mitigating biases**: Prompt engineering can help mitigate biases in LLM responses by providing context and guidance that promotes fairness and inclusivity. By including diverse perspectives and examples, prompts can encourage the model to generate more balanced and unbiased responses.\n","7. **Enabling complex tasks**: Prompt engineering enables users to tackle complex tasks, such as multi-step problems, by breaking them down into smaller, manageable sub-tasks. This allows LLMs to generate more accurate and informative responses to complex questions or tasks.\n","8. **Facilitating human-AI collaboration**: Prompt engineering is essential for effective human-AI collaboration. By providing clear and well-designed prompts, humans can work more efficiently with LLMs, leveraging their strengths and capabilities to achieve common goals.\n","9. **Improving transparency and explainability**: Well-designed prompts can provide insights into the decision-making process of LLMs, making their responses more transparent and explainable. This is particularly important in high-stakes applications, such as healthcare or finance, where understanding the reasoning behind a response is critical.\n","10. **Advancing LLM research and development**: Prompt engineering is a key area of research in the development of LLMs. By exploring the boundaries of prompt engineering, researchers can improve our understanding of how LLMs work, identify areas for improvement, and develop more effective and efficient models.\n","\n","In summary, prompt engineering is a vital aspect of working with LLMs, as it enables users to unlock the full potential of these models, improve their accuracy and relevance, and facilitate more effective human-AI collaboration.\n","\n","------------------------------\n","\n","HOW CAN LLMS BE USED IN CHATBOTS?\n","\n","Large Language Models (LLMs) can be used in chatbots to improve their conversational capabilities and provide more accurate and informative responses to user queries. Here are some ways LLMs can be used in chatbots:\n","\n","1. **Intent identification and classification**: LLMs can be trained to identify the intent behind a user's message, such as booking a flight or making a complaint. This allows the chatbot to respond accordingly and provide relevant information or actions.\n","2. **Contextual understanding**: LLMs can understand the context of a conversation, including the topic, tone, and previous messages. This enables the chatbot to respond in a more personalized and relevant manner.\n","3. **Natural Language Generation (NLG)**: LLMs can generate human-like responses to user queries, making the conversation feel more natural and engaging.\n","4. **Entity recognition and extraction**: LLMs can identify and extract specific entities from user input, such as names, dates, and locations. This information can be used to personalize the conversation and provide more accurate responses.\n","5. **Sentiment analysis**: LLMs can analyze the sentiment of user input, allowing the chatbot to respond in a more empathetic and personalized manner.\n","6. **Dialogue management**: LLMs can manage the flow of a conversation, determining the next response or action based on the user's input and the conversation history.\n","7. **Knowledge retrieval**: LLMs can be trained on large knowledge bases, allowing the chatbot to provide accurate and up-to-date information on a wide range of topics.\n","8. **Personalization**: LLMs can be used to personalize the conversation based on user preferences, behavior, and history, making the interaction more engaging and relevant.\n","9. **Multi-turn conversations**: LLMs can handle multi-turn conversations, allowing the chatbot to engage in longer and more complex conversations with users.\n","10. **Continuous learning**: LLMs can be fine-tuned and updated with new data, allowing the chatbot to improve its performance and adapt to changing user behaviors and preferences.\n","\n","To integrate LLMs into chatbots, developers can use various techniques, such as:\n","\n","1. **API integration**: LLMs can be integrated into chatbots through APIs, allowing developers to access pre-trained models and use them in their chatbot applications.\n","2. **Model fine-tuning**: Developers can fine-tune pre-trained LLMs on their own data, adapting the model to their specific use case and improving its performance.\n","3. **Custom model training**: Developers can train their own LLMs from scratch, using their own data and tailored to their specific requirements.\n","4. **Hybrid approach**: Developers can combine LLMs with other AI technologies, such as rule-based systems or decision trees, to create a more robust and flexible chatbot.\n","\n","By leveraging LLMs, chatbots can become more conversational, informative, and engaging, providing a better user experience and improving customer satisfaction.\n","\n","------------------------------\n","\n","WHAT ARE SOME ETHICAL CONSIDERATIONS WHEN USING LLMS?\n","\n","Large Language Models (LLMs) have the potential to bring about significant benefits, but they also raise important ethical considerations. Here are some of the key concerns:\n","\n","1. **Bias and Discrimination**: LLMs can perpetuate and amplify existing biases and discrimination present in the data used to train them. This can result in unfair outcomes, such as discriminatory language generation or unequal treatment of certain groups.\n","2. **Privacy and Data Protection**: LLMs often require vast amounts of personal data to function effectively. This raises concerns about data privacy, particularly if the data is not anonymized or if it is used without consent.\n","3. **Transparency and Explainability**: LLMs can be complex and difficult to understand, making it challenging to explain their decisions or actions. This lack of transparency can erode trust and make it difficult to identify and address potential biases or errors.\n","4. **Accountability and Responsibility**: As LLMs become more autonomous, it is essential to establish clear lines of accountability and responsibility. This includes determining who is responsible for any errors or harm caused by the model.\n","5. **Job Displacement and Economic Impact**: The increasing use of LLMs could lead to job displacement, particularly in industries where tasks are automated. This raises concerns about the economic impact on workers and the need for retraining and upskilling programs.\n","6. **Misinformation and Disinformation**: LLMs can generate convincing but false information, which can be used to spread misinformation or disinformation. This can have serious consequences, including undermining trust in institutions and exacerbating social tensions.\n","7. **Intellectual Property and Ownership**: The use of LLMs raises questions about intellectual property and ownership. For example, who owns the rights to generated content, and how should royalties be distributed?\n","8. **Security and Vulnerabilities**: LLMs can be vulnerable to attacks, such as data poisoning or model inversion, which can compromise their integrity and confidentiality.\n","9. **Human Values and Alignment**: As LLMs become more advanced, there is a risk that they may not align with human values, such as empathy, fairness, and respect for human dignity.\n","10. **Regulatory Frameworks**: The development and deployment of LLMs often outpace regulatory frameworks, which can create uncertainty and inconsistency in their governance.\n","\n","To address these ethical considerations, it is essential to:\n","\n","1. Develop and implement robust guidelines and regulations for the development and deployment of LLMs.\n","2. Invest in research and development of more transparent, explainable, and fair LLMs.\n","3. Establish clear lines of accountability and responsibility for LLMs.\n","4. Provide education and training programs to help workers develop skills that complement LLMs.\n","5. Encourage transparency and disclosure about the use of LLMs and their limitations.\n","6. Develop and implement effective mechanisms for detecting and mitigating misinformation and disinformation.\n","7. Foster a culture of responsible AI development and deployment, prioritizing human values and well-being.\n","\n","By acknowledging and addressing these ethical considerations, we can ensure that LLMs are developed and used in ways that benefit society and promote human well-being.\n","\n","------------------------------\n","\n","\n"]}],"source":["print(create_faq_document(faq_questions, faq_answers))"]},{"cell_type":"markdown","id":"846a66a0-3cf3-4161-9502-a1fefc647603","metadata":{"id":"846a66a0-3cf3-4161-9502-a1fefc647603"},"source":["---"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}