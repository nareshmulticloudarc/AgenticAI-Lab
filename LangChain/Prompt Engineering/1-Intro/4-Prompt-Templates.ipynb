{"cells":[{"cell_type":"markdown","id":"52c4ef3a-89b6-479e-9a56-1ed05315cdec","metadata":{"id":"52c4ef3a-89b6-479e-9a56-1ed05315cdec"},"source":["# Prompt Templates"]},{"cell_type":"markdown","id":"28013522","metadata":{"id":"28013522"},"source":["In this notebook you'll learn how to capture reusable LLM functionality in prompt templates, and begin working with the powerful prompt template tools provided by LangChain."]},{"cell_type":"markdown","id":"69366671-11a4-4439-b6ad-cb89497ef5d4","metadata":{"id":"69366671-11a4-4439-b6ad-cb89497ef5d4"},"source":["---"]},{"cell_type":"markdown","id":"c08054f2","metadata":{"id":"c08054f2"},"source":["## Objectives"]},{"cell_type":"markdown","id":"a023bc7a-47b5-4508-957c-f3354c9fb363","metadata":{"id":"a023bc7a-47b5-4508-957c-f3354c9fb363"},"source":["By the time you complete this notebook you will:\n","\n","- Appreciate the need and ability to capture LLM-related tasks in prompt templates.\n","- Be able to create reusable prompt templates with LangChain.\n","- Use prompt templates to perform a variety of LLM-powered tasks on a collection of provided text samples."]},{"cell_type":"markdown","id":"f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb","metadata":{"id":"f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb"},"source":["---"]},{"cell_type":"code","source":["!pip install groq langchain-groq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6z-NKhPipev","executionInfo":{"status":"ok","timestamp":1757344645679,"user_tz":0,"elapsed":21339,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"0d479934-6b16-4d2b-8b6f-a00c1268b5fc"},"id":"s6z-NKhPipev","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n","Collecting langchain-groq\n","  Downloading langchain_groq-0.3.7-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.75)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.23)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.4)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.24.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n","Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_groq-0.3.7-py3-none-any.whl (16 kB)\n","Installing collected packages: groq, langchain-groq\n","Successfully installed groq-0.31.1 langchain-groq-0.3.7\n"]}]},{"cell_type":"markdown","id":"327550d4","metadata":{"id":"327550d4"},"source":["## Imports"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key:\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PoujEgqyjTjI","executionInfo":{"status":"ok","timestamp":1757344655119,"user_tz":0,"elapsed":9434,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"30989c81-107c-44d2-ea67-4aba05b2ca1d"},"id":"PoujEgqyjTjI","execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["GROQ API Key:\n","··········\n"]}]},{"cell_type":"markdown","id":"9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d","metadata":{"id":"9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d"},"source":["---"]},{"cell_type":"markdown","id":"5c0e1244","metadata":{"id":"5c0e1244"},"source":["## Create a Model Instance"]},{"cell_type":"code","execution_count":3,"id":"75febe51","metadata":{"id":"75febe51","executionInfo":{"status":"ok","timestamp":1757344658199,"user_tz":0,"elapsed":3084,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_groq import ChatGroq\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)"]},{"cell_type":"markdown","id":"64d304e5-6bbe-4beb-ba92-fa208fb4c2f4","metadata":{"id":"64d304e5-6bbe-4beb-ba92-fa208fb4c2f4"},"source":["---"]},{"cell_type":"markdown","id":"d5021590-a7dd-44a6-b280-3de671f83d82","metadata":{"id":"d5021590-a7dd-44a6-b280-3de671f83d82"},"source":["## One-off Tasks vs. Reusable Functionality"]},{"cell_type":"markdown","id":"f7e49028-b3ee-410a-9572-be6ef67f034d","metadata":{"id":"f7e49028-b3ee-410a-9572-be6ef67f034d"},"source":["If you are the end user of an LLM-based application, especially a chatbot like Perplexity or ChatGPT, then you may very well undertake a process of iterative prompt development to elicit a response from the LLM-based application that is helpful to you. If, however, you are engineering prompts for use in an LLM-based application that you are building, it is often the case that you want to develop a prompt that captures some task or functionality, and that can be reused with a variety of inputs.\n","\n","As a developer you are already deeply familiar with the progression from one-off tasks to more generalized, templated functionalities. There are many ways you do this, but as a simple and universally relatable example consider the following example of calculating the products of 2 numbers.\n","\n","If you just needed to perform a calculation in a one-off manner for yourself, you could simply write a line of code like the following:"]},{"cell_type":"code","execution_count":4,"id":"ef8b2856-3a8a-4f41-b5b6-f6c02b814192","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ef8b2856-3a8a-4f41-b5b6-f6c02b814192","executionInfo":{"status":"ok","timestamp":1757344712263,"user_tz":0,"elapsed":24,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"efa09128-3aa9-4aae-8ce0-99b8efbdfbed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6336"]},"metadata":{},"execution_count":4}],"source":["99 * 64"]},{"cell_type":"markdown","id":"49f8425e-c5e4-406a-957f-3b9f4cc29c39","metadata":{"id":"49f8425e-c5e4-406a-957f-3b9f4cc29c39"},"source":["If however, you wanted to capture some more general functionality, say multiplying two numbers together, and wanted this functionality to be able to be reused across a variety of inputs, you would write something like the following function:"]},{"cell_type":"code","execution_count":5,"id":"213fece1-590d-4f18-b061-1bb91bd4a863","metadata":{"id":"213fece1-590d-4f18-b061-1bb91bd4a863","executionInfo":{"status":"ok","timestamp":1757344731114,"user_tz":0,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["def multiply_two_numbers(a, b):\n","    return a * b"]},{"cell_type":"markdown","id":"1b92ebeb-c7ea-4a86-a58d-fcc4c39c2ea3","metadata":{"id":"1b92ebeb-c7ea-4a86-a58d-fcc4c39c2ea3"},"source":["At this point not only could you do the orignal calculation,..."]},{"cell_type":"code","execution_count":6,"id":"788400eb-3730-497b-8730-6cf4afc38dbf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"788400eb-3730-497b-8730-6cf4afc38dbf","executionInfo":{"status":"ok","timestamp":1757344736065,"user_tz":0,"elapsed":44,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"cb616f06-0d0d-455c-bc1f-95ce1e9c7980"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6336"]},"metadata":{},"execution_count":6}],"source":["multiply_two_numbers(99, 64)"]},{"cell_type":"markdown","id":"8ab3aa48-e39a-4b29-88bc-47108fce3aa9","metadata":{"id":"8ab3aa48-e39a-4b29-88bc-47108fce3aa9"},"source":["...but you can reuse the function for an arbitrary number of calculations, including those deemed useful users other than yourself."]},{"cell_type":"markdown","id":"79e73d35-488d-45ab-bb92-d586a503f497","metadata":{"id":"79e73d35-488d-45ab-bb92-d586a503f497"},"source":["---"]},{"cell_type":"markdown","id":"dcbd94d2-4ed9-4f69-a1b2-3596d17b3bc1","metadata":{"id":"dcbd94d2-4ed9-4f69-a1b2-3596d17b3bc1"},"source":["## Prompt Templates As Reusable Functionality"]},{"cell_type":"markdown","id":"16861b50-c061-4fe7-8d52-2f92b820d71a","metadata":{"id":"16861b50-c061-4fe7-8d52-2f92b820d71a"},"source":["Prompting is not so different. If you have a one off task, you just write a prompt for it:"]},{"cell_type":"code","execution_count":7,"id":"0a9097c0-6c77-4611-acd1-f9d0ea7dc5da","metadata":{"id":"0a9097c0-6c77-4611-acd1-f9d0ea7dc5da","executionInfo":{"status":"ok","timestamp":1757344755297,"user_tz":0,"elapsed":40,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["one_off_prompt = \"Translate the following from English to Spanish: 'Today is a good day.'\""]},{"cell_type":"code","execution_count":8,"id":"5418f61c-fe2c-4e87-9d5f-0d140fd016ee","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5418f61c-fe2c-4e87-9d5f-0d140fd016ee","executionInfo":{"status":"ok","timestamp":1757344756953,"user_tz":0,"elapsed":264,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"b64bf3d6-d110-4f37-ff97-1e0e12493fcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hoy es un buen día.\n"]}],"source":["print(llm.invoke(one_off_prompt).content)"]},{"cell_type":"markdown","id":"38f35154-d644-4ca8-808d-e4c626bcf053","metadata":{"id":"38f35154-d644-4ca8-808d-e4c626bcf053"},"source":["If, however, you'd like to create reusable functionality, you might abstract part of the prompt away into arguments so that you're left with something you could reuse with arbitrary inputs, like the following:"]},{"cell_type":"code","execution_count":9,"id":"2d55a9f8-3d96-441d-ba96-acac1ef5e39d","metadata":{"id":"2d55a9f8-3d96-441d-ba96-acac1ef5e39d","executionInfo":{"status":"ok","timestamp":1757344785877,"user_tz":0,"elapsed":23,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["def translate_from_english_to_spanish(english_statement):\n","    return f\"Translate the following from English to Spanish. Provide just the translated text: {english_statement}\""]},{"cell_type":"code","execution_count":10,"id":"cb8bbd8a-f806-4b31-8e04-11f926a8378f","metadata":{"id":"cb8bbd8a-f806-4b31-8e04-11f926a8378f","executionInfo":{"status":"ok","timestamp":1757344787314,"user_tz":0,"elapsed":37,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["english_statements = [\n","    'Today is a good day.',\n","    'Tomorrow will be even better.',\n","    'Next week, who can say.'\n","]"]},{"cell_type":"code","execution_count":11,"id":"9c48fe36-d129-4fb1-add3-3e52f8f3ab56","metadata":{"id":"9c48fe36-d129-4fb1-add3-3e52f8f3ab56","executionInfo":{"status":"ok","timestamp":1757344799754,"user_tz":0,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompts = [translate_from_english_to_spanish(english_statement) for english_statement in english_statements]"]},{"cell_type":"code","execution_count":12,"id":"4cc1d4b2-3bc4-4e09-a77d-2a17e613d57d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cc1d4b2-3bc4-4e09-a77d-2a17e613d57d","executionInfo":{"status":"ok","timestamp":1757344805665,"user_tz":0,"elapsed":30,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"8b5eeb2a-7514-46f8-83fe-8dc3513b58aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Translate the following from English to Spanish. Provide just the translated text: Today is a good day.',\n"," 'Translate the following from English to Spanish. Provide just the translated text: Tomorrow will be even better.',\n"," 'Translate the following from English to Spanish. Provide just the translated text: Next week, who can say.']"]},"metadata":{},"execution_count":12}],"source":["prompts"]},{"cell_type":"code","execution_count":13,"id":"ba2286f3-da86-4334-b4d5-59fd6379602c","metadata":{"id":"ba2286f3-da86-4334-b4d5-59fd6379602c","executionInfo":{"status":"ok","timestamp":1757344814572,"user_tz":0,"elapsed":204,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["translations = llm.batch(prompts)"]},{"cell_type":"code","execution_count":14,"id":"b9ab3fef-c02e-4f65-97ef-48e9a5671d40","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9ab3fef-c02e-4f65-97ef-48e9a5671d40","executionInfo":{"status":"ok","timestamp":1757344816662,"user_tz":0,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"7e81d5fa-a39e-4aa9-831e-ad5afc0256c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hoy es un buen día.\n","Mañana será aún mejor.\n","La próxima semana, quién sabe.\n"]}],"source":["for translation in translations:\n","    print(translation.content)"]},{"cell_type":"markdown","id":"a7a9c684-ae49-4b63-b19d-ede7310901f1","metadata":{"id":"a7a9c684-ae49-4b63-b19d-ede7310901f1"},"source":["Our `translate_from_english_to_spanish` function therefore creates a **prompt template** that capture the functionality of translating an English statement to Spanish.\n","\n","Of course we could abstract even more out of our prompt and create an even more general template, if we wish:"]},{"cell_type":"code","execution_count":15,"id":"dfa89012-8a32-4997-b116-357ebd375d6a","metadata":{"id":"dfa89012-8a32-4997-b116-357ebd375d6a","executionInfo":{"status":"ok","timestamp":1757344845080,"user_tz":0,"elapsed":41,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["def translate(from_language, to_language, statement):\n","    return f\"Translate the following from {from_language} to {to_language}. Provide only the translated text: {statement}\""]},{"cell_type":"code","execution_count":16,"id":"c60840b2-7302-48e7-bfd3-4d4d912aabf6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c60840b2-7302-48e7-bfd3-4d4d912aabf6","executionInfo":{"status":"ok","timestamp":1757344852369,"user_tz":0,"elapsed":211,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"01ad5c3c-176d-401d-f1dc-117b5da6a129"},"outputs":[{"output_type":"stream","name":"stdout","text":["Les ordinateurs ont de nombreuses langues qui leur sont propres\n"]}],"source":["print(llm.invoke(translate('English', 'French', 'Computers have many languages of their own')).content)"]},{"cell_type":"markdown","id":"7bf4c7f1-1a94-4353-8597-114d94dc2d6e","metadata":{"id":"7bf4c7f1-1a94-4353-8597-114d94dc2d6e"},"source":["---"]},{"cell_type":"markdown","id":"d2d51590-3e47-49d5-8995-dfaa2fd88778","metadata":{"id":"d2d51590-3e47-49d5-8995-dfaa2fd88778"},"source":["## LangChain's `ChatPromptTemplate.from_template`"]},{"cell_type":"markdown","id":"1ade9b7b-0f87-4716-92bb-41abdc08990c","metadata":{"id":"1ade9b7b-0f87-4716-92bb-41abdc08990c"},"source":["There's nothing inherently wrong with developing your own conventions for creating prompt templates, but LangChain ships with a very large collection of templating mechanisms that are easy to use, flexible, well-maintained, and widely adopted.\n","\n","We will start with perhaps the most basic way to create prompt templates for a chat model `ChatPromptTemplate.from_template`. First we need to import `ChatPromptTemplate` into our environment."]},{"cell_type":"code","execution_count":17,"id":"f7771d98-2995-481e-9c5c-fc2fa7e7d7e2","metadata":{"id":"f7771d98-2995-481e-9c5c-fc2fa7e7d7e2","executionInfo":{"status":"ok","timestamp":1757344903781,"user_tz":0,"elapsed":41,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate"]},{"cell_type":"markdown","id":"c23803b3-f1ab-4a99-aa61-3f10063e524e","metadata":{"id":"c23803b3-f1ab-4a99-aa61-3f10063e524e"},"source":["We can now create a template, very much like we did above through the creation of a function. Let's revisit creating a template for translating an English statement to Spanish."]},{"cell_type":"code","execution_count":18,"id":"0dbac811-a1b4-481b-8a30-e33042570cdc","metadata":{"id":"0dbac811-a1b4-481b-8a30-e33042570cdc","executionInfo":{"status":"ok","timestamp":1757344928735,"user_tz":0,"elapsed":10,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["english_to_spanish_template = ChatPromptTemplate.from_template(\"\"\"Translate the following from English to Spanish. \\\n","Provide only the translated text: '{english_statement}'\"\"\")"]},{"cell_type":"markdown","id":"c11e8905-251a-46f9-a445-fdde9c6ffb3d","metadata":{"id":"c11e8905-251a-46f9-a445-fdde9c6ffb3d"},"source":["As you can see, this is pretty much identical to the f-string we returned in our `translate_from_english_to_spanish` function above.\n","\n","To create an actual prompt from the template, we use the template's `invoke` method."]},{"cell_type":"code","execution_count":19,"id":"59f85037-86a7-4b44-8b12-000e88652f74","metadata":{"id":"59f85037-86a7-4b44-8b12-000e88652f74","executionInfo":{"status":"ok","timestamp":1757344952175,"user_tz":0,"elapsed":10,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompt = english_to_spanish_template.invoke(\"Today is a good day.\")"]},{"cell_type":"markdown","id":"6ad6411d-1632-4a26-9f5e-beeb9609e61f","metadata":{"id":"6ad6411d-1632-4a26-9f5e-beeb9609e61f"},"source":["At which point we can now pass it into our LLM as we have been."]},{"cell_type":"code","execution_count":20,"id":"6c346a0c-ccc7-4020-b2dc-f74da0777262","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6c346a0c-ccc7-4020-b2dc-f74da0777262","executionInfo":{"status":"ok","timestamp":1757344954721,"user_tz":0,"elapsed":47,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"c7e11d99-cc2a-4eec-9760-4335e459418a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[HumanMessage(content=\"Translate the following from English to Spanish. Provide only the translated text: 'Today is a good day.'\", additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":20}],"source":["prompt"]},{"cell_type":"code","execution_count":21,"id":"073bdbbf-6313-415e-b7f6-43f4fd9d7607","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"073bdbbf-6313-415e-b7f6-43f4fd9d7607","executionInfo":{"status":"ok","timestamp":1757344972173,"user_tz":0,"elapsed":163,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"fb188a66-5c82-4d7f-cf5b-4d6181851f19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hoy es un buen día.\n"]}],"source":["print(llm.invoke(prompt).content)"]},{"cell_type":"markdown","id":"e74bfdc2-a8bb-4a62-b164-416de00e0db9","metadata":{"id":"e74bfdc2-a8bb-4a62-b164-416de00e0db9"},"source":["---"]},{"cell_type":"markdown","id":"33e37483-54f8-4832-be52-baa6de5cc02a","metadata":{"id":"33e37483-54f8-4832-be52-baa6de5cc02a"},"source":["## Chat Prompt Template Details"]},{"cell_type":"markdown","id":"ff73043b-7a9d-469e-b6fc-1a3c8011bfde","metadata":{"id":"ff73043b-7a9d-469e-b6fc-1a3c8011bfde"},"source":["If we look more carefully at the prompt we just created from the template, we'll see that there's a bit more going on than the creation of a string:"]},{"cell_type":"code","execution_count":22,"id":"c0fcc97c-2f14-45a7-af27-8f1c987b45e8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0fcc97c-2f14-45a7-af27-8f1c987b45e8","executionInfo":{"status":"ok","timestamp":1757344997094,"user_tz":0,"elapsed":38,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"e5faae58-b26a-457a-95e3-dc0ab31d4ea4"},"outputs":[{"output_type":"stream","name":"stdout","text":["messages=[HumanMessage(content=\"Translate the following from English to Spanish. Provide only the translated text: 'Today is a good day.'\", additional_kwargs={}, response_metadata={})]\n"]}],"source":["print(prompt)"]},{"cell_type":"markdown","id":"1ea17ca0-e344-45bb-89ff-272d5c9b1dba","metadata":{"id":"1ea17ca0-e344-45bb-89ff-272d5c9b1dba"},"source":["Under the hood, it would appear that LangChain is constructing a list of `messages`, and specifically, a `HumanMessage` whose `content` is the string prompt we intended to create.\n","\n","You'll be learning a lot throughout the workshop about `messages`, including `HumanMessage`s. When we use `OpenAI` library, we give it like this"]},{"cell_type":"markdown","id":"1f43983e-36c4-43b4-81f0-4cc0cc8c5fd2","metadata":{"id":"1f43983e-36c4-43b4-81f0-4cc0cc8c5fd2"},"source":["```python\n","response = client.chat.completions.create(\n","    model=model,\n","    messages=[{'role': 'user', 'content': prompt}]\n",")\n","```"]},{"cell_type":"markdown","id":"5e3ab781-2d0b-4fa0-8fbf-e7bf9102ac7d","metadata":{"id":"5e3ab781-2d0b-4fa0-8fbf-e7bf9102ac7d"},"source":["The important thing for you to understand now, is that when working with chat models specifically, the model expects interactions via messages in a turn-based structure where for each message, it is associated with a specific role like AI assistant, human user, or others.\n","\n","One of the great things about working with LangChain is that a lot of the specific formatting required to adhere to the expectations of working with a chat model are taken care of for you, but at the same time, when needed, you can drill down for full control of your program."]},{"cell_type":"markdown","id":"4f86c98b-2e40-4fc8-a2b1-6f6b01110e33","metadata":{"id":"4f86c98b-2e40-4fc8-a2b1-6f6b01110e33"},"source":["---"]},{"cell_type":"markdown","id":"7fb40c0d-33c3-46e8-9cba-afebfc7d0093","metadata":{"id":"7fb40c0d-33c3-46e8-9cba-afebfc7d0093"},"source":["## Prompt Templates for Multiple Values"]},{"cell_type":"code","execution_count":23,"id":"88075d6e-7db8-466f-b963-44ffb58ecc83","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88075d6e-7db8-466f-b963-44ffb58ecc83","executionInfo":{"status":"ok","timestamp":1757345070580,"user_tz":0,"elapsed":180,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"88a7ff22-cac1-472b-bd92-aaa3ac7eb666"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hoy es un buen día.\n"]}],"source":["english_to_spanish_template = ChatPromptTemplate.from_template(\"\"\"Translate the following from English to Spanish. \\\n","provide only the translated text '{english_statement}'\"\"\")\n","\n","prompt = english_to_spanish_template.invoke(\"Today is a good day.\")\n","\n","print(llm.invoke(prompt).content)"]},{"cell_type":"markdown","id":"8d61eb3b-6734-4e65-b519-8e7437c3b4f5","metadata":{"id":"8d61eb3b-6734-4e65-b519-8e7437c3b4f5"},"source":["You may have noticed when calling `invoke` on the template, that we passed in a single string..."]},{"cell_type":"code","execution_count":24,"id":"5bacdeb4-bbac-42e9-96d5-618811238cb5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5bacdeb4-bbac-42e9-96d5-618811238cb5","executionInfo":{"status":"ok","timestamp":1757345079760,"user_tz":0,"elapsed":45,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"eda46407-a1d7-4910-e911-89a44fde5748"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[HumanMessage(content=\"Translate the following from English to Spanish. provide only the translated text 'Today is a good day.'\", additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":24}],"source":["english_to_spanish_template.invoke(\"Today is a good day.\")"]},{"cell_type":"markdown","id":"62ea734b-cdd0-47bd-a50e-cdd5aa7bd7fe","metadata":{"id":"62ea734b-cdd0-47bd-a50e-cdd5aa7bd7fe"},"source":["...which in this case was fine since the template `\"Translate the following from English to Spanish: '{english_statement}'\"` was only expecting a single value, namely, `english_statement`.\n","\n","When invoking templates that expect multiple values, and in fact as a best practice even for templates that expect a single value, instead of passing in a string, we pass in a `dict` that maps the template placeholders to their intended values. Thus, a better way of invoking our template would be:"]},{"cell_type":"code","execution_count":25,"id":"d878a348-fcfe-43e0-859b-09e04e2d2f60","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d878a348-fcfe-43e0-859b-09e04e2d2f60","executionInfo":{"status":"ok","timestamp":1757345103156,"user_tz":0,"elapsed":59,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"5c20bd62-4d6d-4a62-c055-fe5cb2fc017d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[HumanMessage(content=\"Translate the following from English to Spanish. provide only the translated text 'Today is a good day.'\", additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":25}],"source":["english_to_spanish_template.invoke({\"english_statement\": \"Today is a good day.\"})"]},{"cell_type":"markdown","id":"c2d8587d-615c-459a-bed5-7670b0661bdc","metadata":{"id":"c2d8587d-615c-459a-bed5-7670b0661bdc"},"source":["...which as you can see results in the same prompt.\n","\n","Specifying which string maps to which placeholder via dictionary items becomes essential when working with prompts that expect multiple value. Here we demonstrate the creation and use of our more general template from above that allows for the translation from and to arbitrary languages."]},{"cell_type":"code","execution_count":26,"id":"08040584-c3e1-4493-a790-316249ef1873","metadata":{"id":"08040584-c3e1-4493-a790-316249ef1873","executionInfo":{"status":"ok","timestamp":1757345117285,"user_tz":0,"elapsed":26,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["translate_template = ChatPromptTemplate.from_template(\"Translate the following from {from_language} to {to_language}. \\\n","provide only the translated text: {statement}\")"]},{"cell_type":"code","execution_count":27,"id":"8dbaf0b6-9fcf-4bf3-8544-34437f4a512e","metadata":{"id":"8dbaf0b6-9fcf-4bf3-8544-34437f4a512e","executionInfo":{"status":"ok","timestamp":1757345122641,"user_tz":0,"elapsed":26,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompt = translate_template.invoke({\n","    \"from_language\": \"English\",\n","    \"to_language\": \"French\",\n","    \"statement\": \"Sometimes a little additional complexity is worth it.\"\n","})"]},{"cell_type":"code","execution_count":28,"id":"5d2cb039-a05a-4398-949f-ea10f96068f0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d2cb039-a05a-4398-949f-ea10f96068f0","executionInfo":{"status":"ok","timestamp":1757345124715,"user_tz":0,"elapsed":237,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"d0bbedca-512a-412f-fa90-149d192bd053"},"outputs":[{"output_type":"stream","name":"stdout","text":["Parfois, un peu de complexité supplémentaire en vaut la peine.\n"]}],"source":["print(llm.invoke(prompt).content)"]},{"cell_type":"markdown","id":"7c750701-1112-4cf8-b6ca-aae6205a738d","metadata":{"id":"7c750701-1112-4cf8-b6ca-aae6205a738d"},"source":["---"]},{"cell_type":"markdown","id":"db332813-03cf-4c59-8f92-9b60aeac80ff","metadata":{"id":"db332813-03cf-4c59-8f92-9b60aeac80ff"},"source":["## Exercise: Create Prompt Templates"]},{"cell_type":"markdown","id":"dbf03ee5-4c8f-4c79-b80f-27bd5f614e7a","metadata":{"id":"dbf03ee5-4c8f-4c79-b80f-27bd5f614e7a"},"source":["This exercise is a little longer than previous exercises and will conclude this section. For it, you are going to capture 3 LLM-related tasks into prompt templates, and apply each of these tasks to a list of statements we will provide you.\n","\n","The LLM-related tasks you should create templates for are:\n","- Sentiment analysis: ascertain whether the overall sentiment of a given piece of text is 'positive' or 'negative'.\n","- Main topic identification: identify and state the main topic for a given piece of text.\n","- Followup question generation: generate an appropriate and interesting followup question that will clarify some aspect of a given piece of text.\n","\n","Please use `statements` immediately below as the pieces text you should use for each of your templates. Upon completion you should be able to perform 3 LLM-related tasks on all 5 pieces of provided text."]},{"cell_type":"code","execution_count":null,"id":"ae8917fa-b01c-455d-9a64-199c260d3e47","metadata":{"id":"ae8917fa-b01c-455d-9a64-199c260d3e47"},"outputs":[],"source":["statements = [\n","    \"I had a fantastic time hiking up the mountain yesterday.\",\n","    \"The new restaurant downtown serves delicious vegetarian dishes.\",\n","    \"I am feeling quite stressed about the upcoming project deadline.\",\n","    \"Watching the sunset at the beach was a calming experience.\",\n","    \"I recently started reading a fascinating book about space exploration.\"\n","]"]},{"cell_type":"markdown","id":"cb38037b-62bb-4659-8fc4-860cca5b363f","metadata":{"id":"cb38037b-62bb-4659-8fc4-860cca5b363f"},"source":["If you're up for the challenge, feel free to begin your work straightaway. If you'd like some assistance, click on *Walkthrough* below to expand a step by step walkthrough of the exercise."]},{"cell_type":"markdown","id":"830ac0f4-f17a-4c79-91ee-e0e6a221842a","metadata":{"id":"830ac0f4-f17a-4c79-91ee-e0e6a221842a"},"source":["### Your Work Here"]},{"cell_type":"markdown","id":"2c3fdbed-16ac-43de-ac81-56ac94312aa7","metadata":{"id":"2c3fdbed-16ac-43de-ac81-56ac94312aa7"},"source":[]},{"cell_type":"markdown","id":"6568c779-a307-4014-825a-280d9c2807b8","metadata":{"id":"6568c779-a307-4014-825a-280d9c2807b8"},"source":["---"]},{"cell_type":"markdown","id":"962ae211-b423-40cc-a855-9bb88ed80b1c","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"962ae211-b423-40cc-a855-9bb88ed80b1c"},"source":["## Walkthrough"]},{"cell_type":"markdown","id":"96670dbf-9451-4510-aae0-132f475f3102","metadata":{"id":"96670dbf-9451-4510-aae0-132f475f3102"},"source":["### Prompt Template for Sentiment Analysis"]},{"cell_type":"markdown","id":"2231a44a-0d42-4039-9a86-9feb52fa8e18","metadata":{"id":"2231a44a-0d42-4039-9a86-9feb52fa8e18"},"source":["Start by constructing prompt templates for each of the LLM-related tasks we'd like to accomplish, beginning with a prompt template for sentiment analysis.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"e922a05e-2068-4fcc-8a93-37327511f8e1","metadata":{"id":"e922a05e-2068-4fcc-8a93-37327511f8e1"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"de1cf88f-87f2-42ef-8529-2a8d02997914","metadata":{"id":"de1cf88f-87f2-42ef-8529-2a8d02997914"},"outputs":[],"source":["sentiment_template = 'TODO'"]},{"cell_type":"markdown","id":"a21e28a9-9d0c-44ee-abab-8b2bfb40d041","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"a21e28a9-9d0c-44ee-abab-8b2bfb40d041"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"af24ea8b-07d5-40a1-a9c0-54e41f07076d","metadata":{"id":"af24ea8b-07d5-40a1-a9c0-54e41f07076d"},"outputs":[],"source":["sentiment_template = ChatPromptTemplate.from_template(\"\"\"In a single word, either 'positive' or 'negative', \\\n","provide the overall sentiment of the following piece of text: {text}\"\"\")"]},{"cell_type":"markdown","id":"9de8bdc3-49cc-4f67-9634-c7704764e2e7","metadata":{"id":"9de8bdc3-49cc-4f67-9634-c7704764e2e7"},"source":["### Prompt Template for Main Topic Identification"]},{"cell_type":"markdown","id":"becad504-79e9-4207-b88d-814b47694bea","metadata":{"id":"becad504-79e9-4207-b88d-814b47694bea"},"source":["Next create a prompt template for main topic identification.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"48159fe9-e432-4035-a402-3665cb874dbc","metadata":{"id":"48159fe9-e432-4035-a402-3665cb874dbc"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"ecc552b4-c287-46ff-a193-40d3094546da","metadata":{"id":"ecc552b4-c287-46ff-a193-40d3094546da"},"outputs":[],"source":["main_topic_template = 'TODO'"]},{"cell_type":"markdown","id":"918f4871-372c-45af-9dbb-49daed2db531","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"918f4871-372c-45af-9dbb-49daed2db531"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"e9b74f70-1242-4b4d-8820-692e88d80c84","metadata":{"id":"e9b74f70-1242-4b4d-8820-692e88d80c84"},"outputs":[],"source":["main_topic_template = ChatPromptTemplate.from_template(\"\"\"Identify and state, as concisely as possible, the main topic \\\n","of the following piece of text. Only provide the main topic and no other helpful comments. Text: {text}\"\"\")"]},{"cell_type":"markdown","id":"0d1d53b4-b62c-4207-9661-ef8f1073be65","metadata":{"id":"0d1d53b4-b62c-4207-9661-ef8f1073be65"},"source":["### Prompt Template for Followup Question Generation"]},{"cell_type":"markdown","id":"51e835b7-7eb8-4805-9976-2a58cfec336a","metadata":{"id":"51e835b7-7eb8-4805-9976-2a58cfec336a"},"source":["Next create a prompt template for generating a relevant followup question.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"7c3e0d58-d5f7-4e97-bebe-26d37b87c2db","metadata":{"id":"7c3e0d58-d5f7-4e97-bebe-26d37b87c2db"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"5aa9e0f3-4d76-4379-8304-0f4be36af843","metadata":{"id":"5aa9e0f3-4d76-4379-8304-0f4be36af843"},"outputs":[],"source":["followup_template = 'TODO'"]},{"cell_type":"markdown","id":"ab89e6df-8e67-437f-a9ab-8dc813640315","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"ab89e6df-8e67-437f-a9ab-8dc813640315"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"f5bacb76-cded-4ed9-bf7f-c5b0d4db9eab","metadata":{"id":"f5bacb76-cded-4ed9-bf7f-c5b0d4db9eab"},"outputs":[],"source":["followup_template = ChatPromptTemplate.from_template(\"\"\"What is an appropriate and interesting followup question that would help \\\n","me learn more about the provided text? Only supply the question. Text: {text}\"\"\")"]},{"cell_type":"markdown","id":"6b35bc56-8bd5-4846-8e75-d02d99c6eec8","metadata":{"id":"6b35bc56-8bd5-4846-8e75-d02d99c6eec8"},"source":["### Create Lists of Prompts for Future Batching"]},{"cell_type":"markdown","id":"87b1d9c2-6fe6-43de-bedc-415c4a41c0b0","metadata":{"id":"87b1d9c2-6fe6-43de-bedc-415c4a41c0b0"},"source":["In order to generate batched responses for each of our 3 LLM-related tasks, we will need a list of prompts for each task.\n","\n","For this step of the exercise, use `statements` (defined above) in conjunction with each of the prompt templates you just created to create a list of prompts for each of the 3 LLM-related tasks you'd like to accomplish.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"60967e4c-3b4c-4725-93b8-31623dc4e15b","metadata":{"id":"60967e4c-3b4c-4725-93b8-31623dc4e15b"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"e877a45b-d8cd-4d65-9142-22f06c936b0c","metadata":{"id":"e877a45b-d8cd-4d65-9142-22f06c936b0c"},"outputs":[],"source":["sentiment_prompts = [] # TODO: populate with sentiment analysis prompts for each statement in `statements`.\n","main_topic_prompts = [] # TODO: populate with main topic prompts for each statement in `statements`.\n","followup_prompts = [] # TODO: populate with followup question prompts for each statement in `statements`."]},{"cell_type":"markdown","id":"bc26e53a-8da2-42f7-851f-3235f8086ef1","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"bc26e53a-8da2-42f7-851f-3235f8086ef1"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"3dd2d2d3-7b4a-48ff-9390-cda6c2bdb40b","metadata":{"id":"3dd2d2d3-7b4a-48ff-9390-cda6c2bdb40b"},"outputs":[],"source":["sentiment_prompts = [sentiment_template.invoke({\"text\": statement}) for statement in statements]"]},{"cell_type":"code","execution_count":null,"id":"18a476d0-7bdf-4a3f-870e-13647ed8e4fd","metadata":{"id":"18a476d0-7bdf-4a3f-870e-13647ed8e4fd"},"outputs":[],"source":["main_topic_prompts = [main_topic_template.invoke({\"text\": statement}) for statement in statements]"]},{"cell_type":"code","execution_count":null,"id":"92658911-73e1-43bc-ad58-fb2b9945df1b","metadata":{"id":"92658911-73e1-43bc-ad58-fb2b9945df1b"},"outputs":[],"source":["followup_prompts = [followup_template.invoke({\"text\": statement}) for statement in statements]"]},{"cell_type":"markdown","id":"1f51e21e-3bbc-4671-a891-99617e65f025","metadata":{"id":"1f51e21e-3bbc-4671-a891-99617e65f025"},"source":["### Generate Responses for Each LLM Tasks Using Batching"]},{"cell_type":"markdown","id":"bf90da80-9870-455f-86f1-be24a3a98819","metadata":{"id":"bf90da80-9870-455f-86f1-be24a3a98819"},"source":["Use batching to call the LLM with each of your constructed prompts, once for each task.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"7f2d746f-4cf7-4f07-847d-39d199134b02","metadata":{"id":"7f2d746f-4cf7-4f07-847d-39d199134b02"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"50b1474e-07d0-42e5-8fb8-62de74504be0","metadata":{"id":"50b1474e-07d0-42e5-8fb8-62de74504be0"},"outputs":[],"source":["sentiments = [] # TODO: use the LLM to populate this list with the sentiment of each statement in `statements`.\n","main_topics = [] # TODO: use the LLM to populate this list with the main topic of each statement in `statements`.\n","followups = [] # TODO: use the LLM to populate this list with a followup question for each statement in `statements`."]},{"cell_type":"markdown","id":"93af4dbd-dd89-4f8d-9f83-cbf54def542f","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"93af4dbd-dd89-4f8d-9f83-cbf54def542f"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"d1e4580e-2828-4255-94f9-082cc9185571","metadata":{"id":"d1e4580e-2828-4255-94f9-082cc9185571"},"outputs":[],"source":["sentiments = llm.batch(sentiment_prompts)"]},{"cell_type":"code","execution_count":null,"id":"b2c16f56-54a7-42cd-a580-0e0550723409","metadata":{"id":"b2c16f56-54a7-42cd-a580-0e0550723409"},"outputs":[],"source":["main_topics = llm.batch(main_topic_prompts)"]},{"cell_type":"code","execution_count":null,"id":"ad8c7249-2f85-4b9d-b4e1-aa79e80a7661","metadata":{"id":"ad8c7249-2f85-4b9d-b4e1-aa79e80a7661"},"outputs":[],"source":["followups = llm.batch(followup_prompts)"]},{"cell_type":"markdown","id":"7cc219c9-7dba-427e-bd8a-41f4413f4c59","metadata":{"id":"7cc219c9-7dba-427e-bd8a-41f4413f4c59"},"source":["### Print Results"]},{"cell_type":"markdown","id":"2d8370f2-1718-42d7-9c67-a46c8f8b38e6","metadata":{"id":"2d8370f2-1718-42d7-9c67-a46c8f8b38e6"},"source":["Finally, loop over the original statements and all the model responses for the various topics to make a nice print out of everthing.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"markdown","id":"fe37efac-77ec-4092-ab12-30cfd3cb028e","metadata":{"id":"fe37efac-77ec-4092-ab12-30cfd3cb028e"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"63ca4b7c-aec4-4be4-b6d6-f2b9a45002e6","metadata":{"id":"63ca4b7c-aec4-4be4-b6d6-f2b9a45002e6"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e21ef99b-8c85-4d07-b09a-5a391284f0fa","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"e21ef99b-8c85-4d07-b09a-5a391284f0fa"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"id":"74d84bec-4ed0-46ef-ae83-4d423c3ddfc4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74d84bec-4ed0-46ef-ae83-4d423c3ddfc4","executionInfo":{"status":"ok","timestamp":1756779188327,"user_tz":-330,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"b2988ec5-de00-40d8-93b1-957d61e7a453"},"outputs":[{"output_type":"stream","name":"stdout","text":["Statement: I had a fantastic time hiking up the mountain yesterday.\n","Overall sentiment: Positive.\n","Main topic: Hiking\n","Followup question: What made the hike so fantastic, was it the scenery, the challenge, or something else?\n","\n","Statement: The new restaurant downtown serves delicious vegetarian dishes.\n","Overall sentiment: Positive.\n","Main topic: New restaurant downtown\n","Followup question: What inspired the chef to create a menu focused on vegetarian options?\n","\n","Statement: I am feeling quite stressed about the upcoming project deadline.\n","Overall sentiment: Negative.\n","Main topic: Project deadline stress\n","Followup question: What specific aspects of the project are causing you the most stress and anxiety?\n","\n","Statement: Watching the sunset at the beach was a calming experience.\n","Overall sentiment: Positive.\n","Main topic: Beach sunset experience\n","Followup question: What made the experience of watching the sunset at the beach particularly calming for you?\n","\n","Statement: I recently started reading a fascinating book about space exploration.\n","Overall sentiment: Positive.\n","Main topic: Space exploration\n","Followup question: What sparked your interest in reading about space exploration, and how is the book meeting your expectations so far?\n","\n"]}],"source":["for statement, sentiment, main_topic, followup in zip(statements, sentiments, main_topics, followups):\n","    print(\n","        f\"Statement: {statement}\\n\"\n","        f\"Overall sentiment: {sentiment.content}\\n\"\n","        f\"Main topic: {main_topic.content}\\n\"\n","        f\"Followup question: {followup.content}\\n\"\n","    )"]},{"cell_type":"markdown","id":"3e0ed26d-7851-49a8-8afc-0f7489105115","metadata":{"id":"3e0ed26d-7851-49a8-8afc-0f7489105115"},"source":["---"]},{"cell_type":"markdown","id":"0e0180ae-c224-41ca-8111-e2e0091e123a","metadata":{"id":"0e0180ae-c224-41ca-8111-e2e0091e123a"},"source":["## Summary"]},{"cell_type":"markdown","id":"a5cb5233-a5f4-491c-9db9-ca7d5a31bb95","metadata":{"id":"a5cb5233-a5f4-491c-9db9-ca7d5a31bb95"},"source":["Through a combination of capturing LLM-related functionality in prompt templates, and batching calls to a chat model, you're already starting to peform legitimate work. Even with what you know thus far, it's not hard to see how you could easily extend it to create a serious amount of LLM-powered analysis and text generation on large collections of textual data.\n","\n","However, we are just getting started. In the next notebook we are going to introduce something called LangChain Expression Language (LCEL) which will enable you to create concise but powerful chains of LLM application functionality. As you'll see, using LCEL chains will allow us to create functionalilty, much like you did in the previous exercise, much more efficiently."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}