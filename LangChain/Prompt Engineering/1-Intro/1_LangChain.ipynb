{"cells":[{"cell_type":"markdown","id":"66d5adbf-f5d6-4792-8c5e-64a73b2d3a3d","metadata":{"id":"66d5adbf-f5d6-4792-8c5e-64a73b2d3a3d"},"source":["# Hello World With LangChain"]},{"cell_type":"markdown","id":"28013522","metadata":{"id":"28013522"},"source":["In this notebook, we will learn how to interact with LangChain to generate chat completions using the Llama 3.1 8b instruct model. This introductory exercise will help you understand the basics of setting up and using LangChain in a Jupyter environment."]},{"cell_type":"markdown","id":"ea5a70fb-0429-4036-82ce-a55c4262561a","metadata":{"id":"ea5a70fb-0429-4036-82ce-a55c4262561a"},"source":["---"]},{"cell_type":"markdown","id":"c08054f2","metadata":{"id":"c08054f2"},"source":["## Objectives"]},{"cell_type":"markdown","id":"a023bc7a-47b5-4508-957c-f3354c9fb363","metadata":{"id":"a023bc7a-47b5-4508-957c-f3354c9fb363"},"source":["By the time you complete this notebook, you will:\n","\n","- Have an introductory understanding of LangChain.\n","- Generate simple chat completions using LangChain.\n"]},{"cell_type":"markdown","id":"9a291cd0-5701-41dc-b3a4-229bce728f10","metadata":{"id":"9a291cd0-5701-41dc-b3a4-229bce728f10"},"source":["---"]},{"cell_type":"markdown","id":"327550d4","metadata":{"id":"327550d4"},"source":["## Imports\n","\n","\n"]},{"cell_type":"markdown","id":"ee597c1a-500b-4eea-8b89-a359988fa534","metadata":{"id":"ee597c1a-500b-4eea-8b89-a359988fa534"},"source":["\n","There has been a lot of effort from developers to utilize AI applications efficiently. Amongst them, [LangChain](https://python.langchain.com/v0.2/docs/introduction/) is a popular LLM orchestration framework that aids users to interact with LLMs easily.\n","\n","LangChain’s simplistic architecture and abstractions let developers effortlessly replace components like language models, prompt, and processing steps, with little modification. In addition, LangChain provides a consistent, unified interface across multiple LLMs from different providers, simplifying interactions and allowing developers to concentrate on application development rather than dealing with model-specific complexities.\n","\n","This library is highly popular and evolves quickly with advancements in the field. While there are many parts of LangChain such as LangGraph, LangSmith, and LangServe, we are going to focus on LangChain core in our workshop today.\n","\n"]},{"cell_type":"markdown","id":"389a972e-fce8-4af5-9a7e-ed940a4fa800","metadata":{"id":"389a972e-fce8-4af5-9a7e-ed940a4fa800"},"source":["---"]},{"cell_type":"markdown","id":"b2e2f950-1450-4f55-a4b3-ed2fbc987513","metadata":{"id":"b2e2f950-1450-4f55-a4b3-ed2fbc987513"},"source":["## Setting Up a Model Instance With LangChain"]},{"cell_type":"code","source":["!pip install groq langchain-groq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OkAY-hxHm6S8","executionInfo":{"status":"ok","timestamp":1757339085269,"user_tz":0,"elapsed":3994,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"87d86084-edcd-4dfa-db10-87e2cc94fb92","collapsed":true},"id":"OkAY-hxHm6S8","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n","Requirement already satisfied: langchain-groq in /usr/local/lib/python3.12/dist-packages (0.3.7)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.75)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.23)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.4)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.24.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n"]}]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key:\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8uvTjbJm8PB","executionInfo":{"status":"ok","timestamp":1757339105588,"user_tz":0,"elapsed":10113,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"f9aedf6c-9a22-432c-974d-efb8b73c505b"},"id":"d8uvTjbJm8PB","execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":["GROQ API Key:\n","··········\n"]}]},{"cell_type":"code","source":["from langchain_groq import ChatGroq"],"metadata":{"id":"OTlkHAttnjQ2","executionInfo":{"status":"ok","timestamp":1757339111251,"user_tz":0,"elapsed":1240,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"id":"OTlkHAttnjQ2","execution_count":5,"outputs":[]},{"cell_type":"code","source":["llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)"],"metadata":{"id":"Vtm4vmwXnaVR","executionInfo":{"status":"ok","timestamp":1757339112691,"user_tz":0,"elapsed":314,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"id":"Vtm4vmwXnaVR","execution_count":6,"outputs":[]},{"cell_type":"markdown","id":"4443786e-e1f6-4f7e-92ae-ca49cf18a571","metadata":{"id":"4443786e-e1f6-4f7e-92ae-ca49cf18a571"},"source":["## Making a Simple Request"]},{"cell_type":"code","execution_count":7,"id":"4e779016-7071-4b44-aef0-72da438ebba0","metadata":{"id":"4e779016-7071-4b44-aef0-72da438ebba0","executionInfo":{"status":"ok","timestamp":1757339124505,"user_tz":0,"elapsed":416,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompt = 'Who are you?'\n","result = llm.invoke(prompt)"]},{"cell_type":"code","execution_count":8,"id":"607b5258-a3af-4f69-92d9-0954bfb9f591","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"607b5258-a3af-4f69-92d9-0954bfb9f591","executionInfo":{"status":"ok","timestamp":1757339125761,"user_tz":0,"elapsed":10,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"56b5c22a-3483-4cef-ef8a-3458aea4736c"},"outputs":[{"output_type":"stream","name":"stdout","text":["content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 39, 'total_tokens': 62, 'completion_time': 0.028452794, 'prompt_time': 0.010878633, 'queue_time': 0.208779624, 'total_time': 0.039331427}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--563a800c-2d3c-44f2-a820-1bbbe8da3019-0' usage_metadata={'input_tokens': 39, 'output_tokens': 23, 'total_tokens': 62}\n"]}],"source":["print(result)"]},{"cell_type":"markdown","id":"092f218c-b0ec-4ec0-ade3-906f3f4edc82","metadata":{"id":"092f218c-b0ec-4ec0-ade3-906f3f4edc82"},"source":["The result includes metadata about the conversation and token usage. This will be useful for maintaining conversation context in more advanced applications.\n","\n","To extract just the response from the model, we can simply use the result's `content` property, as below."]},{"cell_type":"code","execution_count":9,"id":"4d02fcfa-0e81-46ca-9480-61fb0ce0e764","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4d02fcfa-0e81-46ca-9480-61fb0ce0e764","executionInfo":{"status":"ok","timestamp":1757339231038,"user_tz":0,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"df8f989c-6c09-458f-805b-64b7b709cbd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"]}],"source":["print(result.content)"]},{"cell_type":"markdown","id":"2973e10f-0bf4-4fde-851c-410fc960544a","metadata":{"id":"2973e10f-0bf4-4fde-851c-410fc960544a"},"source":["---"]},{"cell_type":"markdown","id":"f2839a2d-65e3-41b3-afd9-ee0982441d67","metadata":{"id":"f2839a2d-65e3-41b3-afd9-ee0982441d67"},"source":["## Exercise: Generate Your Own Completion"]},{"cell_type":"markdown","id":"dd0d6caf-fa74-4709-bd3b-07b70446a416","metadata":{"id":"dd0d6caf-fa74-4709-bd3b-07b70446a416"},"source":["Use our existing model instance `llm` to generate and print a response from our local Llama 3.3 model to a prompt of your choice."]},{"cell_type":"markdown","id":"4bd133b8-f6cb-4cd1-b0a1-5c114531fba9","metadata":{"id":"4bd133b8-f6cb-4cd1-b0a1-5c114531fba9"},"source":["### Your Work Here"]},{"cell_type":"code","execution_count":null,"id":"0142c794-0aa9-4533-a6fe-58bb3ea68f11","metadata":{"id":"0142c794-0aa9-4533-a6fe-58bb3ea68f11"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"994857c1-9d1a-4383-8eb8-818daf659768","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"994857c1-9d1a-4383-8eb8-818daf659768"},"source":["### Solution"]},{"cell_type":"code","execution_count":10,"id":"55d2e2a3-63bd-4b9b-93ac-dbba2830947f","metadata":{"id":"55d2e2a3-63bd-4b9b-93ac-dbba2830947f","executionInfo":{"status":"ok","timestamp":1757339272286,"user_tz":0,"elapsed":672,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompt = 'Give me 3 puns having to do with LangChain.'\n","result = llm.invoke(prompt)"]},{"cell_type":"code","execution_count":11,"id":"6fbd103e-86a3-4992-bda4-bf221a0a4fb3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fbd103e-86a3-4992-bda4-bf221a0a4fb3","executionInfo":{"status":"ok","timestamp":1757339273239,"user_tz":0,"elapsed":6,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"00eb7b5b-0404-437c-8aff-9e28ae912b07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here are three LangChain puns for you:\n","\n","1. LangChain is the \"link\" to a brighter AI future.\n","2. Why did LangChain go to therapy? It had a few \"chain\" reactions to work through.\n","3. LangChain is \"chaining\" the game when it comes to large language models – it's the missing \"link\" to unlocking their full potential.\n"]}],"source":["print(result.content)"]},{"cell_type":"markdown","id":"2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840","metadata":{"id":"2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840"},"source":["---"]},{"cell_type":"markdown","id":"dd5025fa-b314-4565-a199-01396dc2252c","metadata":{"id":"dd5025fa-b314-4565-a199-01396dc2252c"},"source":["## Summary"]},{"cell_type":"markdown","id":"2b1c72d8-4186-4b6c-a9ac-c8b87e0ff9d3","metadata":{"id":"2b1c72d8-4186-4b6c-a9ac-c8b87e0ff9d3"},"source":["By completing this notebook, you should now have a basic understanding of how to use LangChain to generate chat completions and parse out the model response, which we hope you'll agree, is quite straight forward.\n","\n","In the next notebook, you'll go a little further into using chat completions with LangChain by learning how to stream model responses and handle multiple chat completion requests in batches."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}