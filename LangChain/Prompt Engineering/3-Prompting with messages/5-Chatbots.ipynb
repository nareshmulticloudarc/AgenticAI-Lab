{"cells":[{"cell_type":"markdown","id":"108dd8bb-c86c-40fd-a164-991673b2cc21","metadata":{"id":"108dd8bb-c86c-40fd-a164-991673b2cc21"},"source":["# Chatbots"]},{"cell_type":"markdown","id":"28013522","metadata":{"id":"28013522"},"source":["In this notebook, you will learn how to store conversation history and thereby enable chatbot functionality in your LLM-based chains."]},{"cell_type":"markdown","id":"69366671-11a4-4439-b6ad-cb89497ef5d4","metadata":{"id":"69366671-11a4-4439-b6ad-cb89497ef5d4"},"source":["---"]},{"cell_type":"markdown","id":"c08054f2","metadata":{"id":"c08054f2"},"source":["## Objectives"]},{"cell_type":"markdown","id":"a023bc7a-47b5-4508-957c-f3354c9fb363","metadata":{"id":"a023bc7a-47b5-4508-957c-f3354c9fb363"},"source":["By the time you complete this notebook you will:\n","\n","- Understand the core principles and techniques required to create chatbot applications capable of retaining conversation history.\n","- Create easy-to-use chatbots, capable of assuming a variety of different personas.\n","- Interact with a simple chatbot application interface, well-suited for chatbot application prototyping."]},{"cell_type":"markdown","id":"f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb","metadata":{"id":"f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb"},"source":["---"]},{"cell_type":"markdown","id":"327550d4","metadata":{"id":"327550d4"},"source":["## Imports"]},{"cell_type":"code","source":["!pip install groq langchain-groq"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"e0exqpO0IUW_","executionInfo":{"status":"ok","timestamp":1757514766051,"user_tz":-330,"elapsed":10352,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"5ed12f55-871e-4652-9bd3-28bfdd8810aa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n","Collecting langchain-groq\n","  Downloading langchain_groq-0.3.8-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.75)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (0.4.24)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (6.0.2)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (2.32.4)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (0.24.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (2.5.0)\n","Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_groq-0.3.8-py3-none-any.whl (16 kB)\n","Installing collected packages: groq, langchain-groq\n","Successfully installed groq-0.31.1 langchain-groq-0.3.8\n"]}],"id":"e0exqpO0IUW_"},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"GROQ API Key:\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_1hA-z9F9mA","executionInfo":{"status":"ok","timestamp":1757514769643,"user_tz":-330,"elapsed":3582,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"f3427ffd-4012-43c4-e361-2696ecfcc58b"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["GROQ API Key:\n","··········\n"]}],"id":"Y_1hA-z9F9mA"},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"status":"ok","timestamp":1757514770943,"user_tz":-330,"elapsed":1290,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"id":"6548uO7XIR8d"},"outputs":[],"source":["from langchain_groq import ChatGroq\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser"],"id":"6548uO7XIR8d"},{"cell_type":"markdown","id":"9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d","metadata":{"id":"9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d"},"source":["---"]},{"cell_type":"markdown","id":"5c0e1244","metadata":{"id":"5c0e1244"},"source":["## Create a Model Instance"]},{"cell_type":"code","source":["llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)"],"metadata":{"id":"qsL_3kqsK_4P","executionInfo":{"status":"ok","timestamp":1757514771360,"user_tz":-330,"elapsed":406,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"id":"qsL_3kqsK_4P","execution_count":4,"outputs":[]},{"cell_type":"markdown","id":"b2a97370-d9b4-41d4-995e-47d2f80e2b73","metadata":{"id":"b2a97370-d9b4-41d4-995e-47d2f80e2b73"},"source":["---"]},{"cell_type":"markdown","id":"4f694b45-d57d-49bb-aedb-158a34d49997","metadata":{"id":"4f694b45-d57d-49bb-aedb-158a34d49997"},"source":["## Placeholder Messages"]},{"cell_type":"markdown","id":"7b86f9a6-c6ed-4e36-98c3-3bf2333574e5","metadata":{"id":"7b86f9a6-c6ed-4e36-98c3-3bf2333574e5"},"source":["Before we begin work enabling conversation history and chatbot functionality, we need to introduce a new kind of message that we have not yet covered, **placeholder** messages.\n","\n","Put simply, and as the name suggest, a placeholder message is used in a prompt template to hold the place of a list of other messages."]},{"cell_type":"code","execution_count":5,"id":"ed769665-d4fd-4987-871a-103c7820cad8","metadata":{"id":"ed769665-d4fd-4987-871a-103c7820cad8","executionInfo":{"status":"ok","timestamp":1757514800911,"user_tz":-330,"elapsed":7,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["template_with_placeholder = ChatPromptTemplate.from_messages([\n","    ('placeholder', '{messages}'),\n","    ('human', '{prompt}')\n","])"]},{"cell_type":"code","execution_count":6,"id":"b3c746c1-f1fa-41b7-86b0-0bad147c28b0","metadata":{"id":"b3c746c1-f1fa-41b7-86b0-0bad147c28b0","executionInfo":{"status":"ok","timestamp":1757514804738,"user_tz":-330,"elapsed":8,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["messages = [\n","    ('human', 'The sun came up today.'),\n","    ('ai', 'That is wonderful!'),\n","    ('human', 'The sun went down today.'),\n","    ('ai', 'That is also wonderful!.')\n","]"]},{"cell_type":"code","execution_count":7,"id":"261f7cf8-017d-4028-95b5-37e4da2d4887","metadata":{"id":"261f7cf8-017d-4028-95b5-37e4da2d4887","executionInfo":{"status":"ok","timestamp":1757514813778,"user_tz":-330,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["prompt = 'What happened today?'"]},{"cell_type":"markdown","id":"ed0b9906-606e-4df8-a4d5-f585ce5f972c","metadata":{"id":"ed0b9906-606e-4df8-a4d5-f585ce5f972c"},"source":["When invoking (or streaming or batching) prompt templates or chains that contain placeholder messages, we provide a value as the template indicates, only in the case of a placeholder message, we provide a list of other messages, rather than a string.\n","\n","Here we invoke `template_with_placeholder`, passing in the `messages` list to fulfil the template's `messages` parameter, and the `prompt` string to fulfil its `prompt` parameter."]},{"cell_type":"code","execution_count":8,"id":"597606a8-e779-4008-afce-acda2c5c70c0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"597606a8-e779-4008-afce-acda2c5c70c0","executionInfo":{"status":"ok","timestamp":1757514902818,"user_tz":-330,"elapsed":60,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"82e7a877-880a-41ed-89c3-caf03c7c65fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[HumanMessage(content='The sun came up today.', additional_kwargs={}, response_metadata={}), AIMessage(content='That is wonderful!', additional_kwargs={}, response_metadata={}), HumanMessage(content='The sun went down today.', additional_kwargs={}, response_metadata={}), AIMessage(content='That is also wonderful!.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What happened today?', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":8}],"source":["template_with_placeholder.invoke({'messages': messages, 'prompt': prompt})"]},{"cell_type":"markdown","id":"a4fca50f-1f15-483c-aa7a-7ced61bb42ac","metadata":{"id":"a4fca50f-1f15-483c-aa7a-7ced61bb42ac"},"source":["As you can see, LangChain expanded the `placeholder` message, for which we provided a list, into a list of individual messages that we provided when invoking the prompt template."]},{"cell_type":"markdown","id":"47bf6210-c1b7-48a4-9836-2bfca0cdf4d3","metadata":{"id":"47bf6210-c1b7-48a4-9836-2bfca0cdf4d3"},"source":["It should come as no surprise that we can use this prompt template in chains just as we would any other."]},{"cell_type":"code","execution_count":9,"id":"beaab7c9-4b68-425f-9707-41a5f2fd016b","metadata":{"id":"beaab7c9-4b68-425f-9707-41a5f2fd016b","executionInfo":{"status":"ok","timestamp":1757514922132,"user_tz":-330,"elapsed":14,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chain = template_with_placeholder | llm | StrOutputParser()"]},{"cell_type":"code","execution_count":10,"id":"da24eef3-f18a-468a-a95f-822fc4c59c37","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"da24eef3-f18a-468a-a95f-822fc4c59c37","executionInfo":{"status":"ok","timestamp":1757514924749,"user_tz":-330,"elapsed":540,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"0f80f703-dece-4880-ecec-df7204d33753"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"According to our conversation, the sun came up and then went down today, which is a normal part of a day. Is there anything else you'd like to share or talk about that happened today?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["chain.invoke({'messages': messages, 'prompt': prompt})"]},{"cell_type":"markdown","id":"34516e58-a46d-432f-8d9b-20bb78df575f","metadata":{"id":"34516e58-a46d-432f-8d9b-20bb78df575f"},"source":["---"]},{"cell_type":"markdown","id":"1fa2541e-bd11-4bbe-892c-cd6b1b4b2059","metadata":{"id":"1fa2541e-bd11-4bbe-892c-cd6b1b4b2059"},"source":["## Rudimentary Conversation History"]},{"cell_type":"markdown","id":"84b7e30f-7e29-4d9b-be66-828a79fa4d31","metadata":{"id":"84b7e30f-7e29-4d9b-be66-828a79fa4d31"},"source":["We can easily construct a rudimentary conversation history mechanism using a message placeholder. First we'll create a prompt template utilizing a placeholder, and use it in a simple chain."]},{"cell_type":"code","execution_count":11,"id":"7983475b-98e6-476d-a41b-b6b17441aac8","metadata":{"id":"7983475b-98e6-476d-a41b-b6b17441aac8","executionInfo":{"status":"ok","timestamp":1757514976272,"user_tz":-330,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chat_conversation_template = ChatPromptTemplate.from_messages([\n","    ('placeholder', '{chat_conversation}')\n","])"]},{"cell_type":"code","execution_count":12,"id":"65fd62a9-3977-44b7-9e05-748c2a05030b","metadata":{"id":"65fd62a9-3977-44b7-9e05-748c2a05030b","executionInfo":{"status":"ok","timestamp":1757514981673,"user_tz":-330,"elapsed":14,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chat_chain = chat_conversation_template | llm | StrOutputParser()"]},{"cell_type":"markdown","id":"103aa6de-cdbd-4644-a2fd-81eb5a5eee01","metadata":{"id":"103aa6de-cdbd-4644-a2fd-81eb5a5eee01"},"source":["Next we'll create a list to store our conversation, which we will add to over time."]},{"cell_type":"code","execution_count":13,"id":"439c6420-813f-4a98-8c0e-4c1c76f42d16","metadata":{"id":"439c6420-813f-4a98-8c0e-4c1c76f42d16","executionInfo":{"status":"ok","timestamp":1757514985286,"user_tz":-330,"elapsed":18,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chat_conversation = []"]},{"cell_type":"markdown","id":"fc246a3e-aad6-4a72-a018-45998cfc79bd","metadata":{"id":"fc246a3e-aad6-4a72-a018-45998cfc79bd"},"source":["We will begin by appending our first prompt, as a `user` message to the `chat_conversation` list."]},{"cell_type":"code","execution_count":14,"id":"c507c3c5-56b7-4660-a4b2-a05b51b45840","metadata":{"id":"c507c3c5-56b7-4660-a4b2-a05b51b45840","executionInfo":{"status":"ok","timestamp":1757514991044,"user_tz":-330,"elapsed":18,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chat_conversation.append(('user', 'Hello, my name is Michael.'))"]},{"cell_type":"markdown","id":"6dcf30b9-44fd-4353-85d3-78d8ddfcedbe","metadata":{"id":"6dcf30b9-44fd-4353-85d3-78d8ddfcedbe"},"source":["Just to test it out, we can now invoke our `chat_chain` with the current `chat_conversation` list."]},{"cell_type":"code","execution_count":15,"id":"2e7d18de-e74d-4641-bc44-24e43fee4107","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"2e7d18de-e74d-4641-bc44-24e43fee4107","executionInfo":{"status":"ok","timestamp":1757515002521,"user_tz":-330,"elapsed":536,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"c478935c-bbc2-4406-f519-12c434c628d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Hello Michael, it's nice to meet you. Is there something I can help you with or would you like to chat?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["chat_chain.invoke({'chat_conversation': chat_conversation})"]},{"cell_type":"markdown","id":"8e69e1c6-4ff4-4100-8aa9-2837da5fc191","metadata":{"id":"8e69e1c6-4ff4-4100-8aa9-2837da5fc191"},"source":["It looks like the LLM is able to respond just fine. Since we are wanting to keep track of the conversation history, however, let's invoke the chain again, but this time append the response to the `chat_conversation` list as an `ai` message."]},{"cell_type":"code","execution_count":16,"id":"bc2a5f46-30f3-4ad6-a1ba-11a44da0d497","metadata":{"id":"bc2a5f46-30f3-4ad6-a1ba-11a44da0d497","executionInfo":{"status":"ok","timestamp":1757515027967,"user_tz":-330,"elapsed":418,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["response = chat_chain.invoke({'chat_conversation': chat_conversation})\n","chat_conversation.append(('ai', response))"]},{"cell_type":"markdown","id":"7caacfc9-88c0-4245-9162-eba794a6c3cd","metadata":{"id":"7caacfc9-88c0-4245-9162-eba794a6c3cd"},"source":["Looking at `chat_conversation` we see it now contains a list of the messages thus far."]},{"cell_type":"code","execution_count":17,"id":"6a004539-22ad-451f-a117-04082703aeff","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6a004539-22ad-451f-a117-04082703aeff","executionInfo":{"status":"ok","timestamp":1757515038849,"user_tz":-330,"elapsed":19,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"a671f858-8168-4c2b-d552-d1b0e2ce356a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('user', 'Hello, my name is Michael.'),\n"," ('ai',\n","  \"Hello Michael, it's nice to meet you. Is there something I can help you with or would you like to chat?\")]"]},"metadata":{},"execution_count":17}],"source":["chat_conversation"]},{"cell_type":"markdown","id":"7953de91-b8c6-413c-b26a-eb2c956af601","metadata":{"id":"7953de91-b8c6-413c-b26a-eb2c956af601"},"source":["Let's repeat this same process with a new message, and let's pass in a prompt that relies on previous conversation history to answer correctly."]},{"cell_type":"code","execution_count":18,"id":"56199c2f-22ea-42cc-bb9f-4c103df645c8","metadata":{"id":"56199c2f-22ea-42cc-bb9f-4c103df645c8","executionInfo":{"status":"ok","timestamp":1757515043952,"user_tz":-330,"elapsed":13,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chat_conversation.append(('user', 'Do you remember what my name is?'))"]},{"cell_type":"code","execution_count":19,"id":"c9cc2d70-d0d1-4cd6-9236-ca6b2bed4735","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c9cc2d70-d0d1-4cd6-9236-ca6b2bed4735","executionInfo":{"status":"ok","timestamp":1757515055807,"user_tz":-330,"elapsed":365,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"88702858-e2a8-4e8d-c1c1-e17235051580"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('user', 'Hello, my name is Michael.'),\n"," ('ai',\n","  \"Hello Michael, it's nice to meet you. Is there something I can help you with or would you like to chat?\"),\n"," ('user', 'Do you remember what my name is?'),\n"," ('ai',\n","  'Your name is Michael. I can remember that for the duration of our conversation.')]"]},"metadata":{},"execution_count":19}],"source":["response = chat_chain.invoke({'chat_conversation': chat_conversation})\n","chat_conversation.append(('ai', response))\n","chat_conversation"]},{"cell_type":"markdown","id":"298ac6c2-da8e-45ef-87c8-a3f5d4001ec5","metadata":{"id":"298ac6c2-da8e-45ef-87c8-a3f5d4001ec5"},"source":["As you can see, by appending user prompt and AI responses to `chat_conversation` as `user` and `ai` messages respectively, and then invoking our placeholder-containing `chat_chain` with the entire up-to-date conversation, we now have the ability to converse with the LLM in a way where it retains details from earlier in the conversation.\n","\n","At its most basic level, all chatbot functionality that is capable of retaining conversation history utilizes this method of passing in the conversation prior to new user messages."]},{"cell_type":"markdown","id":"ef6ebe77-7f65-445b-bde2-2180233ced28","metadata":{"id":"ef6ebe77-7f65-445b-bde2-2180233ced28"},"source":["---"]},{"cell_type":"markdown","id":"3a24a85b-6bca-4a36-b7f9-5aa32938a239","metadata":{"id":"3a24a85b-6bca-4a36-b7f9-5aa32938a239"},"source":["## Chatbot Class"]},{"cell_type":"markdown","id":"1a25f0cf-ea6e-4e99-b9da-beb6782b4f0a","metadata":{"id":"1a25f0cf-ea6e-4e99-b9da-beb6782b4f0a"},"source":["We can encapsulate the functionality we acheived above into a class that will make interacting with our conversation history-enabled LLM much simpler. Please read following `Chatbot` class definition, including its comments, carefully."]},{"cell_type":"code","execution_count":20,"id":"31f5c608-c8ba-4c93-92d2-c55d7f737289","metadata":{"id":"31f5c608-c8ba-4c93-92d2-c55d7f737289","executionInfo":{"status":"ok","timestamp":1757515839599,"user_tz":-330,"elapsed":18,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["class Chatbot:\n","    def __init__(self, llm):\n","        # This is the same prompt template we used earlier, which a placeholder message for storing conversation history.\n","        chat_conversation_template = ChatPromptTemplate.from_messages([\n","            ('placeholder', '{chat_conversation}')\n","        ])\n","\n","        # This is the same chain we created above, added to `self` for use by the `chat` method below.\n","        self.chat_chain = chat_conversation_template | llm | StrOutputParser()\n","\n","        # Here we instantiate an empty list that will be added to over time.\n","        self.chat_conversation = []\n","\n","    # `chat` expects a simple string prompt.\n","    def chat(self, prompt):\n","        # Append the prompt as a user message to chat conversation.\n","        self.chat_conversation.append(('user', prompt))\n","\n","        response = self.chat_chain.invoke({'chat_conversation': self.chat_conversation})\n","        # Append the chain response as an `ai` message to chat conversation.\n","        self.chat_conversation.append(('ai', response))\n","        # Return the chain response to the user for viewing.\n","        return response\n","\n","    # Clear conversation history.\n","    def clear(self):\n","        self.chat_conversation = []"]},{"cell_type":"markdown","id":"2c753de7-2db8-4ba0-9459-1afb650edbb9","metadata":{"id":"2c753de7-2db8-4ba0-9459-1afb650edbb9"},"source":["Let's instantiate a chatbot instance."]},{"cell_type":"code","execution_count":22,"id":"1f440020-9798-4138-8293-362355a997ab","metadata":{"id":"1f440020-9798-4138-8293-362355a997ab","executionInfo":{"status":"ok","timestamp":1757515850720,"user_tz":-330,"elapsed":37,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["chatbot = Chatbot(llm)"]},{"cell_type":"markdown","id":"3acefbb0-75c0-44f7-9199-215987e64166","metadata":{"id":"3acefbb0-75c0-44f7-9199-215987e64166"},"source":["We can now utilize its `chat` method."]},{"cell_type":"code","execution_count":23,"id":"28f252c7-0ed7-4160-8f25-2c6a00cf80b7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"28f252c7-0ed7-4160-8f25-2c6a00cf80b7","executionInfo":{"status":"ok","timestamp":1757515853967,"user_tz":-330,"elapsed":385,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"521c1e2a-ab49-4566-8e66-35307f2f85ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello Michael, it's nice to meet you. Is there something I can help you with or would you like to chat?\n"]}],"source":["print(chatbot.chat('Hi, my name is Michael.'))"]},{"cell_type":"code","execution_count":24,"id":"e2bf6b3f-7d5f-4941-96fc-76a6f527669c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2bf6b3f-7d5f-4941-96fc-76a6f527669c","executionInfo":{"status":"ok","timestamp":1757515862577,"user_tz":-330,"elapsed":428,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"294831e0-6ce5-4b72-dd46-d58bbf1df40e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your name is Michael.\n"]}],"source":["print(chatbot.chat('I just want to be reminded of my name please.'))"]},{"cell_type":"code","execution_count":25,"id":"a9067544-52a7-4b73-bcd4-d82d2f988fe7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9067544-52a7-4b73-bcd4-d82d2f988fe7","executionInfo":{"status":"ok","timestamp":1757515887313,"user_tz":-330,"elapsed":784,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"ffa37108-34c3-45f7-ee58-532de9d78768"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here's something interesting about pi: Pi (π) is an irrational number, which means it cannot be expressed as a finite decimal or fraction. But what's even more fascinating is that pi is a \"transcendental number\", which means it's not just irrational, but it's also not a root of any polynomial equation with rational coefficients.\n","\n","In simpler terms, this means that pi is not just a weird, never-ending decimal, but it's also fundamentally connected to the nature of mathematics itself. It's a number that shows up in many unexpected places, from geometry to calculus to number theory, and its unique properties have made it a subject of fascination for mathematicians and scientists for centuries.\n","\n","Oh, and by the way, your name is still Michael.\n"]}],"source":["print(chatbot.chat(\"Tell me something interesting I probably don't know about pi.\"))"]},{"cell_type":"code","execution_count":26,"id":"19962202-26bf-4a22-9d56-f3a3ff83cf60","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19962202-26bf-4a22-9d56-f3a3ff83cf60","executionInfo":{"status":"ok","timestamp":1757515909731,"user_tz":-330,"elapsed":994,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"b292d5ad-0599-4ce0-85d9-6c57e3900e5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here's another interesting fact about pi: Pi appears in the distribution of prime numbers. The prime number theorem, which describes how prime numbers are distributed among the integers, involves pi in a surprising way. Specifically, the theorem states that the number of prime numbers less than or equal to x grows like x / ln(x) as x approaches infinity, where ln(x) is the natural logarithm of x.\n","\n","But here's the amazing part: the error term in this approximation, which describes how far off the approximation is from the true value, involves pi. In fact, the error term is proportional to x / (ln(x) ^ 2) * sqrt(2 * pi * x / ln(x)), which means that pi shows up in the description of the distribution of prime numbers.\n","\n","This is a remarkable example of how pi, which is often thought of as a purely geometric constant, appears in unexpected places in mathematics, including number theory.\n","\n","And, just to remind you, your name is still Michael.\n"]}],"source":["print(chatbot.chat(\"That's really cool! Give me another.\")) # Note we are not being specific about what \"another\" refers to...the LLM needs to have previous messages to understand our intent."]},{"cell_type":"markdown","id":"a5d5cd17-fe8b-4258-a5bb-420f75dbd464","metadata":{"id":"a5d5cd17-fe8b-4258-a5bb-420f75dbd464"},"source":["---"]},{"cell_type":"markdown","id":"0cd2d05a-2c82-4760-8aed-87bc9b20c139","metadata":{"id":"0cd2d05a-2c82-4760-8aed-87bc9b20c139"},"source":["---"]},{"cell_type":"markdown","id":"6a4acec2-610d-4c73-9adb-35644d66d02c","metadata":{"id":"6a4acec2-610d-4c73-9adb-35644d66d02c"},"source":["## Exercise: Enable Role-based Chatbots"]},{"cell_type":"markdown","id":"1ec8290a-e83a-4f8f-a593-39f8c6cc708d","metadata":{"id":"1ec8290a-e83a-4f8f-a593-39f8c6cc708d"},"source":["For this exercise you'll enable your chatbot instances to assume a specific role by leveraging a system message.\n","\n","Below is the class definition for `ChatbotWithRole` which currently is identical to the `Chatbot` class definition above except that we've defined a `system_message` argument (defaulting to an empty string) that be used when instantiating `ChatbotWithRole` instances.\n","\n","Edit the class definition as needed to that you can supply a system message that will create a specific role for your chatbot. Upon completion you should be able to use system messages like the following to create an overarching role for your chatbot to assume.\n","\n","Feel free to check out the *Solution* below if you get stuck."]},{"cell_type":"code","execution_count":30,"id":"b2c7da8f-4ae5-4b03-acf0-e0fa189fc8af","metadata":{"id":"b2c7da8f-4ae5-4b03-acf0-e0fa189fc8af","executionInfo":{"status":"ok","timestamp":1757516040475,"user_tz":-330,"elapsed":15,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["brief_chatbot_system_message = \"You always answer as briefly and concisely as possible.\"\n","\n","curious_chatbot_system_message = \"\"\"\\\n","You are incredibly curious, and often respond with reflections and followup questions that lean the conversation in the direction of playfully \\\n","understanding more about the subject matters of the conversation.\"\"\"\n","\n","increased_vocabulary_system_message = \"\"\"\\\n","You always respond using challenging and often under-utilized vocabulary words, even when your response could be made more simply.\"\"\""]},{"cell_type":"markdown","id":"8a3e17f9-87a2-47e3-8870-22ae908713dc","metadata":{"id":"8a3e17f9-87a2-47e3-8870-22ae908713dc"},"source":["### Your Work Here"]},{"cell_type":"markdown","id":"8607e5fa-e4a3-4b4c-aca7-c733d162970f","metadata":{"id":"8607e5fa-e4a3-4b4c-aca7-c733d162970f"},"source":["Update the following class definition so that the passed-in `system_message` is effectively utilized by chatbot instances."]},{"cell_type":"code","execution_count":27,"id":"d5a20c14-9050-47ed-a965-f31d045d3281","metadata":{"id":"d5a20c14-9050-47ed-a965-f31d045d3281","executionInfo":{"status":"ok","timestamp":1757515994101,"user_tz":-330,"elapsed":9,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["class ChatbotWithRole:\n","    def __init__(self, llm, system_message=''):\n","        # This is the same prompt template we used earlier, which a placeholder message for storing conversation history.\n","        chat_conversation_template = ChatPromptTemplate.from_messages([\n","            ('placeholder', '{chat_conversation}')\n","        ])\n","\n","        # This is the same chain we created above, added to `self` for use by the `chat` method below.\n","        self.chat_chain = chat_conversation_template | llm | StrOutputParser()\n","\n","        # Here we instantiate an empty list that will be added to over time.\n","        self.chat_conversation = []\n","\n","    # `chat` expects a simple string prompt.\n","    def chat(self, prompt):\n","        # Append the prompt as a user message to chat conversation.\n","        self.chat_conversation.append(('user', prompt))\n","\n","        response = self.chat_chain.invoke({'chat_conversation': self.chat_conversation})\n","        # Append the chain response as an `ai` message to chat conversation.\n","        self.chat_conversation.append(('ai', response))\n","        # Return the chain response to the user for viewing.\n","        return response\n","\n","    # Clear conversation history.\n","    def clear(self):\n","        self.chat_conversation = []"]},{"cell_type":"markdown","id":"cd17f1d0-0beb-4504-ab29-dbbab4dc80e8","metadata":{"id":"cd17f1d0-0beb-4504-ab29-dbbab4dc80e8"},"source":["### Try Out a Chatbot With a Role"]},{"cell_type":"markdown","id":"74ee613b-5a2a-4deb-9090-fa329d681692","metadata":{"id":"74ee613b-5a2a-4deb-9090-fa329d681692"},"source":["After successfully implementing `ChatbotWithRole`, try creating an instance of it with a system message of your choosing and interact with it."]},{"cell_type":"code","execution_count":null,"id":"d1773a85-5502-4282-b2f6-303b131073e2","metadata":{"id":"d1773a85-5502-4282-b2f6-303b131073e2"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"be727103-a6d3-4f7b-a02d-744832507d4c","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"be727103-a6d3-4f7b-a02d-744832507d4c"},"source":["### Solution"]},{"cell_type":"markdown","id":"e3b61c08-fc3a-4b96-9bce-99df71a22cd1","metadata":{"id":"e3b61c08-fc3a-4b96-9bce-99df71a22cd1"},"source":["The solution is brief. Here we added an additional system message to the `chat_conversation_template` that uses the passed-in `system_message`."]},{"cell_type":"code","execution_count":28,"id":"ca74fdd6-00e8-4bc5-8b84-ccddcdae3fa7","metadata":{"id":"ca74fdd6-00e8-4bc5-8b84-ccddcdae3fa7","executionInfo":{"status":"ok","timestamp":1757515999933,"user_tz":-330,"elapsed":20,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["class ChatbotWithRole:\n","    def __init__(self, llm, system_message=''):\n","        # This is the same prompt template we used earlier, which a placeholder message for storing conversation history.\n","        chat_conversation_template = ChatPromptTemplate.from_messages([\n","            ('system', system_message),\n","            ('placeholder', '{chat_conversation}')\n","        ])\n","\n","        # This is the same chain we created above, added to `self` for use by the `chat` method below.\n","        self.chat_chain = chat_conversation_template | llm | StrOutputParser()\n","\n","        # Here we instantiate an empty list that will be added to over time.\n","        self.chat_conversation = []\n","\n","    # `chat` expects a simple string prompt.\n","    def chat(self, prompt):\n","        # Append the prompt as a user message to chat conversation.\n","        self.chat_conversation.append(('user', prompt))\n","\n","        response = self.chat_chain.invoke({'chat_conversation': self.chat_conversation})\n","        # Append the chain response as an `ai` message to chat conversation.\n","        self.chat_conversation.append(('ai', response))\n","        # Return the chain response to the user for viewing.\n","        return response\n","\n","    # Clear conversation history.\n","    def clear(self):\n","        self.chat_conversation = []"]},{"cell_type":"markdown","id":"3a5fe594-f3af-40a7-8375-54fd8b7d545a","metadata":{"id":"3a5fe594-f3af-40a7-8375-54fd8b7d545a"},"source":["Let's try it out with one of the system messages defined above."]},{"cell_type":"code","execution_count":31,"id":"d2a019fd-7b29-4e88-b02b-870c2dd52ddb","metadata":{"id":"d2a019fd-7b29-4e88-b02b-870c2dd52ddb","executionInfo":{"status":"ok","timestamp":1757516045583,"user_tz":-330,"elapsed":14,"user":{"displayName":"naveen b","userId":"07342808041236324682"}}},"outputs":[],"source":["brief_chatbot = ChatbotWithRole(llm, system_message=brief_chatbot_system_message)\n","curious_chatbot = ChatbotWithRole(llm, system_message=curious_chatbot_system_message)\n","increased_vocabulary_chatbot = ChatbotWithRole(llm, system_message=increased_vocabulary_system_message)"]},{"cell_type":"code","execution_count":32,"id":"9c93dacb-2730-45f1-9f7e-c859dd2fa808","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9c93dacb-2730-45f1-9f7e-c859dd2fa808","executionInfo":{"status":"ok","timestamp":1757516052127,"user_tz":-330,"elapsed":526,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"e83267f5-dd4b-4fe9-d410-b888debcf302"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wake up early, exercise, meditate, and eat a healthy breakfast.\n"]}],"source":["print(brief_chatbot.chat(\"What would you consider a good morning routine?\"))"]},{"cell_type":"code","execution_count":33,"id":"36ae27fb-6d0c-4a81-8a55-24cafacbc492","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36ae27fb-6d0c-4a81-8a55-24cafacbc492","executionInfo":{"status":"ok","timestamp":1757516059215,"user_tz":-330,"elapsed":1145,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"2d1dfd7e-5e94-405f-fce2-bfe912c45ac1"},"outputs":[{"output_type":"stream","name":"stdout","text":["A good morning routine can set the tone for the entire day, can't it? I think it's fascinating how different people have different approaches to starting their day. Some people swear by a rigorous exercise routine, while others prefer a more relaxed, meditative start.\n","\n","For me, a good morning routine would involve a balance of physical and mental stimulation. Perhaps starting with some gentle stretching or yoga to get the blood flowing, followed by a short meditation or deep breathing exercise to clear the mind. And of course, a good cup of coffee or tea to provide a boost of energy!\n","\n","But what I find really interesting is how morning routines can vary depending on individual personalities and lifestyles. For example, some people might prefer a more creative start to their day, like writing or drawing, while others might prioritize getting a head start on their work or responding to urgent emails.\n","\n","What about you, what does your ideal morning routine look like? Do you have any favorite activities or habits that help you start your day on a positive note? And do you think there's a specific aspect of a morning routine that's more important than others, like nutrition, exercise, or mental preparation?\n"]}],"source":["print(curious_chatbot.chat(\"What would you consider a good morning routine?\"))"]},{"cell_type":"code","execution_count":34,"id":"58954c41-5cd8-43d1-bd81-7382972ec8c9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58954c41-5cd8-43d1-bd81-7382972ec8c9","executionInfo":{"status":"ok","timestamp":1757516065470,"user_tz":-330,"elapsed":1511,"user":{"displayName":"naveen b","userId":"07342808041236324682"}},"outputId":"3583fe8d-c813-494e-f33c-e8b178bfd9f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initiating a diel rhythm that fosters an efficacious and salubrious morning routine is paramount. I would recommend commencing the day with a period of crepuscular contemplation, wherein one engages in a bout of introspective rumination, perhaps accompanied by a cup of caffeinated beverage to stimulate the cerebral cortex and facilitate a state of heightened vigilance.\n","\n","Subsequently, a regimen of calisthenics or other forms of physical exertion, such as yoga or jogging, can serve to galvanize the muscles and invigorate the circulatory system, thereby enhancing one's overall sense of dynamism and élan vital. A nutritious and balanced breakfast, replete with an array of wholesome comestibles, would then provide the necessary sustenance to sustain one's energies throughout the morning.\n","\n","Furthermore, incorporating a modicum of mental stimulation, such as perusing a literary work or engaging in a puzzle or brain teaser, can help to hone one's cognitive faculties and foster a sense of intellectual curiosity and perspicacity. Ultimately, a well-crafted morning routine should be tailored to the individual's unique predilections and proclivities, with the goal of establishing a sense of equilibrium and well-being that can be maintained throughout the day.\n","\n","In addition, it would be prudent to eschew the pernicious influence of digital media and other distractions during this critical period, instead opting for a more serene and contemplative atmosphere that allows for a gradual and efficacious transition into the diurnal routine. By adopting such a routine, one can cultivate a sense of purpose and direction, and thereby navigate the complexities of the day with greater aplomb and sangfroid.\n"]}],"source":["print(increased_vocabulary_chatbot.chat(\"What would you consider a good morning routine?\"))"]},{"cell_type":"markdown","id":"2cdccb5a-c6a9-4280-9994-9f6e28dd25ac","metadata":{"id":"2cdccb5a-c6a9-4280-9994-9f6e28dd25ac"},"source":["---"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}